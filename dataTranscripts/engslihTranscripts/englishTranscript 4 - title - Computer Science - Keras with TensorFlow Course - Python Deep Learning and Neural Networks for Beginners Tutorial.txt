Welcome to this course
0:00
Hey, I'm Andy from deep lizard. And in this course, we're going to learn how to use Kerris, and neural network API written
0:07
in Python and integrated with TensorFlow.
0:13
Throughout the course, each lesson will focus on a specific deep learning concept, and show
Keras Course Introduction
0:22
the full implementation in code using the keras API. We'll be starting with the absolute basics of learning how to organize and pre processed
0:30
data, and then we'll move on to building and training our own artificial neural networks.
0:35
And some of these networks will be built from scratch, and others will be pre trained and state of the art models for which we'll fine tune to use on our own custom data sets.
0:45
Now let's discuss the prerequisites needed to follow along with this course. From a knowledge standpoint, we'll give brief introductions of each deep learning concept
Course Prerequisites
0:53
that we are going to work with before we go through the code implementation. But if you're an absolute beginner to deep learning, then we first recommend you go through
1:02
the deep learning fundamentals scores on the tables or Comm. Or if you're super eager to jump into the code, then you can simultaneously take this
1:10
course and the deep learning fundamentals course, the deep learning fundamentals course will give you all the knowledge you need to know to get acquainted with major deep learning
1:18
concepts for which then you can come back and implement and code using the keras API. And this course, in regard to coding prerequisites, just some basic programming skills and some
1:28
Python experience or all that's needed. On peoples.com. You can also find the deep learning learning path.
DEEPLIZARD Deep Learning Path
1:35
So you can see where this Kerris course falls amidst all the deep lizard deep learning content.
1:40
Now let's discuss the Course Resources. So aside from the videos here on YouTube, you will also have video and text resources
Course Resources
1:48
available on peoples are calm. And actually each episode has its own corresponding blog and a quiz available for you to take
1:55
and test your own knowledge. And you can actually contribute your own quiz questions as well. And you can see how to do that on the corresponding blog for each episode.
2:04
Additionally, all of the code resources used in this course, are regularly tested and maintained,
2:10
including updates and bug fixes when needed. Download access to code files used in this course, are available to members of the deep
2:17
lizard hive mind. So you can check out more about that on tables or calm as well. Alright, so now that we know what this course is about, and what resources we have available,
2:26
along with the prerequisites needed to get started, let's now talk a little bit more about Kerris itself.
About Keras
2:32
Kerris was developed with a focus on enabling fast user experimentation. So this allows us to go from idea to implementation and very few steps.
2:41
Aside from this benefit, users often wonder why choose Kerris as the neural network API
2:47
to learn or in general, which neural network API should they learn? Our general advice is to not commit yourself just to only learning one and sticking with
2:56
that one forever, we recommend to learn multiple neural network API's.
3:01
And the idea is that once you have a fundamental understanding of the underlying concepts, then the minor syntactical and implementation differences between the neural network API's
3:11
shouldn't really be that hard to catch on to once you have at least one under your belt already, especially for job prospects.
3:18
Knowing more than one neural network API will show your experience and allow you to compare and contrast the differences between API's and share your opinions for why you think
3:28
that certain API's may be better for certain problems, and others for other problems.
3:34
Being able to demonstrate this will make you a much more valuable candidate. Now, we previously touched on the fact that Kerris is integrated with TensorFlow.
3:44
So let's discuss that more. Now. Historically, Kerris was a high level neural network API that you could configure to run
3:50
against one of three separate lower level API's. And those lower level API's were TensorFlow viano and cntk.
4:00
Later, though, Kerris became fully integrated with the TensorFlow API, and is no longer
4:06
a separate library that you can choose to run against one of the three back end engines
4:11
that we discussed previously. So it's important to understand that cares is now completely integrated with the TensorFlow
4:17
API. But in this course, we are going to be focusing on making use solely of that high level Kerris
4:24
API without necessarily making much use of the lower level TensorFlow API. Now, before we can start working with cares, then we first have to obviously get it downloaded
4:35
and installed onto our machines. And because Kerris is fully integrated with TensorFlow, we can do that by just installing
4:42
TensorFlow Kerris will come completely packaged with the TensorFlow installation.
4:47
So the installation procedure is as simple as running pip install TensorFlow from your
4:52
command line, you might just want to check out the system requirements on tensor flows website to make sure that your specific system we The requirements needed for TensorFlow
5:01
to install. Alright, so we have one last talking point before we can get into the actual meat of
5:07
the coding. And that is about GPU support. So the first important thing to note is that a GPU is not required to follow this course,
5:17
if you're running your machine only on a CPU, then that is totally fine for what we'll be doing in the course.
5:23
If however, you do want to run your code on a GPU, then you can do so pretty easily. After you get through the setup process for setting up your GPU to work with TensorFlow,
5:34
we have a full guide on how to get the GPU setup to work with TensorFlow on deep lizard
5:39
Comm. So if you are interested in doing that, then head over there to go through those steps. But actually, I recommend just going through the course with a CPU if you're not already
5:48
set up with a GPU. And like I said, the all the code will work completely fine, run totally fine using only
5:55
a CPU. But then after the fact, after you go through the course successfully, then get the steps
6:00
in order to work with the GPU if you have one, and then run all the code that you have
6:05
in place from earlier, run it on the GPU, the second go round and just kind of look and see the kind of efficiency and speed ups that you'll see in the code.
6:15
Alright, so that's it for the Kerris introduction. Now we're finally ready to jump in to the code.
6:22
Be sure to check out the blog and other resources available for this episode on the poser.com
6:28
as well as the dibbles at hive mind where you can gain access to exclusive perks and
6:33
rewards. Thanks for contributing to collective intelligence. Now, let's move on to the next episode.
6:40
Hey, I may be from deep lizard. In this episode, we'll learn how to prepare and process numerical data that will later
Keras with TensorFlow - Data Processing for Neural Network Training
6:49
use to train our very first artificial neural network.
6:59
To train any neural network and a supervised learning task, we first need a data set of
7:05
samples along with the corresponding labels for those samples. When referring to the word samples, we're simply just talking about the underlying data
7:13
set, where each data point in the set is referred to as a sample. If we were to train a model to do sentiment analysis on headlines from a media source,
7:23
for example, then the labels that correspond to each headline sample would be positive
7:30
or negative. Or say that we were training an artificial neural network to identify images as cats
7:36
or dogs, well, then that would mean that each image would have a corresponding label of
7:42
cat or dog. Note that in deep learning, you may also hear samples referred to as inputs or input data.
7:50
And you may hear labels referred to as targets or target data.
7:55
When preparing a data set, we first need to understand the task for which the data will be used.
8:01
In our example, we'll be using our data set to train an artificial neural network.
8:06
So once we understand this, then we can understand which format the data needs to be in, in order
8:12
for us to be able to pass the data to the network. The first type of neural network that we'll be working with is called a sequential model
8:20
from the cares API. And we'll discuss more details about the sequential model in a future episode.
8:26
But for now, we just need to understand what type of data format the sequential model expects,
8:32
so that we can prepare our dataset accordingly. The sequential model receives data during training whenever we call the fit function
8:39
on it. And again, we're going to go into more details about this function in the future. But for now, let's check out what type of data format the fit function expects.
8:49
So if we look at the fit documentation here, on tensor flows website, we can see the first
8:55
two parameters that the fit function expects are x and y. So x is our input data, our samples, in other words, so this function expects x our input
9:06
data to be in a NumPy array, a TensorFlow tensor, a dict, mapping, a TF dot data data
9:13
set or a karass generator. So if you're not familiar with all of these data types, that's okay, because for our first
9:21
example, we are going to organize our data to be in a NumPy array. So the first option here in the documentation, so this is for our input samples, but also
9:31
the Y parameter that is expected by the fit function is our data set that contains the
9:39
corresponding labels for our samples. So the target data. Now the requirement for why is that it is formatted as one of the above formats that
9:48
we just discussed for x, but y needs to be in the same format as x.
9:54
So we can't have our samples contained in a NumPy array, for example, and then have y Our target data or our labels for those samples and the TensorFlow tensor.
10:04
So the format of x and y, both need to match. And we're going to be putting both of those into NumPy arrays.
10:12
Alright, so now we know the data format that the model expects. But there's another reason that we may want to transform or process our data.
10:20
And that is to put it in a format that will make it easier or more efficient for the model to learn from.
10:26
And we can do that with data normalization or standardization techniques, data processing,
10:31
and deep learning will vary greatly depending on the type of data that we're working with.
10:37
So to start out, we are going to work with a very simple numerical data set to train our model.
10:43
And later, we'll get exposure to working with different types of data as well. Alright, so now we're ready to prepare and process our first data set.
10:51
So we are in our Jupyter Notebook. And the first step is to import all the packages that we'll be making use of including NumPy
10:58
random and some modules from scikit learn. Next, we will create two list one called train samples, one called train labels, and these
11:08
lists will hold the corresponding samples and labels for our data set. Now about this data set, we are going to be working with a very simple numerical data
11:18
set. And so for this task, we're actually going to create the data ourselves. Later, we'll work with more practical examples and realistic ones where where we won't be
11:27
creating the data, but instead downloading it from some external source. But for now, we're going to create this data ourselves for which will train our first artificial
11:37
neural network. So as a motivation for this kind of dummy data, we have this background story to give
11:44
us an idea of what this data is all about. So let's suppose that an experimental drug was tested on individuals ranging from age
11:53
13 to 100. In a clinical trial, this trial had 2100 participants total, half of these participants were under
12:01
the age of 65. And half were 65 years or older. And the conclusions from this trial was that around 95% of the patients who were in the
12:10
older population, so 65 or older, the 95% of those patients experienced side effects.
12:18
And around 95% of patients who were under 65 years old, experienced no side effects.
12:24
Okay, so this is a very simplistic data set. And so that is the background story.
12:30
Now in this cell, here, we're going to go through the process of actually creating that data set.
12:36
So what this is, is this first for loop is going to generate both the approximately 5%
12:43
of younger individuals who did experience side effects, and the 5% of older individuals
12:50
who did not experience side effects. So within this first for loop, we are first generating a random number or a random integer
12:58
rather between 13 and 64. And that is constituting as a younger individual who is under 65 years of age, or yet who's
13:07
under 65 years of age. And we are going to then append this number to the train samples list.
13:16
And then we append a one to the train labels list. Now a one is representing the fact that a patient did experience side effects.
13:25
And a zero would represent a patient who did not experience side effects. So then similarly, we jumped down to the next line.
13:33
And we are generating a random integer between 65 in 100, to represent the older population.
13:39
And we are doing that Remember, this is in our first for loop. So it's only running 50 times.
13:46
So this is kind of the outlier group, these 5% of older individuals who did not experience
13:51
side effects. So we then take that sample appended to the train samples list, and append a zero to the
14:00
corresponding train labels list. Since these patients were the older patients who did not experience side effects.
14:07
So then if we jumped down to the next for loop, very similarly, we have pretty much
14:12
the same code here, except for this is the bulk of the group. And this for loop, we're running this for the 95% of younger individuals who did not
14:22
experience side effects, as well as the 95% of older individuals who did experience side
14:28
effects. So generating a random number between 13 and six before and then appending that number,
14:34
representing the age of the younger population to the train samples list, and appending the
14:40
label of zero. Since these individuals did not experience side effects, we're appending zero to the
14:46
train labels list. Similarly, we do the same thing for the older individuals from 65 to 100.
14:53
Except for since the majority of these guys did experience side effects, we are appending a one to the train The labels list.
15:01
So just to summarize, we have the samples list that contains a bunch of integers ranging
15:06
from 13 to 100. And then we have the train labels list that has the labels that correspond to each of
15:14
these individuals with the ages 13 to 100. The labels correspond to whether or not these individuals experience side effects.
15:22
So a samples list containing ages, a labels list containing zeros and ones representing
15:27
side effects, or no side effects. And just to get a visualization of the samples, here, we are printing out all of the samples
15:34
in our list. And we see that these are just all integers, like we'd expect, ranging from 13 to 100.
15:42
And then correspondingly, if we run our train labels list and print out all of the data
15:50
there, then we can see this list contains a bunch of zeros and ones. Alright, so now the next step is to take these lists and then start processing them.
15:59
So we have our data generated. Now we need to process it to be in the format for which we saw the fit function expects,
16:06
and we discussed the fact that we are going to be passing this data as NumPy arrays to the fit function.
16:13
So our next step is to go ahead and do that transformation here, where we are taking the
16:18
train labels list and making that now a NumPy array. Similarly, doing the same thing with the train samples list.
16:26
And then we use the shuffle function to shuffle both are trained labels and trained samples
16:33
respective to each other so that we can get rid of any imposed order from the data generation
16:39
process. Okay, so now the data is in the NumPy array format that is expected by the fit function.
16:46
But as mentioned earlier, there's another reason that we might want to do further processing on the data.
16:51
And that is to either normalize or standardize it so that we can get it in such a way that
16:56
the training of the neural network might become quicker or more efficient.
17:02
And so that's what we're doing in this cell. Now we are using this min max scalar object to create a feature range ranging from zero
17:10
to one, which we'll then use in this next line to rescale our data from the current
17:17
scale of 13 to 100, down to a scale of zero to one, and then this reshaping that we're
17:25
doing here is just a formality because the fit transform function doesn't accept one D data by default.
17:30
Since our data is one dimensional, we have to reshape it in this way to be able to pass
17:36
it to the fit transform function. So now if we print out the elements in our new scaled train samples variable, which that's
17:44
what we're calling our new scaled samples, then we can print them out and see that now
17:50
the individual elements are no longer integers ranging from 13 to 100. But instead, we have these values ranging anywhere between zero and one.
18:01
So at this point, we have generated some raw data, and then processed it to be in a NumPy
18:07
array format that our model will expect. And then rescale the data to be on a scale between zero and one, and an upcoming episode,
18:17
we'll use this data to train our first artificial neural network, be sure to check out the blog
18:22
and other resources available for this episode on deep lizard calm, as well as the deep lizard
18:27
hive mind where you can gain access to exclusive perks and rewards. Thanks for contributing to collective intelligence.
18:35
Now, let's move on to the next episode.
Create an Artificial Neural Network with TensorFlow's Keras API
18:40
Hey, I'm Andy from Blizzard. And this episode will demonstrate how to create an artificial neural network using a sequential
18:48
model from the keras API integrated within tensor flow.
18:59
In the last episode, we generated data from an imagined clinical trial. And now we'll create an artificial neural network for which we can train on this data.
19:09
Alright, so first things first, we need to import all of the TensorFlow modules that we'll be making use of to build our first model.
19:17
And that includes everything that you see here except for actually the last two of atom and categorical cross and vitry.
19:26
categorical cross entropy, rather, those two are going to be used when we train the model,
19:31
not when we build it. But we're going ahead and bringing all the imports in now. And then next, if you are running this code on a GPU, then you can run this cell which
19:40
will allow you to make sure that TensorFlow is correctly identifying your GPU as well
19:46
as enable memory growth. And there's a few lines on the blog where you can check out what exactly that means
19:53
and why you might want to do this. But if you are running a GPU then go ahead and run this cell.
19:58
So next, this is the The actual model that we are building this is a sequential model.
20:05
And that is kind of the most simplest type of model that you can build using Kerris or
20:10
TensorFlow. And a sequential model can be described as a linear stack of layers.
20:15
So if you look at how we're creating the model here, that's exactly what it looks like. So we are initializing the model as an instance of the sequential class.
20:26
And we are passing in a list of layers here. Now, it's important to note that this first dense layer that we're looking at, that is
20:36
actually the second layer overall. So this is the first hidden layer.
20:42
And that's because the input layer, we're not explicitly defining using Kerris, the
20:48
input data is what creates the input layer itself. So the way that the model knows what type of input data to expect, or the shape of the
20:58
input data, rather, is through this input shape parameter that we pass to our first
21:04
dense layer. So through this, the model understands the shape of the input data that it should expect.
21:10
And then therefore it accepts that shape, have input data, and then passes that data to the first hidden layer, which is this dense layer here in our case, now, we are telling
21:22
this dense layer that we want it to have 16 units. These units are also otherwise known as nodes or neurons.
21:30
And the choice of 16 here is actually pretty arbitrary. This model overall is very simple.
21:36
And with the arbitrary choice of nodes here, it's actually going to be pretty hard to create
21:42
a simple model, at least, that won't do a good job at classifying this data, just given
21:48
the simplicity of the data itself. So we understand that we're passing in, or that we're specifying 16 units for this first
21:56
hidden layer, we are specifying the input shape, so that the model knows the shape of the input data to expect and then we are stating that we want the relu activation function
22:08
to follow this dense layer. Now, this input shape parameter is only specified for our first hidden layer.
22:15
So after that, we have one more hidden, dense layer. This time, we are arbitrarily setting the number of units for this dense layer to be
22:23
32. And again, we're following the layer by the activation function value.
22:29
And then lastly, we specify our last output layer or our last layer, which is our output
22:35
layer. This is another dense layer, this time with only two units, and that is corresponding
22:42
to the two possible output classes. Either a patient did experience side effects, or the patient did not experience side effects.
22:51
And we're following this output layer with the softmax function, which is just going
22:56
to give us probabilities for each output class. So between whether or not a patient experience side effects or not, we will have an output
23:07
probability for each class, letting us know which class is more probable for any given
23:12
patient. And just in case, it's not clear, this dense layer here is what we call a densely connected
23:20
layer or a fully connected layer, probably the most well known type of layer and an artificial
23:26
neural networks. Now in case you need any refresher on fully connected layers, or activation functions,
23:32
or anything else that we've discussed up to this point, then just know that all of that is covered in the deep learning fundamentals course, if you need to go there and refresh
23:40
your memory on any of these topics here. Alright, so now we'll run this cell to create our model.
23:47
And now we can use model dot summary to print out a visual summary of the architecture of
23:53
the model we just created. So looking here, we can just see the visual representation of the architecture that we
24:00
just created in the cell above. All right, so now we have just finished creating our very first neural network using this simple
24:08
and intuitive sequential model type. In the next episode, we will see how we can use the data that we created last time to
24:16
train this network. Be sure to check out the blog and other resources available for this episode on depot's or.com
24:23
as well as the tables at hive mind where you can gain access to exclusive perks and rewards.
24:29
Thanks for contributing to collective intelligence. Now let's move on to the next episode.
Train an Artificial Neural Network with TensorFlow's Keras API
24:36
Hey, I'm Andy from deep lizard. In this episode, we'll see how to train an artificial neural network using the keras
24:45
API integrated with TensorFlow.
24:54
In previous episodes, we went through steps to generate data and also build an Artificial
25:00
neural network. So now we'll bring these two together to actually train the network on the data that we created
25:06
and processed. Alright, so picking up where we were last time in our Jupyter Notebook, make sure that
25:13
you still have all of your imports included and already ran so that we can continue where
25:18
we were before. So first we have after building our model, we are going to call this model compile function.
25:25
And this just prepares the model for training. So it gets everything in order that's needed before we can actually train the model.
25:33
So first, we are specifying to the compile function, what optimizer that we want to use, and we're choosing to use the very common optimizer atom with a learning rate of 0.0001.
25:45
And next we specify the type of loss that we need to use, which is in our case, we're
25:51
going to use sparse categorical cross in but we're going to use sparse categorical cross
25:57
entropy. And then lastly, we specify what metrics we want to see. So this is just for the model performance, what we want to be able to judge our model
26:07
by and we are specifying this list, which just includes accuracy, which is a very common
26:13
way to be able to evaluate model performance. So if we run this cell, alright, so the model has been compiled and is ready for training.
26:22
And training occurs whenever we call this fit function. Now recall earlier in the course, we actually looked at the documentation for the split
26:30
function, so that we knew how to process our input data. So to fit the first parameter that we're specifying is x here, which is our input data, which
26:39
is currently stored in this scaled at train samples variable, then y, which is our target
26:46
data, or our labels are labels are currently stored in the train labels variable.
26:52
So we are specifying that here. Next, we specify our batch size that we want to use for training.
26:58
So this is how many samples are included in one batch to be passed and processed by the
27:04
network at one time. So we're setting this to 10. And the number of epochs that we want to run, we're setting this to 30.
27:11
So that means that the model is going to process or train on all of the data in the data set
27:18
30 times before completing the total training process. Next, we're specifying this shuffle, shuffle parameter which we are setting to true.
27:29
Now, by default, this is already set to true, but I was just bringing it to your attention to show or to make you aware of the fact that the data is being shuffled by default, when
27:40
we pass it to the network, which is a good thing, because we want any order that is inside
27:47
of the dataset to be kind of erased before we pass the data to the model so that the
27:53
model is not necessarily learning anything about the order of the data set. So this is true by default. So we don't necessarily have to specify that I was just letting you know.
28:02
And actually, we'll see something about that in the next episode about why this is important
28:08
regarding validation data, but we'll see that coming up. The last parameter that we specify here is verbose, which is just an option to allow
28:16
us to see output from whenever we run this fit function. So we can either set it to 01, or two, two is the most verbose level in terms of output
28:25
messages. So we are setting that here so that we can get the highest level of output.
28:31
So now let's run this cell so that training can begin. All right, so training has just stopped.
28:36
And we have run for 30 epochs. And if we look at the progress of the model, so we're starting out on our first epoch,
28:44
our loss value is currently point six, eight and our accuracy is 50%. So no better than chance.
28:50
But pretty quickly looking at the accuracy, we can tell that it is steadily increasing
28:56
all the way until we get to our last a POC, which we are yielding 94% accuracy.
29:03
And our loss has also steadily decreased from the point six five range to now being at point
29:09
two seven. So as you can see this model train very quickly, with each epoch taking only under one second
29:17
to run. And within 30 epochs, we are already at a 94% accuracy right.
29:23
Now although this is a very simple model, and we were training it on a very simple data,
29:28
we can see that without much effort at all, we were able to yield pretty great results in a relatively quick manner of time as well.
29:35
In subsequent episodes, we'll demonstrate how to work with more complex models as well as more complex data.
29:42
But for now, hopefully this example served the purpose of encouraging you on how easy it is to get started with Kerris.
29:49
Be sure to check out the blog and other resources available for this episode on V bowser.com.
29:54
As well as the deep lizard hive mind where you can gain access to exclusive perks and
29:59
rewards. Thanks for contributing to collective intelligence. Now let's move on to the next episode.
Build a Validation Set With TensorFlow's Keras API
30:08
Hey, I'm Andy from deep lizard. In this episode, we'll demonstrate how we can use tensor flows keras API to create a
30:16
validation set on the fly during training.
30:23
Before we demonstrate how to build a validation set using Kerris, let's first talk about what
30:32
exactly a validation set is. So whenever we train a model, our hope is that when we train it that we see good results
30:40
from the training output, that we have low loss and high accuracy. But we don't ever train a model just for the sake of training it, we want to take that
30:50
model and hopefully be able to use it in some way on data that it wasn't necessarily exposed
30:56
to during the training process. And although this new data is data that the model has never seen before, the hope is that
31:03
the model will be good enough to be able to generalize well on this new data and give
31:08
accurate predictions for it, we can actually get an understanding of how well our model
31:14
is generalizing by introducing a validation set during the training process to create
31:20
a validation set. Before training begins, we can choose to take a subset of the training set, and then separate
31:27
it into a separate set labeled as validation data. And then during the training process, the model will only train on the training data,
31:36
and then we'll validate on the separated validation data. So what do we mean by validating?
31:42
Well, essentially, if we have the addition of a validation set, then during training,
31:48
the model will be learning the features of the training set, just as we've already seen. But in addition, in each epoch, after the model has gone through the actual training
31:57
process, it will take what it's learned from the training data, and then validate by predicting
32:02
on the data in the validation set, using only what it's learned from the training data, though.
32:08
So then during the training process, when we look at the output of the accuracy and loss, not only will we be seeing that accuracy and loss computed for the training set, we'll
32:18
also see that computed on the validation set. It's important to understand though, that the model is only alerting on or training
32:26
on the training data. It's not taking the validation set into account during training.
32:33
The validation set is just for us to be able to see how well the model is able to predict
32:38
on data that it was not exposed to during the training process. In other words, it allows us to see how general our model is how well it's able to generalize
32:48
on data that is not included in the training data. So knowing this information will allow us to see if our model is running into the famous
32:57
overfitting problem. So overfitting occurs when the model has learned the specific features of the training set
33:04
really well, but it's unable to generalize on data it hasn't seen before.
33:09
So if while training, we see that the model is giving really good results for the training set, but less than good results for the validation set, then we can conclude that we have an
33:19
overfitting problem, and then take the steps necessary to combat that specific issue.
33:25
If you'd like to see the overfitting problem covered in more detail, then there is an episode for that in the deep learning fundamentals course.
33:31
Alright, so now let's discuss how we can create and use a validation set with a karass sequential
33:37
model, there's actually two ways that we can create and work with validation sets with
33:42
a sequential model. And the first way is to have a completely separate validation set from the training
33:48
set. And then to pass that validation set to the model in the fit function, there is a validation
33:56
data parameter. And so we can just set that equal to the structure that is holding our validation data.
34:04
And there's a write up in the corresponding blog for this episode that contains more details about the format that that data needs to be in.
34:10
But we're going to actually only focus on the second way of creating and using a validation set.
34:16
This step actually saves us a step because we don't have to explicitly go through the creation process, the validation set, instead, we can get Kerris to create it for us.
34:27
Alright, so we're back in our Jupyter Notebook right where we left off last time. And we're here on the model dot fit function.
34:34
And recall, this is what we use last time to train our model. Now, I've already edited this cell to include this new parameter, which is validation split.
34:44
And what validation split does is it does what it sounds like it splits out a portion
34:49
of the training set into a validation set. So we just set this to a number between zero and one.
34:57
So just a fractional number to tell Kerris How much of the training set we need to split
35:03
out into the validation set. So here I'm splitting out 10% of the training set.
35:09
So it's important to note that whenever we do this, the validation set is completely
35:14
held out of the training set. So the training samples that we remove from the training set into validation set are no
35:22
longer contained within the training data any longer. So using this approach, the validation set will be created on the fly whenever we call
35:30
the fit function. Now, there's one other thing worth mentioning here. And remember last time, I discussed this shuffle equals true parameter.
35:40
And I said that by default, the training set is shuffled whenever we call fit. So this shuffle equals true is already set by default.
35:47
But I was just bringing it up to let you know that that the training set is being shuffled.
35:53
So that is a good thing, we want the training set to be shuffled. But whenever we call validation split in this way, this split occurs before the training
36:03
set is shuffled, meaning that if we created our training set and say, we put all of the
36:11
sick patients first and then the non sick patients second, and then we say that we want
36:17
to split off the last 10% of the training data to be our validation data, it's going
36:24
to take the last 10% of the training data. And therefore it could just take all of the the second group that we put in the training
36:33
set and not get any of the first group. So I wanted to mention that because although the training data is being shuffled with the
36:41
fit function, if you haven't already shuffled your training data before you pass it to fit,
36:47
then you also use the validation split parameter, it's important to know that your validation
36:52
set is going to be the last X percent of your training set and therefore may not be shuffled
36:58
and may yield some strange results because you think that everything has been shuffled when really, it's only the training set has been shuffled after the validation set has
37:07
been taken out. So just keep that in mind the way that we created our training set.
37:12
Before this episode, we actually shuffled the training data before it's ever passed
37:18
to the fit function. So in the future, whenever you're working with data, it's a good idea to make sure that your data is also shuffled beforehand, especially if you're going to be making use of the validation
37:28
split parameter to create a validation set. Alright, so now we'll run this cell one more time calling the fit function.
37:34
But this time, not only will we see loss and accuracy metrics for the training set, we'll
37:40
also see these metrics for the validation set. Alright, so the model has just finished running it's 30 epochs.
37:50
And now we see both the loss and accuracy on the left hand side, as well as the validation
37:56
loss and validation accuracy on the right hand side. So we can see, let's just look at the accuracy between the two.
38:04
They're both starting at around the same 50% Mark, and going up gradually around the same
38:11
rate. So we just scroll all the way to our last epoch, we can see that the accuracy and validation
38:18
accuracy are pretty similar with only 1% difference between the two.
38:23
And yet the loss values are similar as well. So we can see in this example that our model is not overfitting, it is actually performing
38:34
pretty well or just as well rather on the validation set as it is on the training set.
38:39
So our model is generalizing well. If however, we saw that the opposite case was true, and our validation accuracy was
38:46
seriously lagging behind our training accuracy, then we know that we have a overfitting problem
38:51
and we would need to take steps to address that issue. Alright, so we've now seen how to train the model how to validate the model, and how to
39:00
make use of both training and validation sets. In the next episode, we're going to see how to make use of a third data set that test
39:07
data set. To use the model for inference. Be sure to check out the blog and other resources available for this episode on depot's or.com
39:16
as well as the deep lizard hive mind where you can gain access to exclusive perks and
39:21
rewards. Thanks for contributing to collective intelligence. Now let's move on to the next episode.
Neural Network Predictions with TensorFlow's Keras API
39:28
Hey, I'm Andy from deep lizard. In this episode, we'll see how we can use a neural network for inference to predict
39:37
on data from a test set using TensorFlow keras API.
39:43
As we touched on previously, whenever we train a model, the hope is that we can then take
39:52
that model and use it on new data that it has not seen before during training, and hopefully
39:58
the model is able to generalize well. And give us good results on this new data.
40:03
So as a simple example, suppose that we trained a network to be able to identify images of
40:09
cats or dogs. And so during the training process, of course, we had a training set that, say, we downloaded
40:16
from a website with 1000s of images of cats and dogs. So the hope is later that if we wanted to, we could maybe build a web app, for example.
40:24
And we could have people from all over the world, submit their dog and cat photos and have our model, tell them with high accuracy, whether or not their animal is a cat or a
40:35
dog. So I don't know why anyone would actually make that web app. but you get the point. The hope is that even though the images that are being sent in from people around the world
40:45
have their own cats and dogs, even though those weren't included in the training set that the model was originally trained on.
40:51
Hopefully, the models able to generalize well enough to understand from what it's learned
40:56
about dog and cat features, that it can predict that Mandy's dog is actually a dog and not
41:03
a cat, for example, we call this process inference. So the model takes what it learned during training, and then uses that knowledge to
41:11
infer things about data that it hasn't seen before. In practice, we might hold out a subset of our training data to put in a set called the
41:20
test set. Typically, after the model has been trained and validated, we take the model and use it
41:26
for inference purposes against the test set, just as one additional step to the validation
41:31
to make sure that the model is generalizing Well, before we deploy our model to production.
41:38
So at this point, the model that we've been working with over the last few episodes has been trained and validated.
41:45
And given the metrics that we saw during the validation process, we have a good idea that the model is probably going to do a pretty good job at inference on the test set as well.
41:55
In order to conclude that though, we first would need to create a test set. So we're going to do that now.
42:01
And then after we create the test set, then we'll use the model for inference on it. Alright, so we are back in our Jupyter Notebook.
42:08
And now we're going to go through the process of creating the test set. And actually, if you just glance at this code here, you can see that this whole process
42:16
of setting up the the samples and labels list and then generating the data from the imagine
42:24
clinical trial that we discussed in a previous episode, we're then taking that generated data and putting it into a NumPy array format, then shuffling that data, and then scaling
42:37
the data to be on a scale from zero to one, rather than from the scale of 13 to 100.
42:43
So actually, that is the same exact process using almost the same exact code, except for
42:49
we're working with our tests, labels, and test samples, variables rather than train
42:55
labels and train samples. So we're not going to go line by line through this code. If you need a refresher, go check out the earlier episode where we did the exact same
43:03
process for the training set. The important thing to take from this process, though, is that the test set should be prepared
43:12
and processed in the same format as the training data was. So we'll just go ahead and run the cells to test and are to create and process the test
43:23
data. And now, we are going to use our model to predict on the test data.
43:31
So to obtain predictions from our model, we call predict on the model that we created
43:37
in the last couple of episodes. So we are calling model dot predict. And we are first passing in this parameter x which we're setting equal to our scaled
43:47
test samples. So that is what we created in just the line above, where we scaled our test samples to
43:54
be on a scale from zero to one. So this is the data that we want our model to predict on.
43:59
Then we specify the batch size, and we are setting the batch size equal to 10, which
44:05
is the exact same batch size that we use for our training data whenever we train the model
44:10
as well. And then the last parameter we're specifying is this verbose parameter, we're setting this
44:15
equal to zero, because during predicting there is not any output from this function that
44:21
we actually care about seeing or that is going to be any use to us at the moment. So we're setting that equal to zero to get no output.
44:28
Alright, so then if we run this, so then our model predicts on all of the data in our test
44:34
set. And if we want to have a visualization of what each of these predictions from the model
44:41
looks like for each sample, we can print them out here. So looking at these predictions, the way that we can interpret this is for each element
44:51
within our test set. So for each sample in our tests that we are getting a probability that maps to either
45:00
The patient not experiencing a side effect, or the patient experiencing a side effect.
45:06
So for the first sample in our test set, this prediction says that the model is a 92, or
45:13
is assigning a 92% probability to this patient, not experiencing a side effect, and just a
45:21
around 8% probability of the patient experiencing a side effect.
45:26
So recall that we said, no side effect experience was labeled as a zero, and a side effect experienced
45:33
was labeled as a one. So that is how we know that this particular probability maps to not having a side effect
45:41
because it's in the zeroeth index. And this specific probability maps to having a side effect because it is in the first index.
45:50
So if we're interested in seeing only the most probable prediction for each sample in the test set, then we can run this cell here, which is taking the predictions and getting
46:01
the index of the prediction with the highest probability.
46:06
And if we print that out, then we can see that these are a little bit easier to interpret
46:12
than the previous output. So we can see for the first sample that the prediction is zero, the second sample is a
46:19
one. And just to confirm, if we go back up here, we can see that the first sample indeed has
46:26
the higher probability of a label of zero, meaning no side effects. And the second sample has a higher probability of one meaning that the patient did experience
46:38
a side effect. So from these prediction results, we're able to actually see the underlying predictions.
46:44
But we're not able to make much sense of them in terms of how well the model did add these
46:49
predictions, because we didn't supply the labels to the model during inference in the same way that we do during training.
46:56
This is the nature of inference. A lot of times inference is occurring once the model has been deployed to production.
47:02
So we don't necessarily have correct labels for the data that the model is inferring from.
47:08
If we do have corresponding labels for our test set, though, which in our case, we do, because we are the ones who generated the test data, then we can visualize the prediction
47:17
results by plotting them to a confusion matrix. And that'll give us an overall idea at how accurate our model was at inference on the
47:26
test data. We'll see exactly how that's done in the next episode.
47:31
Be sure to check out the blog and other resources available for this episode on depot's or.com
47:37
as well as the tables at hive mind where you can gain access to exclusive perks and rewards.
47:42
Thanks for contributing to collective intelligence. Now let's move on to the next episode.
Create a Confusion Matrix for Neural Network Predictions
47:50
Hey, I'm Andy from deep lizard. In this episode, we'll demonstrate how to use a confusion matrix to visualize prediction
47:58
results from a neural network during inference. In the last episode, we showed how we could use our train model for inference on data
48:12
contained in a test set. Although we have the labels for this test set, we don't pass them to the model during
48:19
inference, and so we don't get any type of accuracy readings for how well the model does
48:24
on the test set. Using a confusion matrix, we can visually observe how well a model predicts on test
48:30
data. Let's jump right into the code to see exactly how this is done. We'll be using scikit Learn to create our confusion matrix.
48:37
So the first thing we need to do is import the necessary packages that we'll be making use of next week create our confusion matrix by calling this confusion matrix function
48:47
from scikit learn. And we pass in our test labels as the true labels.
48:55
And we pass in our predictions as the predictions and that the confusion matrix at specs and
49:03
recall this rounded predictions variable, as well as the test labels. These were created in the last episode.
49:10
So rounded predictions recall was when we use the arg max function to select only the
49:16
most probable predictions. So now our predictions are in this format. And then our labels are zeros and ones that correspond to whether or not a patient had
49:25
side effects or not. So next we have this plot confusion matrix function.
49:30
And this is directly copied from socket learns website. There's a link to the site on the corresponding blog where you can copy this exact function.
49:39
But this is just a function that socket learn has created to be able to easily plot in our
49:44
notebook, the confusion matrix, which is going to be the actual visual output that we want to see.
49:50
So we just run this cell to define that function. And now we create this list that has the labels that we will use on our computer.
50:00
In matrix, so we want, the labels have no side effects and had side effects.
50:06
Those are the corresponding labels for our test data, then we're going to call the plot
50:11
confusion matrix function that we just brought in and defined above from scikit learn.
50:16
And to that we are going to pass in our confusion matrix. And the classes for the confusion matrix which we are specifying cm plot labels, which we
50:26
defined just right above. And lastly, just the title that is going to be the title to display above the confusion
50:34
matrix. So if we run this, then we actually get the confusion matrix plot.
50:40
Alright, so we have our predicted labels on the x axis and our true labels on the y axis.
50:47
So the way we can read this is that we look and we see that our model predicted that a patient had no side effects 10 times when the patient actually had a side effect.
50:59
So that's incorrect predictions. On the flip side, though, the model predicted that the patient had no side effects 196 times
51:08
that the patient indeed had no side effects. So the this is the correct predictions.
51:14
And actually, generally reading the confusion matrix, looking at the top left to the bottom
51:20
right diagonal, these squares here and blue going across this diagonal are the correct
51:26
predictions. So we can see total that the model predicted 200 plus 196.
51:32
So 396 correct predictions out of a total of 420, I think, yes.
51:41
So all these numbers added up equal 420 396 out of 420 predictions were correct.
51:49
So that gives us about a 94% accuracy rate on our test set, which is equivalent to what
51:56
we were seeing for our validation, accuracy rate during training. So as you can see, a confusion matrix is a great tool to use to be able to visualize
52:04
how well our model is doing edits predictions, and also be able to drill in a little bit further to see for which classes it might need some work, be sure to check out the blog
52:14
and other resources available for this episode on the poser.com as well as the deep lizard
52:20
hive mind where you can gain access to exclusive perks and rewards. Thanks for contributing to collective intelligence.
52:27
Now let's move on to the next episode. Hey, I'm Andy from deep lizard.
Save and Load a Model with TensorFlow's Keras API
52:35
And this episode will demonstrate the multiple ways that we can save and load a karass sequential
52:40
model.
52:47
We have a few different options when it comes to saving and loading a karass sequential model. And these all work a little bit differently from one another.
52:55
So we're going to go through all of the options. Now. We've been working with a single model over the last few episodes.
53:01
And we're going to continue working with that one. Now, I've printed out a summary of that model to refresh your memory about which one we've
53:07
been working with. But just make sure that in your Jupyter Notebook that you have that model already created,
53:13
because we're going to now show how we can save that model. Alright, so the first way to save a model is just by calling the Save function on it.
53:20
So to save, we pass in a path for which we want to save our model, and then the model
53:29
name or the file name that we want to save our model under with the H five extension.
53:34
So this h5 file is going to be where the model is stored. And this code here is just a condition that I'm checking to see if the model is not already
53:43
saved to disk first, then save it because I don't want to continue saving the model over and over again, on my machine if it's already been saved.
53:51
So that's what this this condition is about. But this model dot save function is the first way that we can save the model.
53:59
Now when we save using this way, it saves the architecture of the model, allowing us
54:04
to be able to recreate it with the same number of learnable parameters and layers and nodes
54:09
etc. It also saves the weights of the model. So if the models already been trained, then the weights that it has learned and optimized
54:18
for are going to be in place within this saved model on disk. It also saves the training configuration.
54:25
So things like our loss and our optimizer that we set whenever we compile the model,
54:31
and the state of the optimizer is also saved. So that allows us if we are training the model, and then we stop and save the model to disk,
54:40
then we can later load that model again and pick up training where we left off because the state of the optimizer will be in that saved state.
54:48
So this is the most comprehensive option when it comes to saving the model because it saves
54:54
everything, the architecture, the learnable parameters and the state of the model.
55:00
where it left off with training. So if we want to load a model later that we previously saved to disk, then we need to
55:07
first import this load model function from TensorFlow Kerris dot models.
55:12
And then we create a variable. In this case, I'm calling it a new model, and then setting it to load model and then
55:19
pointing to where our saved model is on disk. So then, if we run that, and then look at a summary of our new model, we can see Indeed,
55:29
it is an exact replica in terms of its architecture as the original model up here that we had
55:37
previously saved to disk. Also, we can look at the weights of the new model, we didn't look at the weights ahead
55:44
of time to be able to compare them directly. But this is showing you that you can inspect the weights to look and comparatively see
55:52
that the weights are actually the same as the previous models weights. If you were to have taken a look at those beforehand.
56:00
We can also look at the optimizer just to show you that although we never set an optimizer
56:05
explicitly for our new model, because we are loading it from the saved model, it does indeed
56:12
use the atom optimizer that we set a while back whenever we compiled our model for training.
56:18
Alright, so that's it for the first saving and loading option. And again, that is the most comprehensive option to save and load everything about a
56:27
particular model. The next option that we'll look at is using this function called to JSON.
56:33
So we call model.to. json, if we only need to save the architecture of the model.
56:38
So we don't want to set its weights or the training configuration, then we can just save
56:43
the model architecture by saving it to a JSON string.
56:48
So I'm using this example here, creating a variable called JSON string, setting it equal
56:54
to model.to. json. And remember, model is our original model that we've been working with so far up to
57:01
this point. So we call to JSON. And now if we print out this JSON string, then we can see we get this string of details
57:13
about the model architecture. So it's a sequential model. And then it's got the layers organized, with the individual dense layers and all the details
57:22
about those specific layers from our original model. Now, if at a later point, we want to create a new model with our older models, architecture,
57:34
then if we save it to a JSON string, then we can import the model from JSON function
57:40
from TensorFlow Kerris dot models. And now we're creating a new variable called model architecture.
57:47
And we are loading in the JSON string using the model from JSON function.
57:53
So now we have this new model, which I'm just calling model architecture.
57:59
And if we look at the summary of that, then again, we can see that this is identical to
58:05
the summary of the original model. So we have a new model in place now, but we only have the architecture in place.
58:13
So you would have to retrain it to update its weights. And we would need to compile it to get an optimizer and our loss and everything like
58:20
that defined, this only creates the model from an architecture standpoint. Before moving on to our third option, I just wanted to mention a brief point that we can
58:30
go through this same exact process, but using a YAML string instead of a JSON string.
58:37
So the function to create a gamble string is just model.to a gamble instead of to JSON.
58:45
And then the function to load a YAML string is model from a gamma model from a YAML instead
58:53
of model from JSON. Alright, so our next option to save a model is actually just to save the weights of the
59:01
model. So if you only need to save the weights, and you don't need to save the architecture, nor any of the training configurations, like the optimizer or loss, then we can save solely
59:11
models weights by using the Save weights function. So to do that, we just call model dot save weights.
59:20
And this looks exactly the same as when we called model dot save, we're just passing a path on disk to where to save our model along with the file name ending with an H
59:30
five extension. So I'm calling this my model weights dot h five. And again, we have this condition here where I'm just checking if this H five file has
59:39
already been saved to disk, otherwise, I'm not going to keep saving it over and over again. Now, the thing with this is that when we save only the weights, if we want to load them
59:48
at a later time, then we don't have a model already in place because we didn't save the
59:54
model itself. We only save the weights. So to be able to bring in our weights to a new model then we would then need to create
1:00:01
a second model at that point with the same architecture. And then we could load the weights.
1:00:07
And so that's what we're doing in this cell. I'm defining this model called model two. And it is the exact same model from an architecture standpoint as the first model.
1:00:17
So if we run this, then at that point, we have the option to load weights into this
1:00:22
model. And the shape of these weights is going to have to match the shape of what this model
1:00:29
architecture is essentially. So we couldn't have a model with five layers here to find, for example, and load these
1:00:35
weights in because there wouldn't be a direct mapping of where these particular weights should be loaded.
1:00:41
So that's why we have the same exact architecture here as our original model. So to load our weights, we call load weights.
1:00:48
And we point to the place on disk where our weights are saved. And then we can call get weights on our new model, and see that our new model has been
1:00:58
populated with weights from our original model. Alright, so now you know all the ways that we can say various aspects of a karass sequential
1:01:08
model. Be sure to check out the blog and other resources available for this episode on people's are.com
1:01:15
as well as the deep lizard hive mind where you can gain access to exclusive perks and
1:01:20
rewards. Thanks for contributing to collective intelligence. Now, let's move on to the next episode.
Image Preparation for CNNs with TensorFlow's Keras API
1:01:28
Hey, I may be from people's or, in this episode, we'll go through all the necessary image preparation
1:01:35
and processing steps needed to train our first convolutional neural network.
1:01:46
Our goal over the next few episodes will be to build and train a convolutional neural network that can classify images as cats or dogs, the first thing that we need to do is
1:01:56
get and prepare our data set for which we'll be training our model. We're going to work with the data set from the kaggle cats versus dogs competition.
1:02:05
And you can find a link to download the dataset in the corresponding blog for this episode on people's are.com.
1:02:11
So we're mostly going to organize our data on this programmatically. But there's a couple of manual steps that we'll go through first.
1:02:18
So after you've downloaded the data set from kaggle, you will have this zip folder here.
1:02:24
And if we look inside, then this is the contents we have a zipped train folder, and a zipped
1:02:30
test folder along with the sample submission CSV. So we actually are not going to be working with this, or this.
1:02:38
So you can delete the test that as well as the CSV file, we're going to only be working
1:02:43
with this train zip. So we want to extract the high level cats, or dogs versus cats at first, and then extract
1:02:52
the train zip. So because that takes a while I went ahead and did that here.
1:02:58
So now I have just this train directory because I moved the test directory elsewhere and deleted
1:03:04
the CSV file. So now I have this extracted train folder.
1:03:10
And I go in here and I have a nested train folder. As you can see, I'm already in train once.
1:03:15
Now I'm in train again. And in here we have all of these images of cats, and dogs.
1:03:22
Okay, so that is the way that the data will come downloaded. The first step, or the next step that we need to do now is to come into here and grab all
1:03:33
of these images. And we're going to Ctrl x or cut all of these and bring them up to the first train directory.
1:03:43
So we don't want this nested directory structure. Instead, we're going to place them directly within the first train directory.
1:03:51
Alright, now all of our images have been copied into our base train directory here.
1:03:56
So this nested train directory that the images previously belong to is now empty. So we can just go ahead and delete this one.
1:04:03
Alright, so now our directory structure is just this dogs vs. Cat top directory within
1:04:13
we have a train directory. And then we have all of the images within our train directory.
1:04:19
Both of cats and dogs. The last step is to just move the dogs versus cats directory that has all the data in it
1:04:27
to be in the place on disk where you're going to be working. So for me, I am in relative to my Jupyter Notebook, I am within a directory called data
1:04:38
and I have placed dogs versus cats here. Alright, so that's it for the manual labor now everything else that will do to organize
1:04:45
the data, and then later process the data will be programmatically through code. Alright, so we're now here within our Jupyter Notebook.
1:04:52
And First things first, we need to import all of the packages that we'll be making use of and all of the packages here are not just This specific episode on processing the image
1:05:02
data. But actually, these are all of the packages that we'll be making use of over the next
1:05:07
several episodes as we're working with cn ns. Alright, so we'll get that taken care of.
1:05:13
And now, just this cell here is making sure if we are using a GPU that TensorFlow is able
1:05:19
to identify it correctly, and we are an enabling memory growth on the GPU as well.
1:05:24
If you're not using a GPU, then no worries, as mentioned earlier, you are completely fine to follow this course with a CPU only.
1:05:31
Alright, so now we're going to pick back up with organizing our data on disk. So assuming that we've gone through the first steps from the beginning of this episode,
1:05:41
we're now going to organize the data and to train valid and test ders, which correspond
1:05:47
to our training validation and test sets. So this group here is going to first change directories into our dog vs. Cat directory.
1:05:56
And then it's going to check to make sure that the directory structure that we're about to make does not already exist.
1:06:03
And if it doesn't, it's going to proceed with the rest of this script. So the first thing that it's doing as long as the directory structure is not already
1:06:11
in place, is making the following directories. So we already have a train directory.
1:06:18
So it's going to make a nested dog and cat directory within train. And then additionally is going to make a valid directory that contains dog and cat directories,
1:06:29
and a test directory, which also contains dog and cat directories. Now, this particular data set contains 25,000 images of dogs and cats.
1:06:40
And that is pretty much overkill for the tasks that we will be using these images for.
1:06:46
In the upcoming episodes, we're actually only going to use a small subset of this data,
1:06:52
you're free to work with all the data if you'd like. But it would take a lot longer to train our networks and work with the data in general
1:06:58
if we were using the entire set of it. So we are going to be working with a subset consisting of 1000 images in our training
1:07:07
set 200 in our validation set, and 100 in our test set, and each of those sets are going
1:07:14
to be split evenly among cats and dogs. So that's exactly what this block of code here is doing.
1:07:21
It's going into the images that are in our dogs vs cat directory, and moving 500 randomly
1:07:29
moving 500 cat images into our train cat directory 500 dog images into our train dog directory.
1:07:38
And then similarly doing the same thing for our valid, our valid our validation set, both
1:07:44
for cat and dogs and then our test set, both for cat and dogs with just these quantities
1:07:50
differing regarding the amounts that I stated earlier for each of the sets. And we're able to understand which images are cats and which are dogs based on the names
1:08:00
of the files. So if you saw earlier, the cat images actually had the word cat in the file names and then
1:08:07
the dog images had the word dog in the file names. So that's how we're able to select dog images and cat images here with this script.
1:08:15
Alright, so after this script runs, we can pull up our file explorer and look at the
1:08:22
directory structure and make sure it is what we expect. So we have our dogs vs cat directory within our data directory here.
1:08:29
So if we enter, then we have test train and valid directories.
1:08:34
Inside test, we have cat that has cat images, and inside dog, we have dog images.
1:08:42
If we back out and go to train, we can see similarly.
1:08:47
And if we go into valid we can see similarly. And you can select one of the folders and look at the properties to to see how many
1:08:56
files exist within the directory to make sure that is it is the amount that we chose to
1:09:01
put in from our script and that we didn't accidentally make any type of error. So if we go back to the dogs vs cats the root directory here, you can see we have all of
1:09:12
these cat and dog images leftover. These were the remaining 23,000 or so that were left over after we moved our subset into
1:09:23
our train valid and test directories. So you're free to make use of these in any way that you want or delete them or move them
1:09:30
to another location. All of what we'll be working with are in these three directories here.
1:09:35
Alright, so at this point, we have obtained the data and we have organized the data. Now it's time to move on to processing the data.
1:09:44
So if we scroll down, first, we are just creating these variables here where we have assigned
1:09:51
our train valid and test paths. So this is just pointing to the location on disk where our different data sets reside.
1:10:00
Now recall earlier in the course, we talked about that whenever we train a model that we need to put the data into a format that the model expects.
1:10:08
And we know that when we train a karass sequential model, that the model receives the data whenever
1:10:14
we call the fit function. So we are going to put our images and do the format of a karass generator.
1:10:21
And we're doing that in this cell here. We're creating these train valid and test batches, and setting them equal to image data
1:10:31
generator dot flow from directory, which is going to return a directory iterator.
1:10:38
Basically, it's going to create batches of data from the directories where our datasets
1:10:44
reside. And these batches of data will be able to be passed to the sequential model using the
1:10:51
fit function. So now let's look exactly at how we are defining these variables.
1:10:56
So let's focus just on train batches for now. So we're setting train batches equal to image data generator dot flow from directory.
1:11:05
But first to image data generator, we are specifying this pre processing function and
1:11:11
setting that equal to tf Kerris dot applications that VGG 16 dot pre process input.
1:11:18
So I'm just going to tell you for now that this is a function that is going to apply
1:11:24
some type of pre processing on the images before they get passed to the network that
1:11:30
we'll be using. And we're processing them in such a way that is equivalent to the way that a very popular
1:11:38
model known as VGG 16, we're processing our images in the same format as which images
1:11:44
that get passed to this VGG 16 model our process. And we're going to talk about more about this in a future episode.
1:11:51
So don't let it confuse you now just know that this is causing some type of processing
1:11:57
to occur on our images. And we'll talk more about it in a future episode and not stress on it now, because it's not
1:12:02
necessarily very important for us right at this moment, the technical details of that at least.
1:12:08
So besides that, when we call flow from directory, this is where we are passing in our actual
1:12:14
data and specifying how we want this data to be processed. So we are setting directory equal to train path, which appear we defined the location
1:12:26
on disk where our training set was under the train PATH variable. And then we're setting target size equal to 224 by 224.
1:12:35
So this is the height and width that we want the cat and dog images to be resized to.
1:12:41
So if you're working with an image data set that has images of varying sizes, or you just
1:12:46
want to scale them up or scale them down, this is how you can specify that to happen. And this will resize all images in your data set to be of this height and width before
1:12:56
passing them to our network. Now we are specifying our classes, which are just the classes for the potential labels
1:13:05
of our data set. So cat or dog, and we are setting our batch size to 10.
1:13:12
We do the exact same thing for the validation set. And the test set.
1:13:18
Everything is the exact same for both of them, except for where each of these sets live on disk as being specified here under the directory parameter.
1:13:30
And then the only other difference is here for our test batches. We are specifying this shuffle equals false parameter.
1:13:38
Now, this is because whenever we use our test batches later for inference to get our model
1:13:44
to predict on images of cats and dogs after training and validation has been completed,
1:13:50
we're going to want to look at our prediction results in a confusion matrix like we did in a previous video for a separate data set.
1:13:56
And in order to do that, we need to be able to access the unsettled labels for our test
1:14:02
set. So that's why we set shuffle equals false for only this set. For both validation and training sets, we do want the data sets to be shuffled.
1:14:11
Alright, so we run this and we get the output of found 1000 images belonging to two classes.
1:14:19
And that is corresponding to our train batches found at 200 images belonging to two classes,
1:14:25
which corresponds to valid valid batches, and then the 100 belonging to two classes
1:14:31
corresponding to our test batches. So that is the output that you want to see for yourself. That's letting you know that it found the images on disk that belong to both the cat
1:14:40
and dog classes that you have specified here. So if you are not getting this at this point, if you get found at zero images, then perhaps
1:14:48
you're pointing to the wrong place on disk. You just need to make sure that it's able to find all the images that you set up previously.
1:14:55
Right and here we are just verifying that that is indeed the case. Now Next, we are going to just grab a single batch of images and the corresponding labels
1:15:05
from our train batches. And remember, our batch size is 10.
1:15:11
So this should be 10 images along with the 10 corresponding labels.
1:15:16
Next, we're introducing this function plot images that we're going to use to plot the
1:15:22
images from our train batches that we just obtained above. And this function is directly from tensor flows website.
1:15:28
So check the link in the corresponding blog for this episode on people's or.com. To see, to be able to get to the tensor flows site where exactly I pulled this off of.
1:15:40
So we will define this function here. Alright, so now we're just going to use this function to plot our images from our test
1:15:48
batches here. And we're going to print the corresponding labels for those images.
1:15:54
So if we scroll down, we can see this is what a batch of training data looks like.
1:16:01
So this might be a little bit different than what you expected, given the fact that it looks like the color data has been a little bit distorted.
1:16:09
And that's due to the pre processing function that we called to pre process the images in
1:16:14
such a way that in the same type of way that images get pre processed for the famous VGG
1:16:20
16 model. So like I said, we're going to discuss in detail what exactly that pre processing function
1:16:26
is doing technically, as well as why we're using it in a later video. But for now, just know that it's skewing the RGB data in some way.
1:16:36
So we can still make out the fact that this is a cat. And this looks like a cat.
1:16:42
This is the dog, dog, dog, dog cat. Yeah, so we can still kind of generally make out what these images are, but the color data
1:16:50
is skewed. But don't worry too much about the technical details behind that. For right now, just know that this is what the data looks like before we pass it to the
1:16:58
model. And here are the corresponding labels for the data. So we have these one hot encoded vectors that represent either cat or dog.
1:17:09
So a one zero represents a cat, and a zero, a one represents a dog.
1:17:17
Okay, so I guess I was wrong earlier with thinking that this one was a dog. This one is a cat, because as we can see, it maps to the 101 hot encoding.
1:17:29
And if you don't know what I mean by one hot encoding, then check out the corresponding video for that in the deep learning fundamentals course on depot's or.com.
1:17:36
But yeah, we can see that 01 is the vector used to represent the label of a dog.
1:17:42
So this one is a dog. And the next two are dogs as well, this one and this one.
1:17:48
Now, just a quick note about everything that we've discussed up to this point, sometimes we do not have the corresponding labels for our test set.
1:17:57
So in the examples that we've done so far, in this course, we've always had the corresponding
1:18:02
labels for our test set. But in practice, a lot of times you may not have those labels. And in fact, if we were to have used the downloaded test directory that came from the kaggle download,
1:18:14
then we would see that that test directory does not have the images labeled with the
1:18:19
cat or dog. So in this case, we do have the test labels for the cat and dog images since we pulled
1:18:27
them from the original training set from kaggle that did have the corresponding labels.
1:18:32
But if you don't have access to the test labels, and you are wondering how to process your
1:18:38
test data accordingly, then check the blog for this episode on dibbles comm I have a section there that demonstrates what you need to do differently from what we showed in this
1:18:47
video if you do not have access to the labels for your test set. Alright, so now we have obtained our image data, organized it on disk and processed it
1:18:58
accordingly for our convolutional neural network. So now in the next episode, we are going to get set up to start building and training
1:19:05
our first CNN. Be sure to check out the blog and other resources available for this episode on the poser.com
1:19:13
as well as the deep lizard hive mind where you can gain access to exclusive perks and
1:19:18
rewards. Thanks for contributing to collective intelligence. Now let's move on to the next episode.
Build and Train a CNN with TensorFlow's Keras API
1:19:26
Hey, I'm Andy from deep lizard. And this episode will demonstrate how to build a convolutional neural network and then train
1:19:34
it on images of cats and dogs using tensor flows integrated Kerris API.
1:19:41
We'll be continuing to work with the cat and dog image data that we created in the last
1:19:51
episode. So make sure you still have all of that in place, as well as the imports that we brought
1:19:56
in in the last episode as well as we'll be making use of those imports. For the next several videos of working with CNN, so to create our first CNN, we will be
1:20:04
making use of the Kerris sequential model. And recall, we introduced this model in an earlier episode when we were working with
1:20:12
just plain simple numerical data. But we'll continue to work with this model here with our first CNN.
1:20:18
So the first layer to this model that we pass is a comma to D layer. So this is just our standard convolutional layer that will accept image data.
1:20:28
And to this layer, we're arbitrarily setting the filter value equal to 32.
1:20:33
So this first convolutional layer will have 32 filters with a kernel size of three by
1:20:39
three. So the choice of 32 is pretty arbitrary, but the kernel size of three by three is a very
1:20:45
common choice for image data. Now, this first column to D layer will be followed by the popular relu activation function.
1:20:54
And we are specifying padding equals Same here. And this just means that our images will have zero padding, padding the outside so that
1:21:03
the dimensionality of the images aren't reduced after the convolution operations.
1:21:08
So lastly, for our first layer only, we specify the input shape of the data.
1:21:15
And recall we touched on this parameter previously, you can think of this as kind of creating an implicit input layer for our model.
1:21:23
This comp 2d is actually our first hidden layer, the input layer is made up of the input
1:21:28
data itself. And so we just need to tell the model the shape of the input data, which in our case
1:21:35
is going to be 224. By 224. Recall, we saw that we were setting that target size parameter to do 24 by 224.
1:21:44
Whenever we created our valid test and train directory iterators, we said that we wanted
1:21:51
our images to be of this height and width. And then this three here is regarding the color channels.
1:21:57
Since these. Since these images are an RGB format, we have three color channels, so we specified this
1:22:04
input shape. So then we follow our first convolutional layer with a max pooling layer, where we're
1:22:11
setting our pool size to two by two and our strides by two. And if you are familiar with Max pooling, then you know this is going to cut our image
1:22:20
dimensions in half. So if you need to know more about max pooling, or you just need a refresher, same thing with
1:22:26
padding here for zero padding, activation functions, anything like that, then be sure to check those episodes out and the corresponding deep learning fundamentals course on depot's
1:22:36
or.com. So after this max pooling layer, we're then adding a another convolutional layer, that
1:22:41
looks pretty much exactly the same as the first one except for four, we're not including the input shape parameter, since we only specify that for our first hidden layer, and we are
1:22:52
specifying the filters to be 64 here instead of 32. So 64 is again, an arbitrary choice, I just chose that number here.
1:23:01
But the general rule of increasing functions as you go until later layers of the network
1:23:08
is common practice, we then follow this second convolutional layer with another max pooling
1:23:13
layer identical to the first one, then we flatten all of this into a one dimensional
1:23:19
tensor before passing it to our dense output layer, which only has two nodes corresponding
1:23:26
to cat and dog. And our output layer is being followed by the softmax after activation function, which
1:23:34
as you know, is going to give us probabilities for each corresponding output from the model.
1:23:40
Alright, so if we run this, we can then check out the summary of our model.
1:23:46
And this is what we have exactly what we built, along with some additional details about the
1:23:52
learnable parameters and the output shape of the network. So these are things that are also covered in the in the deep learning fundamentals course
1:24:01
as well, if you want to check out more information about learnable parameters and such. So now that our model is built, we can prepare it for training by calling model dot compile,
1:24:10
which again, we have already used in a previous episode, when we were just training on numerical
1:24:16
data. But as you will see here, it looks pretty much the same as before, we are setting our
1:24:22
optimizer equal to the atom optimizer with a learning rate of 0.0001.
1:24:28
And we are using categorical cross entropy. And we are looking at the accuracy as our metrics to be able to judge the model performance.
1:24:38
Just a quick note before moving on. We are using categorical cross entropy.
1:24:43
But since we have just two outputs from our model, either cat or dog, then it is possible
1:24:49
to instead use binary cross entropy. And if we did that, then we would need to just have a one single output node from our
1:24:58
model instead of two And then rather than following our output layer with the softmax activation function, we would need to follow it with sigmoid.
1:25:07
And both of these approaches using categorical cross entropy loss with the setup that we have here, or using binary cross entropy loss with the setup that I just described, work
1:25:17
equally well, they're totally equivalent, and will yield the same results, categorical cross entropy, and using softmax on the app, as the activation function for the output
1:25:28
layer, or just a common approach for when you have more than two classes.
1:25:33
So I like to continue using that approach, even when I only have two classes just because it's general.
1:25:38
And it's the case that we're going to use whenever there's more than two classes anyway, so I just like to stick with using the same type of setup, even when we only have two
1:25:46
outputs. Alright, so after compiling our model, we can now train it using model dot fit, which
1:25:52
we should be very familiar with up to this point. So to the fit function, we are first specifying our training data, which is stored in train
1:26:00
batches. And then we specify our validation data, which is stored in valid batches.
1:26:07
Recall, this is a different way of creating validation data, as we spoke about in an earlier
1:26:13
episode, we're not using validation split here, because we've actually created a validation
1:26:19
set separately ourselves before fitting the model. So we are specifying that separated set here as the validation data parameter, then we
1:26:28
are setting our epochs equal to 10. So we're only going to train for 10 runs this time and setting verbose, verbose equal to
1:26:36
two so that we can see the most verbose output during training. And one other thing to mention is that you will see here that we are specifying x just
1:26:46
as we have in the past, but we are not specifying y, which is our target data usually.
1:26:52
And that's because when data is stored as a generator, as we have here, the the generator
1:26:59
itself actually contains the corresponding labels. So we do not need to specify them separately, whenever we call fit, because they're actually
1:27:07
contained within the generator itself. So let's go ahead and run this now. Alright, so the model just finished training.
1:27:14
So let's check out the results. Before we do just if you get this warning, it appears from the research I've done to
1:27:20
be a bug within TensorFlow that's supposed to be fixed in a in the next release actually,
1:27:25
is what I read. So you can just safely ignore this warning, it has no impact on our training.
1:27:31
But if we scroll down and look at these results, then we can see that by our 10th epoch, our
1:27:38
accuracy on our training set has reached 100%. So that is great. But our validation accuracy is here at 69%.
1:27:47
So not so great, we definitely see that we have some overfitting going on here.
1:27:53
So if this was a model that we really cared about, and that we really wanted to use and be able to deploy to production, then in this scenario, we will need to stop what we're
1:28:01
doing and combat this overfitting problem before going further. But what we are going to do is that we'll be seeing in an upcoming episode, how we can
1:28:10
use a pre trained model to perform really well on this data. And that'll get us exposed to the concept of fine tuning.
1:28:19
Before we do that, though, we are going to see in the next episode how this model holds up to inference at predicting on images in our test set.
1:28:28
Be sure to check out the blog and other resources available for this episode on the poser.com
1:28:33
as well as the deep lizard hive mind where you can gain access to exclusive perks and
1:28:39
rewards. Thanks for contributing to collective intelligence. Now let's move on to the next episode.
CNN Predictions with TensorFlow's Keras API
1:28:47
Hey, I may be from deep lizard. And this episode will demonstrate how to use a convolutional neural network for inference
1:28:55
to predict on image data using tensor flows integrated keras API.
1:29:06
Last time, we built and trained our first cnn against cat and dog image data, and we
1:29:12
saw that the training results were great with the model achieving 100% accuracy on the training
1:29:17
set. However, it lagged behind by quite a good bit at only 70% accuracy on our validation
1:29:24
set. So that tells us that the model wasn't generalizing as well as we hoped.
1:29:29
But nonetheless, we are going to use our model now for inference to predict on cat and dog
1:29:35
images in our test set. Given the less than decent results that we saw from the validation performance.
1:29:41
Our expectation is that the model is not going to do so well on the test set either it's
1:29:46
probably going to perform at around the same 70% rate. But this is still going to give us exposure about how we can use a CNN for inference using
1:29:56
the Kerris sequential API. Alright, so we are back in Jupyter Notebook and we need to make sure that we have all
1:30:03
the code in place from the last couple of episodes as we will be continuing to make
1:30:08
use of both our model that we built last time, as well as our test data from whenever we
1:30:13
prepared the data sets. So the first thing we're going to do is get a batch of test data from our test batches.
1:30:23
And then we're going to plot that batch, we're going to plot the images specifically, and then we're going to print out the corresponding labels for those images.
1:30:31
And we're using before we do that just a reminder, this plot images function that we introduced
1:30:37
in the last couple of episodes. Alright, so if we scroll down, we have our test batches.
1:30:44
Recall, we had the discussion about why the image data looks the way that it does in terms of the color being skewed last time, but we can see that just by looking even though we
1:30:54
have kind of distorted color we have, these are all actually cats here.
1:31:00
And by looking at the corresponding label for these images, we can see that they are
1:31:05
all labeled with the one hot encoded vector of one zero, which we know is the label for
1:31:11
cat. So if you're wondering why we have all cats as our first 10 images in our first batch
1:31:18
here, that is because we're call whenever we created the test set, we specified that
1:31:24
we did not want it to be shuffled. And that was so that we could do the following. If we come to our next cell, and we run test batches classes, then we can get a array that
1:31:35
has all of the corresponding labels for each image in the test set.
1:31:41
So given that we have access to the ns shuffled labels for the test set, that's why we don't
1:31:48
want to shuffle the test set directly, because we want to be able to have this one one direct
1:31:54
mapping from the uncoupled labels to the test data set.
1:32:00
And if we were to shuffle the test data set, every time we generated a batch, then we wouldn't be able to have the correct mapping between labels and samples.
1:32:09
So we care about having the correct mapping, because later after we get our predictions from the model, we're going to want to plot our predictions to a confusion matrix.
1:32:19
And so we want the corresponding labels that belong to the samples in the test set.
1:32:25
Alright, so next, we're actually going to go ahead and obtain our predictions by calling
1:32:30
model dot predict just as we have an earlier episodes for other data sets.
1:32:36
And to x, we are specifying our test batches, so all of our test data set, and we are choosing
1:32:44
verbose to be zero to get no output whenever we run our predictions.
1:32:50
So here, we are just printing out the arounded predictions from the model. So the way that we can read this is first, each one of these arrays is a prediction for
1:33:02
a single sample. So if we just look at the first one, this is the prediction for the first sample and
1:33:10
the test set. So this one, wherever there is a one with each prediction is the one is the index for
1:33:17
the output class that had the highest probability from the model. So in this case, we see that the zeroeth index had the highest probability.
1:33:28
So we can just say that the label that the model predicted for this first sample was
1:33:34
a zero, because we see that there is a one here in the zeroeth index.
1:33:39
And if we look at the first element, or if we look at the first label here, up here,
1:33:46
for our first test sample, it is indeed zero. So we can eyeball that and see that the model did accurately predict all the way down to
1:33:57
here, because that's the first 123456 and 123456. Okay, so But then, whenever we see that the model predicted the first index to be the
1:34:10
highest probability, that means that the, that the model predicted an output label of a one, and so that corresponds to dog.
1:34:19
So it's hard for us to kind of draw an overall conclusion about the prediction accuracy for
1:34:25
this test set, just eyeballing the results like this. But if we scroll down, then we know that we have the tool of a confusion matrix that we
1:34:36
can use to make visualizing these results much easier, like we've seen in previous episodes
1:34:41
of this course already. So we are going to do that. Now we're going to create a confusion matrix using this confusion matrix function from
1:34:49
scikit learn which we've already been introduced to and we are passing in our true labels using
1:34:56
test batches classes. Recall that we just touched on that a few minute ago. And for our predictive labels, we are passing in the predictions from our model, we are
1:35:07
getting the we're actually passing in the index of each.
1:35:13
We're actually using arg max to pass in the index of where the most probable prediction
1:35:19
was from our predictions list. So this is something that we've already covered in previous episodes for why we do that.
1:35:26
So if we run that, we're now going to bring in this plot confusion matrix, which we've
1:35:32
discussed is directly from psychic Lauren's website, link to that is, and the corresponding
1:35:37
blog for this episode on people's are calm. This is just going to allow us to plot our confusion matrix in a moment.
1:35:44
And now if we look at the class indices, we see that cat is first and dog is second.
1:35:51
So we just need to look at that so that we understand in which order, we should put our plot labels for our confusion matrix.
1:35:59
And next we call plot confusion matrix and pass in the confusion matrix itself, as well
1:36:06
as the labels for the confusion matrix and a title for the entire matrix.
1:36:11
So let's check that out. Alright, so from what we learned about how we can easily interpret a confusion matrix,
1:36:18
we know that we can just look at this diagonal here running from top left to bottom right, to see what the model predicted correctly.
1:36:26
So not that great, the model is definitely overfitting at this point. So like I said, if this was a model that we were really concerned about, then we would
1:36:34
definitely want to combat that overfitting problem. But for now, we are going to move on to a new model using a pre trained state of the
1:36:42
art model called VGG 16. In the next episode, so that we can see how well that model does on classifying images
1:36:49
of cats and dogs. Be sure to check out the blog and other resources available for this episode on the poser.com
1:36:56
as well as the deep lizard hive mind where you can gain access to exclusive perks and
1:37:02
rewards. Thanks for contributing to collective intelligence. Now let's move on to the next episode.
Build a Fine-Tuned Neural Network with TensorFlow's Keras API
1:37:11
Hey, I'm Andy from be blizzard. And this episode will demonstrate how we can fine tune a pre train model to classify images
1:37:19
using tensor flows keras API.
1:37:28
The pre train model that we'll be working with is called VGG 16. And this is the model that won the 2014 image net competition.
1:37:36
In the image net competition, multiple teams compete against each other to build a model that best classifies images within the image net library.
1:37:46
And the image net library is made up of 1000s of images that belong to 1000 different classes
1:37:53
using Kerris will import this VGG 16 model, and then fine tune it to not classify on one
1:38:00
of the 1000 categories for which it was originally trained, but instead only on two categories,
1:38:07
cat and dog. Note, however, that cats and dogs were included in the original image net library for which
1:38:14
VGG 16 was trained on. And because of this, we won't have to do much tuning to change the model from classifying
1:38:23
from 1000 classes to just the two cat and dog classes. So the overall fine tuning that we'll do will be very minimal.
1:38:30
in later episodes, though, we'll do more involved fine tuning and use transfer learning to transfer
1:38:36
what a model has learned on an original data set to completely new data and a new custom
1:38:42
data set that we'll be using later. To understand fine tuning and transfer learning at a fundamental level, check out the corresponding
1:38:49
episode for fine tuning in the deep learning fundamentals course on V blizzard.com. before
1:38:54
we actually start building our model, let's quickly talk about the VGG 16 pre processing
1:39:00
that is done. Now recall we are here in our Jupyter Notebook. This is from last time when we plotted a batch from our test data set.
1:39:08
And a few episodes ago, we discussed the fact that this color data was skewed as a result of the VGG 16 pre processing function that we were calling.
1:39:17
Well, now we're going to discuss what exactly this pre processing does. So as we can see, we can still make out the images as these here being all cats, but it's
1:39:27
just the data the color data itself that appears to be distorted. So if we look at the paper that the authors of VGG 16 detailed the model in under the
1:39:39
architecture section, let's blow this up a bit, we can see that they state that the only
1:39:46
pre processing we do is subtract the mean RGB value computing on the trait computed
1:39:51
on the training set from each pixel. So what does that mean? That means that they computed the mean red value pixel for All of the training data.
1:40:02
And then once I had that mean value across each image in the training set, they subtracted
1:40:09
that mean value from the red value, the red pixel, each pixel in the image.
1:40:15
And then they did the same thing for the green and blue pixel values as well. So they found the green picks the Mean Green pixel value among the entire training set.
1:40:26
And then for each sample in the training set, every green pixel, they subtracted that green
1:40:32
value from it. So and same thing for the blue, of course. So that's what they did for the pre processing.
1:40:39
So given that, that's how VGG 16 was originally trained. That means now whenever new data is passed to the model, that it needs to be processed
1:40:48
in the same exact way as the original training set. So Kerris already has functions built in for popular models, like VGG 16, where they have
1:40:58
that pre processing in place that matches for the corresponding model. So that's why we were calling that model whenever we process our cat and dog images earlier,
1:41:08
so that we could go ahead and get those images processed in such a way that matched how the original training set was processed, when VGG was originally trained.
1:41:18
Alright, so that is what that color distortion is all about. So now let's jump back into the code.
1:41:24
Now that we know what that's about, we have an understanding of the pre processing. Now, Let's now get back to actually building the fine tuned model.
1:41:33
So the first thing that we need to do is download the model. And when you call this for the first time, you will need an internet connection because
1:41:40
it's going to be downloading the model from the internet. But after this, subsequent calls to this function will just be grabbing this model from the
1:41:49
download a copy on your machine. Alright, so the model has been downloaded it now we are just running this summary.
1:41:57
And we can see that this model is much more complex than what we have worked with up to
1:42:02
this point. So total, there are almost 140 million parameters in this model.
1:42:09
And on disk, it is over 500 megabytes. So it is quite large.
1:42:15
Now recall I said that this VGG 16 model originally was predicting for 1000 different image net
1:42:23
classes. so here we can see our output layer of the VGG 16 model has a 1000 different outputs.
1:42:31
So our objective is going to simply be to change this last output layer to predict only
1:42:39
two output classes corresponding to cat and dog, along with a couple other details regarding
1:42:44
the fine tuning process that we'll get to in just a moment. So now, we're actually going to just skip these two cells here.
1:42:51
These are just for me to be able to make sure that I've imported the model correctly.
1:42:57
But it is not relevant for any code here. It's just checking that the trainable parameters.
1:43:02
And non trainable parameters are what I expect them to be after importing. But this is not of concern at the moment for us.
1:43:10
So if we scroll down here, we're now going to build a new model called sequential.
1:43:16
Alright, now before we run this code, we're actually just going to look at the type of
1:43:23
model this is. So this is returning a model called model.
1:43:31
So this is actually a model from the Kerris functional API. We have been previously working with sequential models.
1:43:40
So we are in a later episode going to discuss the functional API in depth, it's a bit more
1:43:46
complicated and more sophisticated than the sequential model. For now, since we're not ready to bring in the functional model yet, we are going to
1:43:55
convert the original VGG 16 model into a sequential model.
1:44:01
And we're going to do that by creating a new variable called model here and setting this
1:44:07
equal to an instance of sequential object. And we are going to then loop through every layer and VGG 16.
1:44:18
Except for the last output layer, we're leaving that out, we're going to loop through every layer and then add each layer into our new sequential model.
1:44:28
So now we'll look at a summary of our new model. And by looking at this summary, if you take the time to compare the previous summary to
1:44:37
this summary, what you will notice is that they are exactly the same except for the last
1:44:44
layer has been not included in this new model. So this layer here we have this fully connected two layer is what this is here, with the output
1:44:56
shape of 4096. here if we scroll Back up, we can see that this is this layer here.
1:45:04
So the predictions layer has not been included, because when we were iterating over our for loop, we went all the way up to the second last layer, we did not include the last layer
1:45:14
of VGG 16. Alright, so now let's scroll back down.
1:45:19
And we're now going to iterate over all of the layers within our new sequential model.
1:45:27
And we are going to set each of these layers to not be trainable by setting layer dot trainable
1:45:34
to false. And what this is going to do is going to freeze the trainable parameters, or the weights and
1:45:41
biases from all the layers in the model so that they're not going to be retrained.
1:45:46
Whenever we go through the training process for cats and dogs, because VGG 16 has already
1:45:52
learned the features of cats and dogs and its original training, we don't want it to have to go through more training again it since it's already learned those features.
1:45:59
So that's why we are freezing the weights here, we're now going to add our own output
1:46:06
layer to this model. So remember, we removed the previous output layer that had 10 output or that had 1000
1:46:12
output classes rather. And now we are going to add our own output layer that has only two output classes for
1:46:20
cat and dog. So we add that now since we have set all of the previous layers to not be trainable, we
1:46:29
can see that actually only our last output layer that's going to be the only trainable layer in the entire model.
1:46:37
And like I said before, that's because we already know that VGG 16 has already learned
1:46:42
the features of cats and dogs during its original training. So we only need to retrain this output layer to classify two output classes.
1:46:50
So now if we look at a new summary of our model, then we'll see that everything is the same, except for now we have this new dense layer as our output layer, which only has
1:47:00
two classes, instead of 1000. From the original VGG 16 model, we can also see that our model now only has 8000 trainable
1:47:11
parameters, and those are all within our output layer. As I said that our output layer is our only trainable layer.
1:47:19
So before actually, all of our layers were trainable. If we go take a look at our original VGG 16 model, we see that we have 138 million total
1:47:30
parameters, all of which are trainable, none of which are non trainable.
1:47:35
So if we didn't freeze those layers, then they would be getting retrained during the training process for our cat and dog images.
1:47:42
So just to scroll back down again and check this out. We can see that now we still have quite a bit of learnable parameters or a total parameters
1:47:52
134 million, but only 8000 of which are trainable. The rest are non trainable.
1:47:58
In the next episode, we'll see how we can train this modified model on our images of
1:48:04
cats and dogs. Be sure to check out the blog and other resources available for this episode on the bowser.com
1:48:11
as well as the deep lizard hive mind where you can gain access to exclusive perks and
1:48:16
rewards. Thanks for contributing to collective intelligence. Now let's move on to the next episode.
Train a Fine-Tuned Neural Network with TensorFlow's Keras API
1:48:24
Hey, I'm Mandy from Blizzard. In this episode, we'll demonstrate how to train the fine tuned VGG 16 model that we
1:48:33
built last time on our own data set of cats and dogs.
1:48:40
Alright, so we're jumping straight into the code to train our model.
1:48:46
But of course, be sure that you already have the code in place from last time as we will be building on the code that we have already run previously.
1:48:54
So, we are using our model here to first compile it to get it ready for training.
1:49:01
This model is the sequential model that we built last time that is our fine tuned VGG
1:49:06
16 model containing all the same layers with frozen weights except for our last layer which
1:49:12
we have modified to output only two possible outputs. So, we are compiling this model using the atom optimizer as we have previously used
1:49:23
with the learning rate of 0.0001 and a the loss we are using again categorical cross
1:49:31
entropy just like we have before and we are using the accuracy as our only metric to judge
1:49:37
model performance. So, there is nothing new here with this call to compile.
1:49:42
It is pretty much exactly the same as what we have seen for our previous models.
1:49:47
Now we are going to train the model using model dot fit and we are passing in our training
1:49:54
data set which we have stored in train batches. We are passing in our validation set Which we have stored as valid batches.
1:50:03
And we are only going to run this model for five epochs. And we are setting the verbosity level to two so that we can see the most comprehensive
1:50:12
output from the model during training. So let's see what happens. Alright, so training has just finished, and we have some pretty outstanding results.
1:50:22
So just after five epochs on our training data and validation data, we have an accuracy
1:50:29
on our training data of 99%. And validation accuracy, right on par at 98%.
1:50:35
So that's just after five epochs. And if we look at the first epoch, even our first epoch gives, it gives us a training
1:50:43
accuracy of 85%, just starting out, and a validation accuracy of 93%.
1:50:50
So this isn't totally surprising, because remember, earlier in the course, we discussed how VGG 16 had already been trained on images of cats and dogs from the image net library.
1:51:01
So it had already learned those features. Now, the flight training that we're doing on the output layer, is just to train VGG
1:51:07
16 to output only cat or dog at classes. And so it's really not surprising that it's doing such a good job right off the bat in
1:51:16
its first epoch, and even an even considerably better, and it's fifth epoch at 99% training
1:51:24
accuracy. Now we're called the previous CNN that we built from scratch ourselves the really simple
1:51:29
convolutional neural network, that model actually did really well on the training data, reaching
1:51:35
100% accuracy after a small amount of epochs as well. Where we saw it lagging though was with the validation accuracy.
1:51:43
So it had a validation accuracy of around 70%. Here we see that we are at 98%.
1:51:49
So the main recognizable difference between our very simple CNN and this VGG 16 fine tuned
1:51:57
model is how well this model generalizes to our cat and dog data in the validation set,
1:52:03
whereas the model we built from scratch did not generalize so well on data that was not included in the training set.
1:52:10
In the next episode, we're going to use this VGG 16 model for inference to predict on the
1:52:15
cat and dog images in our test set. And given the accuracy that we are seeing on the validation set here, we should expect
1:52:22
to see some really good results on our test set as well. Be sure to check out the blog and other resources available for this episode on the poser.com
1:52:31
as well as the deep lizard hive mind where you can gain access to exclusive perks and
1:52:36
rewards. Thanks for contributing to collective intelligence. Now let's move on to the next episode.
Predict with a Fine-Tuned Neural Network with TensorFlow's Keras API
1:52:44
Hey, I'm Andy from deep lizard. In this episode, we'll use our fine tuned VGG 16 model for inference to predict on images
1:52:54
in our test set. All right, we are jumping right back into our Jupyter Notebook.
1:53:06
Again, making sure that all the code is in place and has run it from the previous episodes as we will be building on the code that has been run there.
1:53:16
So first things first is that we are going to be using the model to get predictions from
1:53:21
our test set. So to do that we call model dot predict which we have been exposed to in the past.
1:53:28
Recall that this model here is our fine tuned VGG 16 model. And to predict we are passing our test set which we have stored in test batches.
1:53:38
And we are setting our verbosity level to zero as we do not want to see any output any
1:53:44
output from our predictions. Now recall Previously, we talked about how we did not shuffle the test set for our cat
1:53:53
and dog image data set. And that is because we want to be able to access the classes here and uncheck old order
1:54:00
so that we can then pass in the uncheck old classes that correspond to the old sorry,
1:54:07
the shuffled labels that correspond to the ns shuffled test data, we want to be able
1:54:13
to have those in a one to one mapping, where the labels actually are the correct and shuffled
1:54:19
labels for the unsettled data samples, we want to be able to pass those to our confusion
1:54:24
matrix. So this is the same story as we saw whenever we were using our CNN that we built from scratch
1:54:31
a few episodes back, we did the same process, we're using the same dataset recall.
1:54:37
And so now we are plotting this confusion matrix in the exact same manner as before.
1:54:42
So this is actually the exact same line that we used to plot the confusion matrix A few
1:54:47
episodes back when we plotted it on this same exact test set for the CN n that we built
1:54:53
from scratch. And just a reminder from last time recall that we looked at the class indices of the
1:54:58
test batches. So that we could get the correct order of our cat and dog classes to use for our labels
1:55:06
for our confusion matrix. So we are again doing the same thing here. And now we are calling the psychic learn plot confusion matrix which you should have defined
1:55:16
earlier in your notebook from a previous episode. And to plot confusion matrix we are passing in our confusion matrix, and our labels defined
1:55:26
just above as well as a general title for the confusion matrix itself. So now, let's check out the plot so that we can see how well our model did on these predictions.
1:55:36
So this is what the third or fourth time that we've used a confusion matrix in this course
1:55:41
so far, so you should be pretty normalized to how to read this data.
1:55:48
So recall, the quick and easy way is to look from the top left to the bottom right along this diagonal, and we can get a quick overview of how well the model did.
1:55:57
So the model correctly predicted a dog for 49 times for images that were truly dogs.
1:56:06
And it correctly predicted a cat 47 times for images that truly were cats.
1:56:13
So we can see that one time it predicted a cat when it was actually a dog. And three times it predicted a dog when images were actually cats.
1:56:23
So overall, the model incorrectly predicted four samples, so that gives us 96 out of 100.
1:56:33
Correct. Or let's see 96 correct predictions out of 100 total predictions.
1:56:39
So that gives us an accuracy rate on our test set of 96%. Not surprising given what we saw in the last episode for the high level of accuracy that
1:56:49
our model had on the validation set. So overall, this fine tuned VGG 16 model does really well at generalizing on data that it
1:56:59
had not seen during training a lot better than our original model for which we build from scratch.
1:57:04
Now recall that we previously discussed that the overall fine tuning approach that we took to this model was pretty minimal since cat and dog data was already included in the original
1:57:15
training set for the original VGG 16 model. But in upcoming episodes, we are going to be doing more fine tuning, more fine tuning
1:57:23
than what we saw here for VGG 16. As we will be fine tuning another well known pre trained model, but this time for a completely
1:57:30
new data set that was not included in the original data set that it was trained on. So stay tuned for that.
1:57:37
Be sure to check out the blog and other resources available for this episode on the poser.com
1:57:43
as well as the deep lizard hive mind where you can gain access to exclusive perks and
1:57:48
rewards. Thanks for contributing to collective intelligence. Now let's move on to the next episode.
MobileNet Image Classification with TensorFlow's Keras API
1:57:57
Hey, I'm Andy from deep lizard. And this episode we'll introduce mobile nets a class of a lightweight deep convolutional
1:58:05
neural networks that are much smaller and faster and size than many of the mainstream
1:58:10
popular models that are really well known.
1:58:17
neural nets are a class of small, low power, low latency models that can be used for things
1:58:27
like classification, detection, and other things that cn ns are typically good for.
1:58:33
And because of their small size, these models are considered great for mobile devices, hence
1:58:39
the name mobile nets. So I have some stats taken down here. So just to give a quick comparison, in regards to the size, the size of the full VGG 16 network
1:58:50
that we've worked with in the past few episodes is about 553 megabytes on disk.
1:58:56
So pretty large, generally speaking, the size of one of the currently largest mobile nets
1:59:03
is only about 17 megabytes. So that's a pretty huge difference, especially when you think about deploying a model to
1:59:10
run on a mobile app, for example, this vast size difference is due to the number of parameters
1:59:17
or weights and biases contained in the model. So for example, let's see VGG 16, as we saw previously has about 138 million total parameters.
1:59:29
So a lot. And the 17 megabytes mobile net that we talked about, which was the largest mobile net currently
1:59:37
has only about 4.2 million parameters. So that is much much smaller on a relative scale than VGG 16 with 138 million.
1:59:46
Aside from the size on disk being a consideration when it comes to comparing mobile nets to
1:59:52
other larger models. We also need to consider the memory as well.
1:59:57
So the more parameters that a model has The more space in memory it will be taking up also.
2:00:03
So while mobile nets are faster and smaller than the competitors of these big hefty models
2:00:10
like VGG 16, there is a catch or a trade off, and that trade off is accuracy.
2:00:18
So, mobile nets are not as accurate as some of these big players like VGG 16, for example,
2:00:25
but don't let that discourage you. While it is true that mobile nets aren't as accurate as these resource heavy models like
2:00:32
VGG 16, for example, the trade off is actually pretty small, with only a relatively small
2:00:38
reduction in accuracy. And in the corresponding blog for this episode, I have a link to a paper that goes more in
2:00:45
depth to this relatively small accuracy difference. If you'd like to check that out further, let's now see how we can work with mobile nets and
2:00:52
code with Kerris. Alright, so we are in our Jupyter Notebook. And the first thing we need to do is import all of the packages that we will be making
2:01:00
use of which these are not only for this video, but for the next several videos where we will
2:01:05
be covering mobilenet. And as mentioned earlier in this course, a GPU is not required. But if you're running a GPU, then you want to run this cell.
2:01:13
This is the same cell that we've seen a couple of times already in this course earlier, where we are just making sure that TensorFlow can identify our GPU if we're running one, and
2:01:23
it is setting the memory growth to true if we do have a GPU.
2:01:29
So again, don't worry, if you don't have a GPU, don't worry. But if you do, then run this cell, similar to how we downloaded the VGG 16 model, when
2:01:37
we were working with it in previous episodes, we take that same approach to download mobile net here.
2:01:42
So we call TF dot cares applications dot mobile net dot mobile net, and that the first time
2:01:49
we call it is going to download mobile net from the internet. So you need an internet connection.
2:01:54
But subsequent calls to this are just going to be getting the model from a saved model
2:02:00
on disk and loading it into memory here. So we are going to do that now and assign that to this mobile variable.
2:02:08
Now mobile net was originally trained on the image net library, just like VGG 16.
2:02:14
So in a few minutes, we will be passing some images to mobile net that I've saved on this. They are not images from the image net library, but they are images of some general things.
2:02:24
And we're just going to get an idea about how mobile net performs on these random images.
2:02:29
But first, in order to be able to pass these images to mobile net, we're going to have to do a little bit of processing first.
2:02:35
So I've created this function called prepare image. And what it does is it takes a file name, and it then inside the function, we have an
2:02:45
image path, which is pointing to the location on disk, where I have these saved image files
2:02:51
that we're going to use to get predictions from mobile net. So we have this image path defined to where these images are saved, we then load the image
2:03:02
by using the image path, and appending, the file name that we pass in. So say if I pass in image, one dot png here, then we're going to take that file path, append
2:03:17
one PNG here, pass that to load image, and pass this target size of 224 by 224.
2:03:24
Now, this load image function is from the keras API. So what we are doing here is we are just taking the image file, resizing it to be of size
2:03:34
224 by 224, because that is the size of images that mobile net expects, and then we just
2:03:41
take this image and transfer it to be in a format of an array, then we expand the dimensions
2:03:48
of this image, because that's going to put the image in the shape that mobilenet expects.
2:03:54
And then finally, we pass this new processed image to our last function, which is TF Kerris,
2:04:02
that application stop mobile net, that pre process input. So this is a similar function to what we saw a few episodes back when we were working with
2:04:11
VGG 16, it had its own pre process input. Now mobile net has its own pre process input function, which is processing images in a
2:04:22
way that mobile net expects, so it's not the same way as VGG 16. Actually, it's just scaling all the RGB pixel values to be on a scale instead of from zero
2:04:33
to 255, to be on a scale from minus one to one. So that's what this function is doing.
2:04:39
So overall, this entire function is just resizing the image and putting it into an array format
2:04:46
with expanded dimensions and then mobile net, processing it and then returning this processed
2:04:52
image. Okay, so that's kind of a mouthful, but that's what we got to do two images before we pass
2:04:57
them to mobile net. Alright, so we will just Define that function. And now we are going to display our first image called one dot png, from our mobile
2:05:09
net samples directory that I told you, I just set up with a few random images. And we're going to plot that to our Jupyter Notebook here.
2:05:18
And what do you know, it's a lizard. So that is our first image.
2:05:23
Now, we are going to pass this image to our prepare image function that we defined right
2:05:30
above that we just finished talking about. So we're going to pass that to the function to pre process the image accordingly, then
2:05:37
we are going to pass the pre processed image returned by the function to our mobile net
2:05:43
model, we're going to do that by calling predict on the model, just like we've done in previous
2:05:48
videos, when we've called predict on models to use them for inference, then after we get
2:05:54
the prediction for this particular image, we are then going to give this prediction
2:05:59
to this image net utils dot decode predictions function. So this is a function from cares, that is just going to return the top five predictions
2:06:10
from the 1000 possible image net classes. And it's going to tell us the top five that mobile net is predicting for this image.
2:06:19
So let's run that and then print out those results. And maybe you can have a better idea of what I mean once you see the printed output.
2:06:27
So we run this, and we have our output. So these are the top five in order results from the imagenet classes that mobile net
2:06:36
is predicting for this image. So it's assigning a 58% probability to this image of being a an American chameleon 28%
2:06:47
probability to green lizard 13% to a gamma. And then we have some small percentages here under 1% for these other two types of lizards.
2:06:56
So it turns out, if you're not aware, and I don't know how you could not be aware of this, because everyone should know this.
2:07:03
But this is an American chameleon. And I don't know I've always called these things green and OLS.
2:07:09
But I looked it up. And they're also known as American chameleon. So mobilenet got it right.
2:07:15
So yeah, it assigned at 58% probability that was the highest, most probable class.
2:07:21
Next was green lizard. So I'd say that that is still really good for that to be your second place.
2:07:26
I don't know if green lizard is supposed to be more general. But and then a gamma, which is also a similar looking lizard, if you didn't know.
2:07:34
So between these top three classes that it predicted, this is almost 100%.
2:07:39
between the three of these, I would say mobile net did a pretty good job on this prediction. So let's move on to number two.
2:07:47
Alright, so now we are going to plot our second image. And this is a cup of well, I originally thought that it was espresso, and then someone called
2:08:00
it a cup of cappuccino. So let's say I'm not sure I'm not a coffee connoisseur, although I do like both espresso
2:08:07
and cappuccinos. This looks like it has some cream in it. So hey, now we're going to go through the same process of what we just did for the lizard
2:08:16
image where we are passing that image passing this new image to our prepare image function,
2:08:22
so that they undergoes all of the pre processing, then we are going to pass the pre processed image to the predict function for our mobile net model.
2:08:31
Then finally, we are going to get the top five results from the predictions for this
2:08:37
model relative in regards to the imagenet classes. So let's see. All right, so according to mobile net, this is an espresso, not a cappuccino.
2:08:48
So I don't know. But it predicts 99% probability of espresso as being the most probable class for this
2:08:57
particular image. And I'd say that that is pretty reasonable. So let me know in the comments, what do you think is this espresso or cappuccino?
2:09:05
I don't know if mobile or if image net had a cappuccino class. So if it didn't, then I'd say that this is pretty spot on.
2:09:12
But you can see that the other the other four predictions are all less than 1%.
2:09:18
But they are reasonable. I mean, the second one is cup, third eggnog, fourth coffee mug.
2:09:26
Fifth, wooden spoon gets a little bit weird, but there is wood, there is a circular shape
2:09:32
going on here. But these are all under 1%. So they're pretty negligible. I would say mobilenet did a pretty great job at giving a 99% probability to espresso for
2:09:43
this image. All right, we have one more sample image. So let's bring that in.
2:09:48
And this is a strawberry or multiple strawberries if you call if you consider the background.
2:09:55
So same thing we are pre processing the strawberry image. Then we are getting a prediction For a mobile net for this image, and then we are getting
2:10:02
that top five results for the most probable predictions among the 1000 image net classes.
2:10:10
And we see that mobile net with 99.999% probability classifies this image as a strawberry correctly,
2:10:21
so very well, and the rest are well, well under 1%. But they are all fruits.
2:10:27
So interesting. Another really good prediction from mobile net. So even with the small reduction in accuracy that we talked about, at the beginning of
2:10:35
this episode, you can probably tell from just these three random samples that that reduction is maybe not even noticeable when you're just doing tests like the ones that we just ran
2:10:45
through. So in upcoming episodes, we're actually going to be fine tuning this mobile net model to
2:10:51
work on a custom data set. And this custom data set is not one that was included in the original image net library,
2:11:00
it's going to be a brand new data set, we're going to do more fine tuning than what we've done in the past. So stay tuned for that.
2:11:07
Be sure to check out the blog and other resources available for this episode on deep lizard calm, as well as the deep lizard hive mind where you can gain access to exclusive perks
2:11:17
and rewards. Thanks for contributing to collective intelligence. Now let's move on to the next episode.
Process Images for Fine-Tuned MobileNet with TensorFlow's Keras API
2:11:26
Hey, I'm Mandy from deep lizard. And this episode, we'll be preparing and processing a custom data set that we'll use to fine tune
2:11:36
mobilenet using tensor flows keras API.
2:11:45
Previously, we saw how well VGG 16 was able to classify and generalize on images of cats
2:11:55
and dogs. But we noted that VGG 16 was actually already trained on cats and dogs, so it didn't take
2:12:02
a lot of fine tuning at all to get it to perform well on our cat and dog dataset. Now, with mobile net, we'll be going through some fine tuning steps as well.
2:12:12
But this time, we'll be working with a data set that is completely different from the original image net library for which mobile net was originally trained on.
2:12:20
This new data set that we'll be working with is a data set of images of sign language digits,
2:12:27
there are 10 classes total for this dataset, ranging from zero to nine, where each class
2:12:33
contains images of the particular sign for that digit.
2:12:38
This data set is available on kaggle as grayscale images, but it's also available on GitHub
2:12:44
as RGB images. And for our particular task, we'll be using the RGB images.
2:12:50
So check out the corresponding blog for this episode on Peebles or.com, to get the link for where you can download the data set yourself.
2:12:57
So after downloading the data set, the next step is to organize the data on disk.
2:13:03
And this will be a very similar procedure to what we saw for the cat and dog data set earlier in the course.
2:13:09
So once we have the download, this is what it looks like. It is a zipped folder called sign language digits data set master.
2:13:18
And the first thing we want to do is extract all the contents of this directory.
2:13:24
And when we do that, we can navigate inside until we get to the dataset directory.
2:13:30
And inside here we have all of the classes. And each of these directories has the corresponding images for this particular class.
2:13:41
So the what we want to do is we want to grab zero through nine.
2:13:46
And we are going to Ctrl x or cut these directories.
2:13:52
And we're going to navigate back to the root directory, which is here, we're going to place
2:14:00
all of the directories zero through nine in this route. And then we're going to get rid of everything else by deleting.
2:14:08
So now, one last thing. So I'm just going to delete this master.
2:14:13
I don't necessarily like that. So we have sign language digits data set. And directly within we have our nested directories consisting of zero through nine, each of which
2:14:24
has our training data inside then the last step is to move the sign language digits data
2:14:29
set directory to where you're going to be working. So for me that is relative to my Jupyter Notebook, which lives inside of this deep learning with
2:14:38
cares directory, I have a data directory and I have the sign language digits data set located
2:14:45
here. Now everything else that will do to organize and process the data will be done programmatically
2:14:50
in code. So we are in our Jupyter Notebook. Make sure that you do have the imports that we brought in last time still in place because
2:14:58
we'll be making use of those Now, this is just the class breakdown to let you know how
2:15:04
many images are in each class. So across the classes zero through nine, the there are anywhere from 204 to 208 samples
2:15:14
in each class. And then here I have just an explanation of how your data should be structured up to this
2:15:21
point. Now the rest of the organization, we will do programmatically with this script here.
2:15:28
So this script is going to organize the data into train valid and test directories.
2:15:33
So recall right now, we just have all the data located in the corresponding classes of zero through nine.
2:15:40
But the data is not broken up yet into the separate data sets of train, test and validation.
2:15:46
So to do that, we are first changing directory into our sign language digits, data set directory.
2:15:53
And then we are checking to make sure that the directory structure that we're about to setup is not already in place on disk.
2:16:01
And if it's not, then we make a train valid and test directory, right within sign language
2:16:09
digits dataset. So next we are then iterating over all of the directories within our sign language digits
2:16:18
dataset directory. So recall, those are directories labeled zero through nine. So that's what we're doing in this for loop with this range zero to 10.
2:16:26
That's going from directory zero to nine, and moving each of these directories into
2:16:32
our train directory. And after that, we're then making two new directories, one inside of valid with the
2:16:42
directory for whatever place we're at in the loop. So if we are on run number zero, then we are making a directory called zero with invalid,
2:16:53
and a directory called zero within test. And if we are on run number one, then we will be creating a directory called one with invalid
2:17:03
and one within test. So we do this whole process of moving each class in to train and then creating each class
2:17:13
directory empty, within valid and test. So at this point, let's suppose that we are on the first run in this for loop.
2:17:23
So in this range, here we are following add a number zero. So here on this line, what we are doing is we are sampling 30 samples from our train
2:17:35
slash zero directory. Because up here we created or we moved the class directory, zero into train.
2:17:46
And now we're going to sample 30 random samples from the zero directory within train, then,
2:17:55
so we're calling these valid samples, because these are going to be the samples that we move into our validation set.
2:18:01
And we do that next. So for each of the 30 samples that we collected from the training set randomly in this line,
2:18:11
we're now going to be moving them from the training set to the validation, zero set to
2:18:19
the validation set in Class Zero. And then we do similarly the same thing for the test samples, we randomly select five
2:18:29
samples from the train slash zero directory. And then we move those five samples from that train slash zero directory into the test zero
2:18:42
directory. So we just ran through that loop using the Class Zero as an example.
2:18:49
But that's going to happen for each class zero through nine. And just in case you have any issue with visualizing what that script does, if we go into sign
2:18:58
language, digits dataset, now let's check out how we have organized the data.
2:19:03
So recall we previously had classes zero through nine all listed directly here within the this
2:19:12
route folder. Now we have train valid and test directories.
2:19:17
And within train, we moved the original zero through nine directories all into train.
2:19:24
And then once that was done, then we sampled 30 images from each of these classes and moved
2:19:31
them into the valid directory classes. And then similarly, we did the same thing for the test directory.
2:19:40
And then once we look in here, we can see that the test directory has five samples for
2:19:46
zero, we see that it has five samples for one.
2:19:52
If we go check out the valid directories, look at zero, it should have 30 zeros so See
2:20:00
that here 30. So every valid directory has 30 samples, and the training directory classes have not necessarily
2:20:10
uniform samples because remember, we saw that the number of samples in each class ranged
2:20:16
anywhere from 204 to 209, I think. So the number of images within each class directory for the training sample will differ
2:20:26
slightly by maybe one or two images. But the number of images in the classes for the validation and test directories will be
2:20:33
uniform since we did that programmatically with our script here. So checking this dataset out on disk, we can see that this is exactly the same format,
2:20:42
for which we structured our cat and dog image data set earlier in the course, now, we're
2:20:48
just dealing with 10 classes instead of two. And when we downloaded our data, it was in a slightly different organizational structure
2:20:56
than the cat and dog data that we previously downloaded. Alright, so we have obtained the data, we've organized the data.
2:21:04
Now the last step is to pre process the data. So we do that first by starting out by defining where our train valid and test directories
2:21:15
live on disk. So we supply those paths here. And now we are setting up our directory iterators, which we should be familiar with, at this
2:21:25
point. Given this is the same format that we processed our cat and dog images whenever we build our
2:21:31
cnn from scratch, and we use the fine tuned VGG 16 model.
2:21:37
So let's focus on this first variable train batches. First, we are calling image data generator dot flow from directory which we can't quite
2:21:47
see here. But we'll scroll into a minute to image data generator, we are passing this pre processing
2:21:52
function, which in this case is the mobile net pre processing function.
2:21:58
Recall, we saw that already in the last episode. And there we discussed how this pre processes images in such a way that it scales the image
2:22:06
data to be rather than on a scale from zero to 255, to instead be on a scale from minus
2:22:12
one to one. So then on image data generator, we call flow from directory, which is blowing off the screen
2:22:20
here. And we set the directory equal to train path, which we have defined just above saying where
2:22:28
our training data resides on disk, we are setting the target size to 224 by 224, which
2:22:35
recall just resizes any training data to be a height or to have a height of 224 and a
2:22:42
width of 224, since that is the image size that mobile net expects, and we are setting
2:22:49
our batch size equal to 10. So we've got the same deal for valid batches.
2:22:54
And for test batches as well, everything exactly the same except for the paths deferring to
2:23:00
show where the validation and test sets live on disk. And we are familiar now that we specify shuffle equals false only for our test set so that
2:23:11
we can later appropriately plot our prediction results to a confusion matrix.
2:23:17
So we run this cell, and we have output that we have 17 112 images belonging to 10 classes.
2:23:25
So that corresponds to our training set 300 images to 10 classes for our validation set,
2:23:31
and 50 images belonging to 10 classes for our test set. So I have this cell with several assertions here, which just assert that this output that
2:23:42
we find, or that we have right here is what we expect. So if you are not getting this, then it's perhaps because you are pointing to the wrong
2:23:51
location on disk. So a lot of times if you're pointing to the wrong location, then you'll probably get found
2:23:57
zero images belonging to 10 classes. So you just need to check your path to where your data set resides if you get that.
2:24:04
Alright, so now our data set has been processed. We're now ready to move on to building and fine tuning our mobile net model for this
2:24:12
data set. Be sure to check out the blog and other resources available for this episode on depot's or.com
2:24:18
as well as the deep lizard hive mind where you can gain access to exclusive perks and
Fine-Tuning MobileNet on Custom Data Set with TensorFlow's Keras API
2:24:24
rewards. Thanks for contributing to collective intelligence. Now let's move on to the next episode.
2:24:32
Hey, I'm Mandy from Blizzard. In this episode, we'll go through the process of fine tuning mobile net for a custom data
2:24:40
set. Alright, so we are jumping right back into our Jupyter Notebook from last time.
2:24:53
So make sure your code is in place from then since we will be building directly on that
2:24:58
now. So first thing we're going to do is we are going to import mobile net just as we did
2:25:04
in the first mobile net episode by calling TF Kerris, the applications that mobile net
2:25:09
dot mobile net, remember, if this is your first time running this line, then you will need an internet connection to download it from the internet.
2:25:16
Now, let's just take a look at the model that we downloaded. So by calling model dot summary, we have this output here, that is showing us all of these
2:25:28
lovely layers included in mobile net. So this is just to get a general idea of the model because we will be fine tuning it.
2:25:38
So the fine tuning process now is going to start out with us getting all of the layers
2:25:46
up to the sixth to last layer. So if we scroll up and look at our output 123456.
2:25:55
So we are going to get all of the layers up to this layer, and everything else is not
2:26:03
going to be included. So all of these layers are what we are going to keep and transfer into a new model, our
2:26:11
new fine tune model. And we are not going to include the last five layers. And this is just a choice that I came to after doing a little experimenting and testing,
2:26:21
the number of layers that you choose to include versus not include whenever you're fine tuning, a model is going to come through experimentation and personal choice.
2:26:31
So for for us, we are getting everything from this layer and above.
2:26:36
And we are going to keep that in our new fine tuned model. So let's scroll down. So we're doing that by calling mobile dot layers, passing in that six to last layer
2:26:47
and output, then we are going to create a variable called output. And we're going to set this equal to a dense layer with 10 units.
2:26:58
So this is going to be our output layer. That's why it's called output and 10 units due to the nature of our classes, ranging
2:27:05
zero through nine. And this, as per usual is going to be followed by a softmax activation function to give us
2:27:15
a probability distribution among those 10 outputs. Now, this looks a little strange.
2:27:22
So we're calling this and then we're like putting this x variable next to it.
2:27:27
So what's this about? Well, the mobile net model is actually a functional model.
2:27:33
So this is from the functional API from Kerris, not the sequential API.
2:27:40
So we kind of touched on this a little bit earlier, whenever we fine tuned VGG 16, we
2:27:45
saw that VGG 16 was also indeed a functional model. But when we fine tuned it, we iterated over each of the layers and added them to a sequential
2:27:54
model at that point, because we weren't ready to introduce the functional model yet.
2:28:00
So here, we are going to continue working with a functional model type. So that's why we are basically taking all of the layers here up to the sixth the last.
2:28:11
And whenever we create this output layer, and then call this the previous layers stored
2:28:19
in x here, that is the way that the functional model works, we're basically saying to this
2:28:25
output layer, pass all of the previous layers that we have stored in X, up to this six to
2:28:34
last layer and mobile net. And then we can create the model using these two pieces x and output by saying by calling
2:28:43
model, which is indeed a functional model when specified this way, and specifying inputs
2:28:50
equals mobile dot input. So this is taking the input from the original mobile net model, and outputs equals output.
2:28:58
So at this point, output is all of the mobile net model up until the six the last layer
2:29:05
plus this dense output layer. Alright, so let's run these two cells to create our new model.
2:29:11
Alright, so our new models now been created. So the next thing we're going to do is we're going to go through and freeze some layers.
2:29:20
So through some experimentation on my own, I have found that if we freeze, all except
2:29:27
for the last 23 layers, this appears to yield some decent results.
2:29:32
So 23 is not a magic number here, play with this yourself and let me know if you get better
2:29:37
results. But basically, what we're doing here is we're going through all the layers in the model,
2:29:43
and by default, they are all trainable. So we're saying that we want only the last 23 layers to be trainable.
2:29:52
All the layers except for the last one, a three, make those not trainable.
2:29:57
And just so that You understand, relatively speaking, there are 88 total layers in the
2:30:04
original mobile net model. And so we're saying that we don't want to train, or that we only want to train the last
2:30:11
23 layers in our new model that we built just above. Recall, this is much more than we earlier trained with our fine tuned VGG 16 model where
2:30:21
we only train the output layer. So let's go ahead and run that now.
2:30:26
And now let's look at a summary of our new fine tuned model. So here, if we just glance it looks basically the same as what we saw from our original
2:30:36
summary. But we will see here that now now our model ends with this global average pooling 2d layer,
2:30:46
which recall before was the sixth the last layer, where I said that we would include that layer and everything above it.
2:30:52
So all the layers below the global average pooling layer that we previously saw and the original mobile net summary are now gone.
2:31:00
And instead of an output layer with 1000 classes, we now have an output layer with 10 classes
2:31:07
from the or corresponding to the 10 potential output classes that we have for our new sign
2:31:14
language digits dataset. If we compare the total parameters, and how they're split amongst trainable and non trainable
2:31:23
parameters, in this model with the original mobile, mobile that model, then we will see a difference there as well.
2:31:29
Alright, so now this model has been built, we are ready to train the model.
2:31:34
So the code here is nothing new. We are compiling the model in the same exact fashion using the atom optimizer 0.00, with
2:31:45
our little bit of luck, 0.0001 learning rate, categorical categorical cross entropy loss
2:31:51
and accuracy as our metric. So this, we have probably seen 2 million times up to this point in this course.
2:31:59
So that's exactly the same. Additionally, we have exactly the same fit function that we are running to train the
2:32:06
model. So we're passing in our train batches as our data set, we are passing in validation batches
2:32:14
as our validation data. And we are running this for 10 epochs.
2:32:20
Actually, we're going to go ahead and run this for 30. I had 10 here just to save time earlier from testing, but we're going to run this for 30
2:32:27
epochs. And we are going to set verbose equal to two to get the most verbose output.
2:32:33
Now, let's see what happens. All right, so our model just finished training over 30 epochs.
2:32:38
So let's check out the results. And if you see this output, and you're wondering why the first output took 90 seconds, and
2:32:45
then we get or the first epoch took 90 seconds, and then we got it down to five seconds, just a few later, it's because I realized that I was running on battery and not on my laptop
2:32:57
being plugged in. So once we plug the laptop in, it beefed up and started running much quicker.
2:33:03
So let's scroll down and look basically just like how we ended here.
2:33:09
So we are at 100% accuracy on our training set, and 92% accuracy on our validation set.
2:33:17
So that is pretty freakin great considering the fact that this is a completely new dataset,
2:33:24
not having images that were included in the original image net model. So these are pretty good results.
2:33:29
A little bit overfitting here since our validation accuracy is lower than our training accuracy.
2:33:36
So if we wanted to fix that, then we could take some necessary steps to combat that overfitting
2:33:42
issue. But if we look earlier at the earlier epochs, to see what kind of story is being told here,
2:33:49
on our first epoch, our training accuracy actually starts out at 74% among 10 classes,
2:33:56
so that is not bad for a starting point. And we quickly get to 100% on our training accuracy, just within four epochs.
2:34:06
So that's great. But you can see that, at that point, we're only at 81% accuracy for our validation set.
2:34:12
So we have a decent amount of overfitting going on earlier on here.
2:34:17
And then, as we progress through the training process, the overfitting is becoming less
2:34:22
and less of a problem. And you can see that we actually at this point, if we just look at the last eight epochs that
2:34:29
have run here, we've not even stalled out yet on our validation loss.
2:34:36
It's not stalled out in terms of decreasing and our value. Our validation accuracy has not stalled out in terms of increasing so perhaps just running
2:34:44
more epochs on this data will eradicate the overfitting problem.
2:34:49
Otherwise, you can do some tuning yourself changing some hyper parameters around do a
2:34:56
different structure of fine tuning on the model. So freeze Morrow. Less than the last 23 layers for during the fine tuning process, or just experiment yourself.
2:35:09
And if you come up with something that yields better results than this, then put it in the comments and let us know. So we have one last thing we want to do with our fine tuned mobile net model, and that
2:35:18
is use it on our test set. So we are familiar, you know the drill with this procedure at this point, we have done
2:35:25
it several times. So we are now going to get predictions from the model on our test set.
2:35:32
And then we are going to plot those predictions to a confusion matrix. So we are first going to get our true labels by calling test batches classes.
2:35:45
We're then going to gain predictions from the model by calling model dot predict and
2:35:50
passing in our test set stored in test batches here, setting verbose equal to zero, because
2:35:56
we do not want to see any output from the predictions. And now we are creating our confusion matrix.
2:36:03
Using socket learns confusion matrix that we imported earlier, we are setting our true labels equal to the test labels that we defined just here above, we are setting our predicted
2:36:14
labels to the arg max of our predictions across x is one.
2:36:22
And now we are going to check out our class indices of the test batches just to make sure they are what we think they are.
2:36:29
And they are of course, classes labeled zero through nine. So we define our labels for our confusion matrix here, accordingly.
2:36:39
And then we call our plot confusion matrix that we brought in earlier in the notebook and that we have used 17,000 Times up to this point in this course, and we are passing in
2:36:50
our confusion matrix for what to plot, we are passing in our labels that we want to
2:36:57
correspond to our confusion matrix, and giving our confusion matrix, the very general title
2:37:03
of confusion matrix, because hey, that's what it is. So let's plot this. Oh, no.
2:37:08
So plot confusion matrix is not defined? Well, it definitely is just somewhere in this notebook, I must have skipped over here we
2:37:15
go. Nope, here we are. Alright, so here's where plot confusion matrix is defined. Let's bring that in.
2:37:22
Now it is defined and run back here.
2:37:27
So looking from the top left to the bottom right, and now we see that the model appears
2:37:33
to have done pretty well. So we have 10 classes total, with five samples per class.
2:37:41
And we see that we have mostly we have all fours and fives across this diagonal, meaning
2:37:46
that most of the time the model predicted correctly. So for example, for a nine, five times out of five, the model predicted an image was
2:37:55
a nine when it actually was for an eight. However, only four out of five times did the model correctly predict looks like one of
2:38:04
the times the model, let's see, predicted a one when it should have been an eight.
2:38:12
But in total, we've got 12345 incorrect predictions out of 50 total.
2:38:20
So that gives us a 90% accuracy rate on our test set, which is not surprising for us,
2:38:28
given the accuracy that we saw right above on our validation set. So hopefully this series on a mobile net has given you further insight to how we can fine
2:38:38
tune models for custom data set and use transfer learning. To use the information that a model gained from its original training set on a completely
2:38:47
new task in the future. Be sure to check out the blog and other resources available for this episode on the poser.com
2:38:54
as well as the deep lizard hive mind where you can gain access to exclusive perks and
Data Augmentation with TensorFlow' Keras API
2:39:00
rewards. Thanks for contributing to collective intelligence. Now let's move on to the next episode.
2:39:07
Hey, I'm Andy from deep lizard. And in this episode, we're going to learn how we can use data augmentation on images
2:39:16
using tensor flows. keras API.
2:39:25
Data augmentation occurs when we create a new data by making modifications to some existing
2:39:32
data. We're going to explain the idea a little bit further before we jump into the code. But if you want a more thorough explanation, then be sure to check out the corresponding
2:39:41
episode in the deep learning fundamentals course on depot's or.com. For the example that will demonstrate in just a moment, the data we'll be working with is
2:39:50
image data. And so for image data specifically, data augmentation would include things like flipping the image,
2:39:58
either horizontally or vertically. It could include rotating the image, it could include changing the color of the image, and
2:40:06
so on. One of the major reasons we want to use data augmentation is to simply just get access
2:40:13
to more data. So a lot of times that not having access to enough data is an issue that we can run into.
2:40:20
And we can run into problems like overfitting if our training data set is too small.
2:40:25
So that is a major reason to use data augmentation is to just grow our training set, adding augmented
2:40:31
data to our training set can in turn reduce overfitting as well. Alright, so now let's jump into the code to see how we can augment image data using Kerris.
2:40:41
Alright, so the first thing that we need to do, of course, is import all of the packages that we will be making use of for this data augmentation.
2:40:49
Next, we have this plot images function, which we've introduced earlier in the course, this
2:40:55
is directly from tensor flows website. And it just allows us to apply images to our Jupyter Notebook.
2:41:02
So check out the corresponding blog for this episode on deep lizard calm to get the link so that you can go copy this function yourself.
2:41:09
Alright, so next we have this variable called Gen, which is an image data generator.
2:41:17
And recall, we've actually worked with image data generators. earlier in the course, whenever we create our train at test and valid batches that we were using for training seeing it
2:41:28
and with this, though, we are using image data generator in a different way. Here, we are creating this generator that we are specifying all of these parameters
2:41:38
like rotation, range, width, shift range, heights of range, sheer range, zoom range channels of drains in horizontal flip.
2:41:46
So these are all options that allow us to augment our image data.
2:41:51
So you need to check the documentation on tensor flows website to get an idea of the
2:41:57
units for these parameters, because they're not all the same. So for example, rotation range here, I believe is measured in radians.
2:42:07
Whereas like this width, shift range is measured as a percentage of the width of the image.
2:42:14
So these are all ways that we can augment image data. So this is going to be rotating the image by 10 radians, this is going to be shifting
2:42:23
the width of the image by 10%, the height by 10%. Zooming in shifting the color channels, flipping the image, so all sorts of things.
2:42:34
So just all different ways that we can augment image data. So there are other options to just be sure to check out the image data generator documentation
2:42:44
if you want to see those options. But for now, these are the ones that we'll be working with.
2:42:49
So we store that in our Jen variable. Next, we are going to choose a random image from a dog directory that we had set up earlier
2:43:00
in the course under the dogs versus cats dataset, we're going to go into train into dog, then
2:43:07
we are going to choose a random image from this directory. And then we're going to set this image path accordingly.
2:43:14
So we're just going to set this to point to whatever that chosen image was on disk, then
2:43:20
we have this assertion here just to make sure that that is indeed a valid file before we proceed with the remaining code.
2:43:27
Now we're just going to plot this image to the screen. And I'm not sure what this is going to be since it is a random image from disk.
2:43:35
So that is a cute looking. I don't know. Beagle, Beagle, basset hound mix, I don't know, what do you guys think.
2:43:45
So this is the random dog that was selected from from our dog trained directory.
2:43:53
Now we are creating this new variable called all etre. And to our image data generator that we created earlier called Jin, we're calling this flow
2:44:03
function and passing our image in to flow. And this is going to generate a batch of augmented images from this single image.
2:44:14
So next, we are defining this og images variable, which is going to give us 10 samples of the
2:44:22
augmented images created by og etre here. Lastly, we are going to plot these images using the plot images that we defined just
2:44:32
above. All right, so let's zoom in a bit. All right, we can see now that first let's take a look at our original dog Alright, so
2:44:42
that is the original image. Now we can look and see that given those things like rotation and width shift, and everything
2:44:51
that we defined earlier, whenever we defined whenever we defined our image data generator
2:44:57
that has now been done to All of these images in one random way or another so we can see
2:45:04
kind of what's happening here. So for example, this particular image looks like it has been shifted up some because we
2:45:11
can see that the head of the dog is being cut off a little bit. And this image, or let's see which way was the dog originally facing, so its head is
2:45:21
facing to the right. So yeah, so this image here has been flipped, the dog is now facing to the left.
2:45:29
And this image appears to be shifted down some. And so some of these, like this one looks like it's been rotated.
2:45:37
So we can get an idea just by looking at the images, the types of data augmentation that
2:45:42
have been done to them. And we can see how this could be very helpful for growing our dataset in general, because
2:45:50
for example, say that we have a bunch of images of dogs, but for whatever reason, they're
2:45:55
all facing to the left. And we want to deploy our model to to be some general model that will classify different
2:46:04
dogs. But the types of images that will be accepted by this model later might have dogs facing to the right as well.
2:46:10
Well, maybe our model totally implodes on itself whenever it receives a dog facing to
2:46:15
the right. So through data augmentation, given the fact that it is very normal for dogs to face left
2:46:21
or right, we could augment all of our dog images to have the data or to have the dogs
2:46:28
also face in the right direction, as well as the left direction to have a more have
2:46:35
a more dynamic data set. Now, there is a note in the corresponding blog for this episode on people's are calm,
2:46:42
giving just the brief instruction for how to save these images to disk if you want to
2:46:48
save the images after you augment them, and then add them back to your training set. So check that out if you're interested in doing that to actually grow your training
2:46:56
set. Rather than just plot the images here and your Jupyter Notebook. By the way, we are currently in Vietnam filming this episode.
2:47:03
If you didn't know we also have a vlog channel where we document our travels and share a
2:47:09
little bit more about ourselves. So check that out at beetles or vlog on YouTube. Also, be sure to check out the corresponding blog for this episode, along with other resources
2:47:19
available on the blizzard.com. And check out the people's hive mind where you can gain exclusive access to perks and
Collective Intelligence and the DEEPLIZARD HIVEMIND
2:47:26
rewards. Thanks for contributing to collective intelligence.
2:47:32
I'll see you next time.