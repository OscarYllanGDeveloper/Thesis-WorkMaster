Introduction to quantum mechanics
0:04
welcome to quantum mechanics my name is brent carlson since this is the first lecture on
0:10
quantum mechanics um we ought to have some sort of an introduction and what i want to do to introduce quantum mechanics is to
0:17
explain first of all why it's necessary and second of all to put it in historical context to
0:23
well i'll show one of the most famous photographs in all of physics that
0:29
really gives you a feel for the brain power that went into the construction of this theory and hopefully we'll put it in some
0:34
historical context as well so you can understand where it fits in the broader philosophy of science
0:40
but the the main goal of this lecture is about the need for quantum mechanics which i really ought to just have called
0:47
why do we need quantum mechanics uh this subject has a reputation for being
0:53
a little bit annoying so why do we bother with it well first off for some historical context
1:01
imagine yourself back in 1900 turn of the century science has really advanced a lot we
1:08
have electricity we have all this fabulous stuff that electricity can do
1:13
and even almost 100 years before that physicists thought they had things figured out there's a famous quote from
1:19
laplace given for one instant an intelligence which could comprehend all the forces by which nature is animated
1:25
and the respective position of the beings which compose it nothing would be uncertain in the future as the past
1:31
would be present to its eyes now maybe you think intelligence which can
1:37
comprehend all the forces of nature is a bit of a stretch and maybe such a being which can
1:45
know all the respective positions of everything in the universe is a bit of a stretch as well
1:50
but the feeling at the time was that if you could do that you would know everything if you had perfect knowledge
1:56
of the present you could predict the future and of course you can infer what happened in the past
2:03
and everything is connected by one unbroken chain of causality
2:10
now in 1903 albert michaelson another famous quote from that time period said
2:15
the more important fundamental laws and facts of physical science have all been discovered our future discoveries must be looked
2:21
for in the sixth place of decimals now this sounds rather audacious this is
2:26
1903 and he thought that the only thing that we had left to nail down was the
2:32
part in a million level precision well to be fair to him he wasn't talking about never discovering new fundamental
2:39
laws of physics he was talking about really astonishing discoveries like
2:44
the discovery of uranus on the basis of orbital perturbations of neptune never having seen the planet
2:51
uranus before they figured out that it had to exist just by looking at things that they had seen that's pretty
2:56
impressive and michaelson was really on to something precision measurements are really really useful especially today
3:03
but back in 1903 it wasn't quite so simple and michaelson probably regretted that remark for the rest of his life
3:11
the attitude that i want you guys to take when you approach quantum mechanics though is not this
3:16
sort of 1900s notion that everything is predicted it comes from shakespeare horatio says one
3:24
oh day and night but this is wondrous strange to which hamlet replies one of the most famous lines in all of shakespeare and
3:30
therefore as a stranger give it welcome there are more things in heaven and earth horatio than are dreamt of in your philosophy
3:37
so that's the attitude i want you guys to take when you approach quantum mechanics it is wondrous strange
3:42
and we should give it welcome there are some things in quantum mechanics that are deeply non-intuitive
3:50
but if you approach them with an open mind quantum mechanics is a fascinating subject there's a lot of really fun
3:56
stuff that goes on now to move on to the necessity for
4:02
quantum mechanics there were some dark clouds on the horizon even at the early 20th century
4:08
michelson wasn't quite having a big enough picture in his mind when he said that everything was down to the sixth
4:13
place of decimals the dark clouds on the horizon at least according to kelvin here
4:19
were a couple of unexplainable experiments one the black body spectrum now a black
4:25
body you can just think of as a hot object and a hot object like for example the
4:30
coils on an electrical stove when they get hot will glow and the question is what color do they
4:36
glow do they glow red they go blue what is the distribution of radiation that is emitted by a hot object
4:44
another difficult to explain experiment is the photoelectric effect if you have some light
4:51
and it strikes a material electrons will be ejected from the
4:56
surface and as we'll discuss in a minute the properties of this experiment do not
5:03
fit what we think we know about or at least what physicists thought they knew
5:08
about the physics of light in the physics of electrons at the turn of the 20th century
5:14
the final difficult experiment to explain is bright line spectra for example if i have a flame coming from
5:19
say a bunsen burner and i put a chunk of something perhaps sodium
5:25
in that flame it will emit a very particular set
5:30
of frequencies that looks absolutely nothing like a black body we'll talk about all these experiments
5:36
in general or in a little bit more detail in a minute or two but
5:41
just looking at these experiments now these are all experiments that are very difficult to explain
5:46
knowing what we knew at the turn of the 20th century about classical physics they're also also experiments that
5:52
involve light and matter so we're really getting down to the
5:58
details of what stuff is really made of and how it interacts with the things around it
6:06
so these are some pretty fundamental notions and that's where quantum mechanics really got its start
6:12
so let's pick apart these experiments in a little more detail the black body spectrum as i mentioned
6:18
you can think of as the light that's emitted just by a hot object
6:24
and while hot objects have some temperature associated with them let's call that t the plot here on the right is showing
6:31
very qualitatively i'll just call it the intensity of the light emitted as a function of the
6:38
wavelength of that light so short wavelengths high energy long
6:43
wavelengths low energy now if you look at t equals 3 500 kelvin
6:49
curve here it has a long tail to long wavelengths and it cuts off pretty quickly as you go
6:55
to short wavelength so it doesn't emit very much high energy light whereas if you have a much hotter object
7:01
5500 kelvin it emits a lot more high energy light the red curve here is much
7:06
higher than the black curve now if you try to explain this
7:11
knowing what early 20th century physicists knew about radiation and about electrons and about
7:18
atoms and how they could possibly emit light you get a prediction and it works wonderfully well
7:24
up until about here at which point it blows up to infinity um infinities are bad in physics
7:32
this is the the rayleigh jeans law and it works wonderfully well for long wavelengths but does not work at all for
7:38
short wavelengths that's called the ultraviolet catastrophe if you've heard that term on the other end of things
7:45
if you look at what happens down here well it's not so much a prediction but
7:50
an observation but there's a nice formula that fits here
7:56
so on one side we have a prediction that works well on one end but doesn't
8:02
work on the other and on the other hand we have a sort of empirical formula called veen's law
8:09
that works really well at the short wavelengths but well also blows up to infinity at the long wavelengths both of
8:15
these blowing up things are a problem the question is how do you get something that explains both of them
8:21
this is the essence of the the black body spectrum and how it was difficult to interpret in the context of classical physics
8:28
the next experiment i mentioned is the photoelectric effect this is sort of the opposite
8:34
problem it's not how a material emits light it's how light interacts with the material
8:40
so you have light coming in and the experiment is usually done like this
8:46
you have your chunk of material typically a metal
8:52
and when light hits it electrons are ejected from the surface hence the electric part of the photoelectric
8:58
effect and you do all this in a vacuum and the electrons are then allowed to go
9:04
across a gap to some other material another chunk of metal
9:10
where they strike this metal and the experiment is usually done like this you connect it up
9:15
to a battery so you have your material on one side and your material on the other
9:21
you have light hitting one of these materials and ejecting electrons and you tune the voltage on this battery
9:27
such that your electrons when they're ejected never quite make it so the electric field produced by this
9:33
voltage is opposing the motion of the electrons when that voltage is just high enough to
9:39
stop the motion of the electrons keep them from completely making it all the way across we'll call that the stopping voltage
9:48
now it turns out that what classical e m predicts
9:56
as i mentioned doesn't match what actually happens in reality but let's think about what does classical e m
10:02
predict here well classical electricity and magnetism says
10:08
that electromagnetic waves here have electric fields and magnetic fields associated with them
10:14
and these are propagating waves if i increase the intensity
10:20
of the electromagnetic wave that means the magnitude of the electric field
10:25
involved in the electromagnetic wave is going to increase and if i'm an electron
10:31
sitting in that electric field the energy i acquire is going to
10:37
increase that means the stop
10:42
is going to increase because i'll have to have more voltage to stop a higher energy electron as would be produced by
10:48
higher intensity beam of light the other parameter of this incoming light is its frequency so we can think
10:55
about varying the frequency if i increase the frequency i have more intense light
11:01
now that doesn't say anything about the string sorry if i increase the frequency
11:07
i don't necessarily have more intense light the electric field magnitude is going to be the same
11:15
which means the energy and the stopping voltage
11:23
will also be the same now it turns out what actually happens
11:29
in reality does not match this at all in reality when the intensity increases
11:36
the energy which i should really write as v stop the stopping voltage necessary
11:43
doesn't change and when i increase the frequency
11:49
the voltage necessary to stop those electrons increases so this is sort of exactly the opposite
11:56
what's going on here that's the puzzle in explaining the photoelectric effect
12:02
just to briefly check your understanding consider these plots of stopping voltage as a function of the parameters of the
12:08
incident light and check off which you think shows the classical prediction for the
12:14
photoelectric effect the third experiment that i mentioned is
12:21
bright line spectra and as i mentioned this is what happens if you take a flame
12:26
or some other means of heating a material like the bar of sodium i mentioned earlier
12:33
this will emit light and uh in this case
12:39
the spectrum of light from red to blue of sodium looks like this
12:46
oh actually i'm sorry that's not sodium that's mercury uh the these are four different elements
12:53
hydrogen mercury neon and xenon and instead of getting a broad
13:00
continuous distribution like you would from a black body under these circumstances where you're
13:05
talking about gases you get these very bright regions it's the spectrum
13:10
instead of looking like a smooth curve like this looks like spikes
13:17
those bright lines are extraordinarily difficult to explain with classical physics and this is really the
13:22
straw that broke the camel's back broke classical physics is back that really kicked off quantum mechanics
13:28
how do you explain this
13:33
this is that famous photograph that i mentioned this is really the group of people who first built quantum mechanics
13:40
now i mentioned three key experiments the
13:45
black body spectrum this guy figured that out this is plonk
13:52
the photoelectric effect this guy who i hope needs no introduction this is einstein
14:02
that out this is the paper that won einstein the nobel prize
14:08
and as far as the brightline spectra of atoms
14:13
it took a much longer time to figure out how all of that fit together
14:19
and it took a much larger group of people but they all happen to be present in this photograph there's this guy
14:26
and this guy and these two guys and this guy
14:31
this photograph is famous because these guys worked out quantum mechanics
14:40
but that's not the only these aren't the only famous people in this photograph you know this lady as
14:45
well this is marie curie this is lorenz
14:50
which if you studied special relativity you know einstein used the lorentz transformations
14:57
pretty much everyone in this photograph is a name that you know i went through and looked up who these people were
15:03
these were all of the names that i recognized which doesn't mean that the people whose names i didn't recognize weren't also excellent scientists
15:10
for example ctr wilson here one of my personal favorites inventor of the cloud chamber
15:15
this is the brain trust that gave birth to quantum mechanics and it was quite a brain trust
15:24
you had some of the most brilliant minds of the century working on some of the most difficult problems of the century
15:32
and what's astonishing is they didn't really like what they found they discovered explanations that made
15:38
astonishingly accurate predictions but throughout the history you keep seeing them disagreeing like no that can't
15:44
possibly be right not necessarily because the predictions were wrong or they
15:50
thought there was a mistake somewhere but because they just disliked the nature of what they were doing they were
15:55
upending their view of reality einstein in particular really disliked quantum mechanics to the day that he died
16:02
just because it was so counter-intuitive and so with that introduction
16:08
to a counter-intuitive subject i'd like to remind you again of that shakespeare quote
16:13
there are more things in heaven and earth horatio than are dreamt of in your philosophy
16:19
uh try to keep an open mind and hopefully we'll have some fun at this
The domain of quantum mechanics
16:26
knowing that quantum mechanics has something to do with explaining the interactions of light and matter for
16:31
instance in the context of the photoelectric effect or black body radiation or bright line
16:38
spectra of atoms and molecules one might be led to the question of when
16:44
is quantum mechanics actually relevant the domain of quantum mechanics is unfortunately not a particularly simple
16:52
question when does it apply well on the one hand you have classical physics
16:58
and on the other hand you have quantum physics
17:04
and the boundary between them is not really all that clear on the classical side you
17:10
have things that are certain whereas on the quantum side you have things that are uncertain
17:18
what that means in the context of physics is that on the classical side things are predictable
17:25
they may be chaotic and difficult to predict but in principle they can be predicted
17:30
well on the quantum side things are predictable too
17:36
but with a caveat in the classical side you determine everything
17:44
basically every property of the system can be
17:49
known with perfect precision whereas in quantum mechanics what you predict are probabilities
17:58
and learning to work with probabilities is going to be the first step to getting comfortable with quantum mechanics
18:06
the boundary between these two realms when the uncertain and probabilistic
18:12
effects of quantum mechanics start to become relevant
18:17
is really a dividing line between things that are large and things that are small
18:23
and that's not a particularly precise way of stating things doing things more mathematically
18:30
quantum mechanics applies for instance when angular momentum l is on the scale of planck's constant
18:38
or the reduced flux constant h bar now h bar is the fundamental scale of
18:44
quantum mechanics and it appears not only in the context of angular momentum planck's constant has units of angular
18:50
momentum so if your angular momentum is of order planck's constant or smaller you're in the domain of quantum
18:55
mechanics we'll learn more about uncertainty principles later as well but
19:01
uncertainties in this context have to do with products of uncertainties for instance the uncertainty in the
19:07
momentum of a particle times the uncertainty in the position of the particle this if it's comparable to planck's
19:14
constant is also going to give you the realm of quantum mechanics
19:20
energy and time also have an uncertainty relation again approximately equal to planck's constant
19:28
most fundamentally the classical action
19:33
when you get into more advanced studies of classical mechanics you'll learn
19:38
about a quantity called the action which has to do with the path the system takes as it evolves in space and time
19:45
if the action of the system is of order planck's constant then you're in the quantum mechanical
19:50
domain now klonk's constant is a really small number it's 1.05
19:57
times 10 to the negative 34 kilogram meters squared per second times 10 to the negative 34 is a small
20:04
number so if we have really small numbers then
20:10
we're in the domain of quantum mechanics in practice these guys are the most
20:15
useful whereas this is the most fundamental but we're more interested in useful
20:21
things than we are in fundamental things after all for example
20:26
the electron in the hydrogen atom now you know from looking at the bright line spectra that this should be in the
20:32
domain of quantum mechanics but how can we tell
20:37
well to use one of the uncertainty principles as a calculation
20:42
consider the energy the energy of an electron in a hydrogen atom is you
20:49
know let's say about 10 electron volts
20:55
if we say that's p squared over 2m using the classical kinetic energy
21:01
relation between momentum and kinetic energy that tells us
21:07
that the momentum p is going to be about 1.7 times 10 to the
21:14
minus 24th kilogram meter square sorry kilogram
21:20
where'd it go where's my eraser kilogram
21:26
meters per second now this suggests that the momentum of
21:32
the electron is you know non-zero but if the hydrogen atom itself is not moving we know the average
21:38
momentum of the electron is zero so if the momentum of the electron is going to be zero with
21:44
still some momentum being given to the electron this is more the uncertainty in
21:50
the electron momentum than the electron momentum itself
21:56
the next quantity if we're looking at the uncertainty relation between momentum and position is we need to know
22:02
the size of or the uncertainty in the position of the electron which has to do with the size of the atom
22:11
now the size of the atom that's about 0.1 nanometers
22:17
which if you don't remember the conversion from nanometers is 10 to the minus 10th meters
22:24
so let's treat this as delta x our uncertainty in position
22:30
because we don't really know where the electron is within the atom so this is a reasonable guess at the uncertainty
22:36
now if we calculate these two things together delta p delta x
22:41
you get something i should say this is approximate because this is very approximate
22:46
1.7 times 10 to the negative 34th and if you plug through the units
22:53
it's kilogram meters squared per second this
22:59
is about equal to h-bar so this tells us that quantum mechanics is definitely important here
23:05
we have to do some quantum in order to understand this system
23:11
as an example of another small object that might have quantum mechanics
23:18
relevant to it this is one that we would actually have to do a calculation i don't know intuitively whether a speck
23:23
of dust in a light breeze is in the realm of quantum mechanics or classical physics now
23:29
i went online and looked up some numbers for a speck of dust let's say the mass
23:34
is about 10 to the minus sixth kilograms a microgram
23:40
uh has a velocity in this light breeze
23:45
of let's say one meter per second
23:51
and let me make myself some more space here um the size
23:58
of this speck of dust is going to be about 10 to the minus 5
24:04
meters so these are the basic parameters of this speck of dust in a light breeze
24:11
now we can do some calculations with this for instance momentum
Key concepts of quantum mechanics
24:18
well in order to understand quantum mechanics there's some basic vocabulary that needs
24:23
to that i need to go over so let's talk about the key concepts in quantum mechanics
24:28
thankfully there are only a few there's really only three and the first is the wave function
24:34
the wave function is and always has been written as psi the greek letter
24:39
my handwriting gets a little lazy sometimes and it'll end up just looking like this but technically
24:45
it's supposed to look something like that details are important provided you recognize the symbol
24:52
psi is a function of position potentially in three dimensions x y and
24:57
z and time and the key facts here
25:02
is that psi is a complex function
25:08
which means that while x y z and t here are real numbers psi
25:13
evaluated at a particular point in space will potentially be a complex number with both real and imaginary part
25:22
what is subtle about the wave function and we'll talk about this in great detail later is that it while it
25:28
represents the state of the system
25:34
it doesn't tell you with any certainty what the observable properties of the system are it really only gives you
25:40
probabilities
25:47
so for instance if i have coordinate system something like this
25:54
where say this is position in the x direction
25:59
psi with both real and imaginary parts might look something like this this could be the real part of psi
26:07
and this could be say the complex or the imaginary part of psi
26:14
what is physically meaningful is the squared magnitude of psi which might look something like this
26:20
in this particular case and that is related to the probability
26:26
of finding the particle at a particular point in space as i said we'll talk about this later
26:32
but the key facts that you need to know about the wave function is that it's complex and it describes the state of the system but not with certainty
26:40
the next key concept in quantum mechanics is that of an operator
26:46
now operators are what connect psi
26:53
to observable quantities that is one thing operators can do
27:00
just a bit of notation usually we use hats for operators for instance x hat
27:06
or p-hat our operators that you'll encounter shortly operators
27:13
act on psi so if you want to apply for instance the x-hat operator to psi you would write x
27:20
hat psi as if this were something that were as it appears on the left of psi the
27:25
assumption is that x acts on psi if i write psi x hat doesn't necessarily mean that x
27:32
hat acts on psi you assume operators act on whatever lies to the right
27:38
likewise of course p hat psi now we'll talk about this in more detail
27:43
later but x hat the operator can be thought of as just multiplying by
27:49
x so if i have psi as a function of x
27:57
x hat psi is just going to be x times psi of x so if psi was a polynomial you could
28:03
multiply x by that polynomial the the p operator p hat
28:10
is another example is a little bit more complicated this is just an example now and technically this is the momentum operator but we'll talk
28:16
more about that later it's equal to minus h bar times
28:21
the derivative with respect to x so this is again something that
28:27
needs a function needs the wave function to actually give you anything meaningful
28:35
now the important thing to note about the operators is that they don't give you the observable quantities either
28:41
but in quantum mechanics you can't really say the momentum of the
28:46
wave function for instance p hat psi is not
28:53
and i'll put this in quotes because you won't hear this phrase very often momentum
29:01
of psi it's the momentum operator acting on psi and that's not the same thing as the
29:07
momentum of psi the final key concept
29:13
in quantum mechanics is the schrodinger equation and this is really the big equation so
29:19
i'll write it big i h bar partial derivative of psi
29:26
with respect to time is equal to h hat that's an operator
29:34
acting on psi now h hat here is the hamiltonian
29:43
which you can think of as the energy operator
29:51
so the property of the physical system that h is associated with is the energy
29:57
of the system and the energy of the system can be thought of as a kinetic energy
30:05
so we can write a kinetic energy operator plus a potential energy operator together acting on psi
30:12
and it turns out the kinetic energy operator can be written down this is going to end up looking like minus h bar
30:17
squared over 2m partial derivative of psi
30:22
with respect to sorry second partial derivative of side with respect to position
30:28
plus and then the potential energy operator is going to look like the potential energy as a function of position just multiplied by psi
30:37
so this is the schrodinger equation typically you'll be working with it in this form
30:47
so i h bar times the partial derivative with respect to time is related to the partial derivative with respect to space
30:53
and then multiply multiplied by some function the basic quantum mechanics that we're
30:58
going to learn in this course mostly revolves around solving this function and interpreting the results
31:06
so to put these in a bit of a roadmap we have operators
31:14
we have the schrodinger equation
31:21
and we have the wave function
31:26
now operators act on the wave function
31:34
and operators are used in the schrodinger equation
31:40
now the wave function that actually describes the state of the system is going to be the solution
31:47
to the schrodinger equation now i mentioned operators acting on the
31:53
wave function what they give you when they act on the wave function is
31:59
some property of the system some observable perhaps
32:06
and the other key fact that i mentioned so far is that the wave function doesn't describe the system perfectly it only gives you probabilities
32:17
so that's our overall concept map to put this in the context of the course outline the probabilities are really the
32:24
key feature of quantum mechanics and we're going to start this course with the discussion of probabilities
32:30
we'll talk about the wave function after that and how the wave function is related to those probabilities
32:35
and we'll end up talking about operators and how those operators and the wave functions together give you
32:41
probabilities associated with observable quantities
32:46
that will lead us into a discussion of the schrodinger equation which will be most of the course really
32:54
the bulk of the material before the first exam will be considered with very concerned with various examples
33:00
a solution to the schrodinger equation under various circumstances this is really the main meat of quantum mechanics in the beginning
33:08
after that we'll do some formalism and what that means is we'll learn about
33:14
some advanced mathematical tools that make keeping track of all the details of how all of this fits together
33:20
a lot more straightforward and then we'll finish up the course by doing some applications
33:29
so those are our key concepts and a general road map through the course hopefully now you have the basic
33:34
vocabulary necessary to understand phrases like the momentum operator acts on the wave function or the solution to
33:42
the schrodinger equation describes the state of the system and that sort of thing
33:47
don't worry too much if these concepts haven't quite clicked in order to really understand quantum
33:52
mechanics you have to get experience with them these are not things that you really have any intuition for based on anything you've seen in physics so far
33:59
so bear with me and this will all make sense in the end i promise
A review of complex numbers for QM
34:05
complex numbers or numbers involving conceptually you can think about it as the square root of negative one
34:11
i are essential to understanding quantum mechanics since some of the most fundamental concepts in quantum
34:17
mechanics for instance the wave function are expressed in terms of complex numbers
34:22
complex analysis is also one of the most beautiful subjects in all of mathematics but unfortunately in this course i don't have the time to go into the details
34:29
lucky you perhaps here's what i think you absolutely need to know to understand quantum mechanics
34:36
from the perspective of complex analysis first of all there's basic definition i
34:41
squared is equal to negative 1 which you can think of also as i equals the square root of negative 1.
34:48
a in general a complex number z then can be written as a the sum of a
34:55
purely real part x and a purely imaginary part
35:01
i times y note in this expression z is complex x and y are real where i
35:07
times y is purely imaginary the terms purely real or purely imaginary
35:12
in the context of this expression like this x plus i y something is purely real if y is zero something is purely
35:18
imaginary if x is zero as far as some notation for extracting the real and imaginary parts typically
35:24
mathematicians will use this funny calligraphic font to indicate the real part of x plus iy
35:30
or the imaginary part of x plus iy and that just pulls out x and y note that both of these
35:35
are real numbers when you pull out the imaginary part you get x and y you don't get i y for instance
35:45
another one of the most beautiful results in mathematics is e to the i pi plus one
35:52
equals zero this formula kind of astonished me when i first encountered it
35:58
but it is a logical extension of this more general formula that e raised to a
36:04
purely imaginary power i y is equal to the cosine of y plus i times
36:09
the sine of y this can be shown in a variety of ways
36:14
in particular involving the taylor series if you know the taylor series for the exponential the taylor series for cosine of y and the taylor series for
36:21
sine of y you can show quite readily that the taylor series for complex exponential is the taylor series of
36:27
cosine plus the taylor series of sine and while that might not necessarily constitute a rigorous proof
36:34
it's really quite fun if you get the chance to go through it at any rate the trigonometric functions
36:39
here cosine and sine should should be suggestive and there is a geometric interpretation of complex
36:45
numbers that we'll come back to in a minute but for now know that while we have rectangular forms like this
36:55
x plus i y where x and y the nomenclature there is chosen on purpose
37:00
you can also express this in terms of r e to the i theta where you have now a
37:05
radius and an angle the angle here
37:10
by the way is going to be the arc tangent of y over x
37:17
and we'll see why that is in uh in a moment when we talk about the geometric interpretation
37:23
but given these rectangular and polar forms of complex numbers
37:28
what do the basic operations look like how do we manipulate these things well addition and subtraction in
37:34
rectangular form is straightforward if we have two complex numbers a plus ib plus
37:40
and we want to add to that the second complex number c plus id we just add the real parts a and c and we add the
37:46
imaginary parts b and d this is just like adding in any other sort of algebraic expression
37:52
multiplication is a little bit more complicated you have to distribute
37:57
and you distribute in the usual sort of draw smiley face kind of way a times c and b times d are going to end
38:05
up together in the real part the reason for that is well a times c a and c both being real numbers a times c will be
38:11
real whereas ib times id both being purely complex numbers
38:18
you'll end up with b times d times i squared and i squared is minus 1. so you just
38:25
end up with minus bd which is what we see here otherwise the complex part is perhaps a
38:32
little more easy to understand you have i times b times c and you have a times i times d both of which end up with plus
38:39
signs in the complex part division in this case
38:44
is like rationalizing the denominator except instead of involving radicals you have complex numbers
38:50
if i have some number a plus i b divided by c plus id i can simplify this by both multiplying
38:56
and dividing by c minus id note the sign change in the denominator here c plus id is then
39:04
prompting me to multiply by c minus id over c minus id
39:09
now when you do the distribution there for instance let's just do it in the denominator
39:14
c plus id times c minus id my top eyebrows here of the smiley face
39:22
c squared minus sorry c squared times id c squared plus now id
39:30
times minus id which is well i'll just write it out i times minus id
39:36
which is going to be d squared times i times minus i so i squared times minus one and i
39:44
squared is minus one so i have minus one times minus one which is just one so i can ignore that
39:50
i've just got d squared so what i end up with in the denominator is just c squared plus d squared what i end up with in the
39:56
numerator well that's the same sort of multiplication thing that we just discussed
40:01
so the simplified form of this has no complex part in the denominator
40:06
which helps keep things a little simple and a little easier to interpret
40:12
now in polar form addition and subtraction while they're complicated
40:19
under most circumstances if you have two complex numbers given in polar form it's easiest just to convert to rectangular
40:24
form and add them together there multiplication and division though in
40:29
polar form have very nice expressions q e to the i theta times r e to the i
40:36
phi well these are just all real numbers multiplying together and then i can use the rules regarding multiplication of
40:42
exponentials meaning if i have two things like e to the i theta and e to the iv i can just add
40:49
the exponents together it's like taking x squared times x to the fourth and getting x to the sixth
40:55
but q are e to the i theta plus v so that was easy we didn't have to do any distribution at all
41:02
the key factor is that you add the angles together
41:07
in the case of division it's also quite easy you simply divide the radii q over r and instead of adding you subtract the
41:14
angles so polar complex numbers expressed in polar form
41:20
are much easier to manipulate in multiplication and division while complex numbers represented in
41:25
rectangular form are much easier to manipulate for addition and subtraction
41:33
taking the magnitude of complex number usually we'll write that as something like z if z is a complex number just
41:40
using the same notation for absolute value of a real number
41:46
usually is expressed in terms of the complex conjugate the complex conjugate notationally speaking is usually written
41:52
by whatever complex number you have here in this case x plus iy with a star after it
41:58
and what that signifies is you flip the sign on the complex part on the imaginary part x plus iy becomes x minus iy
42:06
the squared magnitude then which is always going to be a real and positive number
42:13
this absolute value squared notation is what you get for multiplying a number
42:19
by its complex conjugate and that's what we saw earlier with c plus id
42:24
say i take the complex conjugate of c plus id and multiply it by c plus i d well the complex conjugate of c plus id
42:31
is c minus id times c plus id
42:37
and doing the distribution like we did when we calculated the denominator when we were simplifying
42:43
the division of complex numbers in rectangular form just gave us c squared plus d squared
42:50
this should be suggestive if you have something like x plus i y
42:56
that's really messy x plus i y and i want to know the squared absolute magnitude
43:01
thinking about this as a position in cartesian space should make this formula c squared plus
43:07
d squared in this case just make uh make a little more sense
43:13
you can also of course write that in terms of real and imaginary parts but let's do an example
43:19
if w is 3 plus 4i and z is -1 plus 2i
43:25
first of all let's find w plus z well w plus z is three plus four i plus
43:33
minus one plus two i that's straightforward if you can keep track of your terms 3 minus 1 is going
43:40
to be our real part so that's 2 and 4i plus 2i which is plus 6i is going
43:47
to be our complex part sorry our imaginary part
43:53
now w times z 3 plus 4 i times minus 1 plus 2i
44:01
for this we have to distribute like usual
44:08
so from our top eyebrow terms here we've got three
44:15
times minus one which is minus three and four i times 2i both positive so i
44:23
have 4 times 2 which is 8 and i times i which is minus 1 minus 8.
44:29
then for my imaginary part the i guess the mouth and the chin if
44:34
you want to think about it that way i have 4i times minus 1 minus 4 with the i out front will just
44:41
be minus 4 inside the parentheses here and 3 times 2i is going to give me 6i
44:49
plus 6 inside the end result you get here is 8 or
44:54
minus 8 minus 3 is minus 11 and minus 4 plus 6 is going to be
45:02
2. so i get minus 11 plus 2i for my multiplication here i guess i'm
45:07
going to circle that answer i should circle this answer as well now slightly more complicatedly w over z
45:16
w is three plus four i and z is minus one plus two i
45:24
and you know when you want to simplify an expression like this you multiply by the complex conjugate of the denominator
45:29
divided by the complex conjugate of the denominator so minus 1 minus 2i divided
45:35
by -1 minus 2i and if we continue
45:42
the same sort of distribution i'll do the numerator first
45:49
same sort of multiplication we just did here only the signs will be flipped a little bit we'll end up with minus three
45:55
plus eight instead of minus three minus eight and for the complex sorry for the
46:00
imaginary part we'll end up with minus 4 minus 6 instead of minus 4 plus 6
46:06
and you can work out the details of that distribution on your own if you want
46:11
the denominator is not terribly complicated since we know we're taking the absolute magnitude of a complex number by multiplying a complex number
46:18
by its complex conjugate we can just write this out as the square of the
46:24
real part 1 plus the square of the imaginary part minus 2 which squared is 4.
46:32
so if i continue this final step this is going to be 5
46:40
this is going to be minus 10 i and our denominator here is just going to be
46:45
5. so in the end what i'll end up with is going to be 1 minus 2 i so it actually ended up being
46:52
pretty simple in this case now for the absolute magnitude of w
46:58
3 plus 4 i you can think of this as w times w star
47:06
square root you can think of this as the square root of the real part of w plus the imaginary
47:13
part of w sorry square root of the squared of the real real part plus the square of
47:18
the imaginary part which is perhaps a little easier to work with in this case so you don't have to distribute out
47:24
complex numbers in that in that way real part is three imaginary part is
47:30
four so we end up with the square root of three squared plus four squared
47:35
which is five now this was all in rectangular form
47:42
let me move this stuff out of the way a little bit
47:47
and let's do it again at least a subset of it in polar form
47:55
in polar form w three plus four i
48:00
we know the magnitude of w that's five so that's going to be our radius 5
48:07
and our e to the i theta where theta is like i said the arctan
Examples of complex numbers
48:13
since complex numbers are so important to quantum mechanics let's do a few more examples in this case i'm going to demonstrate how to manipulate complex
48:19
numbers in a more general way not so much just doing examples with numbers first example simplify this expression
48:27
you have two complex numbers multiplied in the numerator and then a division
48:34
first of all the first thing to simplify is this multiplication you have x plus iy times
48:40
ic this is pretty easy it's a simple sort of distribution we're going to have
48:47
x times ic that's going to be a complex part so i'm going to write that down a little bit to the right i x c
48:55
and then we're going to have i y times i c which is going to be minus y c that's going to be real we also have
49:02
a real part in the numerator from d here so i'm going to write this as d minus y c
49:09
plus i c that's the result of multiplying this out
49:15
that's then going to be divided by f plus i g
49:20
now in order to simplify this we have a complex number in the denominator you know you need to multiply by the complex conjugate
49:28
and divide by the complex conjugate so f minus i g divided by f minus ig
49:40
now expanding this out is a little bit messier but
49:46
fundamentally you've seen this sort of thing before you have
49:51
real part real part an imaginary part imaginary part in the numerator and then you're going to have imaginary
49:58
part real part and real part imaginary part and what you're going to end up with
50:03
from this first term you get f times d minus yc
50:08
from the second term you have minus ig times ixc which is going to give you
50:14
xcg we have a minus i times an i which is
50:20
going to give us a plus incidentally if you're having trouble figuring out something like minus i
50:26
times i think about it in the geometric interpretation this is i in the complex
50:32
plane this is minus i in the complex plane so i have one angle going up one angle
50:37
going down if i'm multiplying them together i'm adding the angles together so i essentially go up and back down and
50:42
i just end up with 1 equals i times minus i otherwise you can keep track of i
50:49
squared equals minus 1s and just count up your minus signs this
50:54
then is the real part suppose i should write that in green
50:59
unless my fonts get too confusing excuse me so that's the real part the imaginary
51:06
part then is what you get from these terms here i'm going to write an i out front
51:13
and now we have x c times f so x c f with an i from here
51:19
and then we have d minus y yc times ig
51:24
which i'll just write as g d minus yc
51:30
in the denominator we're now multiplying a number by its complex conjugate you know what to do there
51:37
f squared plus g squared this is just the magnitude of this
51:42
complex number sorry squared magnitude now this doesn't necessarily look more
51:49
simple than what we started with but this is effectively fully simplified you could further distribute this
51:55
and distribute this but it's not really going to help you very much the thing to notice about this is that
52:01
the denominator is purely real
52:06
we've also separated out the real part of the numerator and the imaginary part
52:13
of the numerator my handwriting is getting messier as i go
52:18
imaginary part of the numerator so we can look at this numerator now and say ah this is the complex number real
52:24
part imaginary part and then it's just divided by this real number which effectively is just a scaling it's it's
52:30
a relatively simple thing to do to divide by a real number as a second example
52:36
consider solving this equation for x now this is the same expression that we had
52:42
in the last problem only now we're solving it for equal to zero
52:48
so from the last page i'm going to borrow that first simplification step we did distributing
52:55
this through we had d minus y c for the real part
53:01
plus i x c for the imaginary part and that was divided by f plus i g
53:10
if we're setting this equal to zero the nice part about dealing with complex expressions like this is that 0 treated
53:17
as a complex number is 0 plus 0 i it has a real part and an imaginary part
53:22
as well it's just kind of trivial and in order for this complex number to be equal to zero the real part must be
53:29
zero and the imaginary part must be zero so we can think of this as d minus y c
53:35
plus i x c this has to equal zero and this has to
53:41
equal 0 separately so we effectively have two equations here not just 1 which is nice we have d
53:47
minus yc equals 0 and xc equals 0 which unless c equals 0 just means
53:55
x equals zero that's the only way that this equation can hold
54:01
is if x equals zero the key factor is to keep in mind that
54:06
the in order for two complex numbers to be equal both the real parts and the imaginary parts have to be equal
54:14
as a slightly more involved example consider finding this the cubed roots of one
54:20
now you know one cubed is one that's a good place to start we'll see that fall out of the algebra pretty quickly
54:27
what we're trying to do is solve the equation z cubed
54:32
equals one which you can think of as x plus i y
54:38
where x and y are real numbers cubed equals one
54:44
now if we expand out this cubic you get x cubed plus three x squared times i y
54:52
plus 3 x times i y squared plus
54:58
i y cubed and this is going to have to equal 1.
55:03
excuse me equal 1. now
55:10
looking at these expressions here we have an i y here we have an i y squared
55:16
this is going to give me an i squared which is going to be a minus sign and here i have an i y cubed this is
55:22
going to give me an i cubed which is going to be minus i so i have two complex parts
55:29
and two real parts so i'm going to rewrite that x cubed
55:35
and then now a minus sign from the i squared 3 x y squared
55:41
plus pulling an i out front the imaginary part then is going to come from this 3x squared y and this y cubed
55:49
so i've got a 3 x squared y here and then a minus y cubed minus coming from
55:55
the i squared and this
56:00
is also going to have to equal 1. now in order for this complex number to
56:06
equal this complex number both the real parts and the imaginary parts have to be equal
56:13
so let's write those two separate equations x cubed minus three x y
56:18
squared equals the real part of this is the real part of the left hand side has to equal the real part of the right hand side one
56:26
and the imaginary part of the left hand side three x squared y minus y cubed has
56:32
to equal the imaginary part of the right hand side zero so those are our two equations
56:39
this one in particular is pretty easy to work with um we can simplify this
56:45
this is you know we can factor a y out this is y times three x squared minus y squared
56:52
equals zero one possible solution then is going to come from this
56:58
you know you have a product like this equals zero either this is equal to zero or this is equal to zero and saying y
57:05
equals to zero is rather straightforward so let's say y equals zero
57:11
and let's substitute that into this expression that's going to give us x cubed equals 1
57:18
which might look a lot like the equation we started with z cubed equals 1 but it's subtly different because z is a
57:25
general complex number whereas our assumption in starting the problem this way is that x is a purely real number
57:32
so a purely real number which when cubed gives you 1 that means x equals 1.
57:38
so x equals one y equals zero that's one of our solutions z equals
57:44
one plus zero i or just zero z equals one now we could have told me that right off
57:50
the bat z z cubed equals one well z one possible solution is that z equals one since one
57:56
cubed is one the other thing we can do here is we can
58:02
say three x squared minus y squared is equal to zero
58:10
this means that i'll just cheat a little bit and simplify this 3x squared equals y
58:16
squared now i can substitute this in this y squared into this expression as
58:23
well and what you end up with is x cubed minus 3x and then y squared was equal to
58:31
3x squared so 3x squared is going to go in there that has to equal 1.
58:38
now let's move up here what does that leave us with that says
58:45
x cubed minus nine x cubed equals one so minus
58:54
eight x cubed equals one this means x again being a purely real
59:01
number is equal to minus one-half minus one-half times minus one-half times
59:07
minus one-half times eight times minus one is equal to one you can check that pretty easily
59:18
now where does that leave us
59:24
where do they go that leaves us substituting this back in to this expression which tells us that
59:31
three x squared equals y squared x equals minus one half so
59:36
three minus one half squared equals y squared
59:42
which tells you that y equals plus or minus the square root of three fourths
59:48
if you finish your solution so now we have two solutions for y here
59:53
coming from one value for x and that gives us our other two solutions to this
59:58
cubic we have a cubic equation we would expect there to be three solutions especially when we're working with complex numbers like this
1:00:05
and this is our other solution z equals minus one half
1:00:10
plus or minus the square root of three fourths i
1:00:18
so those are our three solutions now
1:00:24
finding the cubed roots of one to be these complex numbers is not
1:00:29
necessarily particularly instructive however there's a nice geometric interpretation the cubed roots of unity
1:00:36
like this the nth roots of unity doesn't have to be a cubed root
1:00:41
all lie on a circle of radius 1 in the complex plane and if you check the complex magnitude
1:00:48
of this number the complex magnitude of this number you will find that it is indeed unity
1:00:55
to check your understanding of this slightly simpler problem is to find the square roots of i
1:01:04
um put another way you've got z some generic complex number here equals to x
1:01:10
squared plus x plus i y quantity squared if that's going to equal y we'll expand this out solve for x and y
1:01:18
in the two equations that will result from setting real and imaginary parts equal to each other
1:01:23
same as with the cubed roots of one the square roots of i will also fall on a circle of radius one in the complex
1:01:30
plane so those are a few examples of how
1:01:36
complex numbers can actually be manipulated in particular finding the roots of unity
1:01:41
there are better formulas for that than the approach that we took here but i feel this was hopefully instructive
Probability in quantum mechanics
1:01:48
if probability is at the heart of quantum mechanics what does that actually mean well the fundamental source of
1:01:54
probability in quantum mechanics is the wave function psi psi tells you everything that you can in
1:02:00
principle know about the state of the system but it doesn't tell you everything with perfect precision
1:02:07
how that actually gives rise to probability distributions in observable quantities like position or energy or
1:02:13
momentum is something that we'll talk more about later but from the most basic perspective
1:02:19
psi can be thought of as related to a probability distribution but let's take a step back and talk
1:02:25
about probabilistic measurements in general first if i have some space
1:02:31
let's say it's position space say this is the floor of a lab
1:02:37
and i have a ball that is somewhere on in the floor somewhere on
1:02:43
the floor i can measure the position of that ball maybe i measure the ball to be there on
1:02:48
the floor if i prepare the experiment in exactly the same way attempting to put the ball
1:02:55
in the same position on the floor and measure the position of the ball again i won't always get the same answer
1:03:00
because of perhaps some imprecision in my measurements or some imprecision in how i'm reproducing the
1:03:06
system so i might make a second measurement there or a third measurement there
1:03:14
um if i repeat this experiment many times i'll get a variety of measurements at a variety of locations
1:03:21
and maybe they cluster in certain regions or maybe they're very unlikely in other regions
1:03:26
but this distribution of measurements we can describe that mathematically with the probability distribution
1:03:32
uh probability distribution for instance i could plot p of x here and p of x tells you roughly
1:03:40
how many or how likely you are to make a measurement so i would expect p of x as a function to be larger here where
1:03:46
there's a lot of measurements and 0 here where there's no measurements and relatively small here where there's few
1:03:51
measurements so p of x might look something like this
1:03:59
so the height of p of x here tells us how likely we are to make a measurement in a given location
1:04:05
this concept of a probability distribution is intimately related to the wave function
1:04:12
so the most simple way that you can think of probability in quantum mechanics is to think of the wave
1:04:17
function psi of x now psi of x you know is a complex function
1:04:22
and a complex number can never really be observable what would it mean for example to measure
1:04:28
a position of say two plus three i meters this isn't something that's going to
1:04:33
occur in the physical universe but the fundamental interpretation of
1:04:38
quantum mechanics that most that your book in this book in particular that most uh physicists think
1:04:45
of is the interpretation that psy in the context of
1:04:52
a probability distribution the absolute magnitude of psi squared
1:04:58
is related to the probability of finding the particle
1:05:06
described by psi
1:05:12
so if the squared magnitude of psi is large at a particular location
1:05:17
that means it is likely that the particle will be found at that location
1:05:23
now the squared magnitude here means that we're not that we have to say well we have to take the squared
1:05:29
magnitude of psi we can't just take psi itself so for instance in the context of the plot that i just made on the last page
1:05:36
if this is x and our y axis here
1:05:42
is psi psi has real and imaginary parts so the real part of psi might look
1:05:47
something like this and the imaginary part might look
1:05:52
something like this
1:05:58
and the squared magnitude would look something like well what you can imagine the square
1:06:03
magnitude of that function looking like you can think of the squared magnitude
1:06:09
of size the probability distribution let me move this up a little bit give myself some more space
1:06:16
the squared magnitude of psi then can be thought of as a probability distribution
1:06:21
in the likelihood of finding the particle at a particular location like i said now what does that mean
1:06:27
mathematically mathematically suppose you had two positions a and b and you wanted to know what the
1:06:35
probability of finding the particle between a and b was
1:06:40
given a probability distribution you can find that by integrating the probability distribution
1:06:48
so the probability that the particle is between
1:06:54
a and b is given by the integral from a to b
1:07:01
of the squared absolute magnitude of psi
1:07:06
dx you can think of this as a definition
1:07:12
you can think of this as an interpretation but fundamentally this is what the physical meaning of the wave function is
1:07:20
it is related to the probability distribution of position associated with this particular state of
1:07:27
the system now what does that actually mean
1:07:32
and that's a bit of a complicated question it's very difficult to answer suppose i have
1:07:38
a wave function which i'm just going to write as the square plot is the square of magnitude
1:07:43
of psi now suppose it looks something like this
1:07:52
now that means i'm perhaps likely to measure the position of the particle somewhere in the middle here
1:07:58
so suppose wrong color so suppose i do that
1:08:04
suppose i measure the position of the particle here
1:08:10
so i've made a measurement now
1:08:15
messy handwriting i've made a measurement and i've observed the particle b here
1:08:21
what does that mean in the context of the wave function now everything that i can possibly know about the particle has
1:08:26
to be encapsulated in the wave function so after the measurement when i know the particle is here you can
1:08:32
think of the wave function as looking something like this it's not going to be infinitely narrow because there might
1:08:39
be some uncertainty the width of this is related to the precision of the measurement
1:08:45
but the wave function before the measurement was broad like this and the wave function after the measurement is
1:08:50
narrow what actually happened here what about the measurement caused this to happen this is one of the deep issues in
1:08:57
quantum mechanics that is quite difficult to interpret so what do we make of this
1:09:05
well one thing that you could think
1:09:11
just intuitively is that well this probability distribution wasn't really all the information that was there
1:09:17
really the particle was there let's say this is point c
1:09:22
one interpretation is that the particle
1:09:28
really was at c all along
1:09:35
that means that this distribution reflects ignorance on our part as physicists not fundamental uncertainty
1:09:42
in the physical system this turns out to not be true and you
1:09:47
can show mathematically and in experiments that this is not the case
1:09:54
the main interpretation that physicists use is to say that this wave function psi here
1:10:03
also shown here collapses
1:10:09
now that's a strange term collapses
1:10:17
but it's hard to think of it any other way suppose you were concerned with the wavefunction's value here
1:10:22
before the measurement it's non-zero whereas after the measurement it's zero
1:10:28
so this decrease in the wave function out here is a well it's reasonable to call that a
1:10:34
collapse what that wave function collapse means is
1:10:39
subject to some debate and there are other interpretations one interpretation that i'll mention
1:10:44
very briefly but we won't really discuss very much is the many worlds interpretation and that's that when you make a measurement like this
1:10:52
the universe splits so it's not that the wave function all
1:10:57
of a sudden decreases here it's that for us in our tiny little chunk of the
1:11:02
universe the wave function is now this and there's another universe
1:11:08
somewhere else where the wave function is this because the particle is observed to be here
1:11:14
don't worry too much about that but the interpretation issues in quantum mechanics are really fascinating once
1:11:20
you start to get into them you can think about this as the universe splitting
1:11:26
into oh sorry splits the universe you can think about this as the universe splitting into many
1:11:32
little subuniverses where the probability of uh observable where the particle is observed at a variety of
1:11:38
locations one location per universe really this question of how measurements take
1:11:45
place is really fundamental but hopefully this explains a little bit of where
1:11:51
probability comes from in quantum mechanics the wave function itself can be thought of as a probability distribution
1:11:57
for position measurements and unfortunately the measurement process is not something that's
1:12:02
particularly easy to understand but that's the fundamental origin of probability in quantum mechanics
1:12:11
to check your understanding here is a simple question about probability distributions and how to interpret them
Variance of probability distribution
1:12:18
variance and standard deviation are properties of a probability distribution that are related to the uncertainty
1:12:24
since uncertainty is such an important concept in quantum mechanics we need to know how to quantify how uncertainty results from probability distributions
1:12:32
so let's talk about the variance and the standard deviation these questions are related to the shape
1:12:39
of a probability distribution so if i have a set of coordinates let's say this is
1:12:45
the x-axis and i'm going to be plotting then the
1:12:50
probability density function as a function of x probability distributions come in lots of shapes and sizes
1:12:57
you can have probability distributions that look like this probability distributions that look like
1:13:02
this you can even have probability distributions that look like this or probability distributions that look
1:13:09
like this and these are all different the
1:13:14
narrow peak here versus the broad distribution here
1:13:23
the distribution with multiple peaks or multiple modes in this case it has two modes so we call this distribution
1:13:30
bimodal or multimodal and then this distribution
1:13:35
which is asymmetric has a long tail in the positive direction and a short tail in the negative direction
1:13:41
we would say this distribution is skewed so distributions have lots of different
1:13:48
shapes and if what we're interested in is the uncertainty you can think about that roughly as the width of the
1:13:53
distribution for instance if i'm drawing random numbers from the orange distribution the narrow one here
1:13:58
they'll come over roughly this range whereas if i'm drawing from the blue distribution
1:14:04
they'll come over roughly this range so if this were say the probability
1:14:10
density for position say this is the squared magnitude of the wave function for a
1:14:15
particle i know where the particle represented by the orange
1:14:20
distribution is much more accurately than the particle represented by the blue distribution
1:14:27
so this concept of width of a distribution and the uncertainty in the position for
1:14:33
instance are closely related the broadness
1:14:40
is related to the uncertainty
1:14:46
uh this is fundamental to quantum mechanics so how do we quantify it in statistics the the broadness of a
1:14:53
distribution is called the variance variance is a way of measuring the
1:14:58
broadness of a distribution for example so suppose this is my distribution
1:15:04
the mean of my distribution is going to fall roughly in the middle here let's say that's the expected value of x
1:15:09
if this is the x-axis now if i draw a random number from this
1:15:14
distribution i won't always get the expected value suppose i get a value here
1:15:20
if i'm interested in the typical deviation of this value from the mean that will tell me something about how
1:15:26
broad this distribution is so let's define this displacement here to be delta x
1:15:33
delta x is going to be equal to x minus the
1:15:38
expected value of x and first of all you might think well if i'm looking for the typical values of
1:15:44
delta x let's just try the expected value of delta x well what is that
1:15:50
unfortunately the expected value of x doesn't really work for this purpose because delta x is positive if you're on
1:15:55
this side of the mean and negative if you're on this side of the mean so the expected value of delta x
1:16:01
is zero sometimes it's positive sometimes it's negative and they end up cancelling out
1:16:08
now if you're interested in only positive numbers the next guess you might come up with is let's use
1:16:14
not delta x but let's use the absolute value of delta x what is that
1:16:19
well absolute values are difficult to work with since you have to keep track of whether a number is positive or
1:16:25
negative and keep flipping signs if it's negative so this turns out to just be kind of
1:16:30
painful what is this what statisticians and physicists do in the end then is instead
1:16:36
of taking the absolute value of a number just to uh make it positive we square it so you calculate the expected
1:16:43
value of the squared deviation sort of the mean squared deviation
1:16:49
this has a name in statistics it's written as sigma squared and it's called the variance
1:16:57
to do an example let's do a discrete example
1:17:07
suppose i have two probability distributions all with equally likely outcomes say the
1:17:12
outcomes of one distribution are one two and 3 while the outcomes for the second
1:17:18
distribution are 0 2 and 4.
1:17:23
photographically these numbers are more closely spaced than these numbers
1:17:29
so i would expect the broadness of this distribution to be larger than the broadness of this distribution
1:17:35
you can calculate this out by calculating the mean squared deviation so first of all we need to know the mean
1:17:42
expected value of x is 2 in this case and also in this case
1:17:51
knowing the expected value of x you can calculate the deviations
1:17:57
so let's say delta x here is going to be -1 0 and 1 are the possible deviations
1:18:03
from the mean for this probability distribution whereas in this case it's -2 0 and 2.
1:18:11
then we can calculate the delta x squareds that are possible and you get 1
1:18:17
0 and 1 for this distribution and
1:18:22
4 0 and 4. for this distribution now when you calculate the mean
1:18:30
of these squared deviations in this case the expected
1:18:36
value of the squared deviation is two thirds whereas in this case
1:18:42
the expected value of the squared deviation is eight thirds
1:18:49
so indeed we did get a larger number for the variance in this distribution
1:18:56
so you can think of that as the definition this is not the easiest way of calculating the variance though
1:19:04
it's actually much easier to calculate the variance as an expected value of a squared quantity and an expected and
1:19:09
minus the square of the expected value of the quantity itself so the mean of the square minus the square of the mean
1:19:16
if that helps you to remember it you can see how this results fairly easily by plugging through some
1:19:22
basic algebra so given our definition the expected value of delta x squared
1:19:30
we're calculating an expected value so suppose we have a continuous distribution now the continuous
1:19:35
distribution expected value has an integral in it so we're going to have the integral of delta x squared
1:19:43
times rho of x dx now delta x squared we can we know what
1:19:50
delta x is delta x is x minus the expected value of x so we can plug that
1:19:55
in here and we're going to get the integral of x minus expected value of x
1:20:01
squared times rho of x dx i can expand this out
1:20:08
and i'll get integral of x squared minus 2 x expected value of x
1:20:15
plus expected value of x quantity squared rho of x dx
1:20:23
and now i'm going to split this integral up into three separate pieces first piece integral of x squared rho of
1:20:31
x dx second piece integral of 2 x expected value of x
1:20:39
rho of x dx and third piece integral of expected value of x squared
1:20:48
rho of x dx now this integral
1:20:53
you recognize right away this is the expected value of x squared
1:21:00
this integral i can pull this out front since this is a constant this is just a number this is the expected value
1:21:08
so this integral is going to become 2 i can pull the 2 out of course as well
1:21:15
2 times the expected value of x and then what's left is the integral of x
1:21:20
rho of x dx which is just the expected value of x this integral again this is a constant
1:21:26
so i can pull it out front and when i do that i end up with just the integral of rho of x dx
1:21:34
and we know the integral of rho of x dx over the entire domain i should specify that this is the
1:21:40
integral from minus infinity to infinity now all of these are integrals from minus
1:21:46
infinity to infinity the integral of minus infinity to infinity of rho of x dx is 1.
1:21:52
so this after i pull the expected the expected value of x quantity squared out is just going to be the expected value
1:21:59
of x quantity squared so this is expected value of x squared
1:22:05
this is well i can simplify this as well this is the expected value of x quantity squared
1:22:10
as well so i'm going to erase that and say squared there so i have this minus twice this plus
1:22:17
this and in the end that gives you expected value of x squared minus
1:22:23
the expected value of x squared so mean of the square minus the square of
1:22:28
the mean to check your understanding of how to use this formula i'd like you to
1:22:35
complete the following table now i'll give you a head start on this
1:22:42
if your probability distribution is given by 1 2 4 5 and 8 all
1:22:49
equally likely you can calculate the mean
1:22:56
now once you know the mean you can calculate the deviations x minus the mean which i'd like you to
1:23:01
fill in here then square that quantity and fill it in here and take the mean of that square
1:23:06
deviation same as what we did when we talked about the variance as the mean squared deviation
1:23:13
then taking the other approach i'd like you to calculate the
1:23:18
squares of all of the x's and calculate the mean square you know the mean you know the mean
1:23:24
square you can calculate this quantity mean of the square minus the square of
1:23:30
the mean and you should get something that equals the mean squared deviation
1:23:36
that's about it for variance but just to say a little bit more about this
1:23:43
variance is not the end of the story it turns out there's well there's more
1:23:52
i mentioned the distributions that we were talking about earlier on the first slide here
1:24:01
keep forgetting to turn my ruler off the distributions that look like this versus distributions that look like this
1:24:09
this is a question of symmetry and the mathematical name for this is skew or skewness
1:24:18
there's also distributions that look like this
1:24:29
versus distributions that look like this
1:24:35
and this is what mathematically this is called kurtosis
1:24:42
which kind of sounds like a disease or perhaps a villain from a comic book kurtosis has to do with the relative
1:24:48
weights of things near the peak versus things in the tails now mathematically speaking you know the
1:24:54
variance sorry let me go back a little further you know the mean
1:25:01
that was related to the integral of x
1:25:06
rho of x dx we also just learned about the variance
1:25:13
which was related to the integral of x squared rho of x dx
1:25:20
it turns out the skewness is related to the integral of x cubed row of x
1:25:26
dx and the kurtosis is related to the integral of x to the fourth row of x dx
1:25:33
at least those are common ways of measuring skewness and kurtosis these are not exact formulas for skewness and kurtosis nor is this an
1:25:40
exact formula for the variance of course so i'm taking some liberties with the math but you can imagine well what happens if
1:25:45
you take the integral of x to the fifth row of x dx you could keep going and you would keep
1:25:52
getting properties of the probability distribution that are relevant to its shape
1:25:57
now you won't hear very much about skewness and kurtosis in physics but i thought you should know that this field does sort of continue on
1:26:04
for the purposes of quantum mechanics what you need to know is that variance is related to the uncertainty and we will be doing lots of calculations of
1:26:10
variance on the basis of probability distributions derived from wave functions in this class
Normalization of wave function
1:26:17
we talked a little bit about the probabilistic interpretation of the wave function psi that's one of the really remarkable
1:26:22
aspects of quantum mechanics that there are probabilities rolled up in your description of the physical state
1:26:29
we also talked a fair amount about probability itself and one of the things we learned was that probabilities had to be normalized meaning the total sum of
1:26:36
all of the probable outcomes the probabilities of all of the outcomes in a probability distribution has to equal
1:26:41
1. that has some implications for the wave function especially in the context of the schrodinger equation so let's talk
1:26:48
about that in a little more detail normalization in the context of a probability distribution
1:26:54
just means that the integral from minus infinity to infinity of rho of x
1:27:00
dx is equal to 1. you can think about that as the
1:27:05
sort of extreme case of the probability that say x is between a and b
1:27:11
being given by the prob the integral from a to b of row of x
1:27:16
dx in the context of the wave function
1:27:23
that that statement becomes the probability that the particle
1:27:29
is between a and b is given by the integral from a to b of
1:27:37
the squared magnitude of psi of x integrated between a and b
1:27:45
so this is the same sort of statement you're integrating from a to b and in
1:27:51
the case of the probability density you have just the probability density in the case of the wave function you have the squared absolute magnitude of the wave
1:27:58
function this is our probabilistic interpretation we're may sort of making an analogy between psi
1:28:03
squared magnitude and a probability density this normalization condition then has to
1:28:10
also hold for psi if the squared magnitude of psi is going to is going to
1:28:15
be treated as a probability density so integral from minus infinity to infinity of
1:28:21
squared absolute magnitude of psi dx has to equal
1:28:26
1. this is necessary for our statistical interpretation of
1:28:32
the wave function this brings up an interesting question
1:28:38
though because not just any function can be a probability distribution therefore
1:28:43
this normalization condition treating size of probability density means there are some conditions on what
1:28:49
sorts of functions are allowed to be wave functions this is a question of normalizability
1:28:55
suppose for instance i had a couple of functions that i was interested in
1:29:01
say one of those functions looks sort of like this keeps on rising
1:29:07
as it goes to infinity if i wanted to consider the squared magnitude
1:29:13
of this function this is our possible psi this is our
1:29:20
possible psi squared sorry about the messy there this function
1:29:26
since it's going to you know it's it's continuing to increase as x increases both in the
1:29:31
negative direction and in the positive direction its squared magnitude is going to look something like this
1:29:37
i can do a little better there sorry
1:29:44
if i tried to say calculate the integral from minus infinity to infinity of this
1:29:50
function i've got a lot of area out here
1:29:55
from say 3 to infinity where the wave function is positive
1:30:01
this would go to infinity therefore what that means is that this function is
1:30:06
not normalizable
1:30:12
not all functions can be normalized if i drew a different function for example something that looked maybe
1:30:18
something like this its squared magnitude might look something like this
1:30:28
there is a finite amount of area here so if i integrated the squared magnitude
1:30:33
of the blue curve i would get something finite
1:30:38
what that means is that whatever this function is i could
1:30:45
multiply or divide it by a constant such that this area was equal to one
1:30:51
i could take this function and convert it into something such that the integral from minus infinity to infinity of the
1:30:56
squared magnitude of psi equaled one and it obeyed our sort of statistical constraint on the probability
1:31:02
distribution in order for this to be possible psi has to have this property and the
1:31:09
mathematical way of stating it is that psi must be
1:31:16
square integrable and all this means is that the integral
1:31:22
from minus infinity to infinity of the squared magnitude of psi
1:31:27
is finite you don't get zero you don't get
1:31:32
infinity in order for this square integrability
1:31:38
to hold for example though you need a slightly weaker condition that
1:31:44
psi goes to zero as x goes to either plus or minus
1:31:49
infinity it's not possible to have a function that stays non-zero or goes to infinity
1:31:56
itself as x goes to infinity and still have things be integrable
1:32:06
like i said if this holds if this integral here is finite you can convert any function into
1:32:13
something that is normalized by just multiplying or dividing by a constant
1:32:19
is that possible though in the schrodinger equation does multiplying or dividing by a constant do anything
1:32:26
well the schrodinger equation here
1:32:33
you can just glance at it and see that multiplying and dividing by a constant doesn't do anything the short injury equation is i
1:32:39
h bar partial derivative with respect to time of psi equals minus h bar squared over 2m
1:32:47
second derivative of psi with respect to position plus the potential times psi
1:32:54
now if i made the substitution psi went to
1:33:00
some multiple or some constant a multiplied by psi
1:33:06
you can see what would happen here i would have psi times a here i would have psi times a and here i would have psi
1:33:13
times a so i would have an a here an a here and an a here so i could divide through this
1:33:19
entire equation by a and all of those a's would disappear and i would just get the original schrodinger equation back
1:33:25
what that means is that if psi solves the schrodinger equation
1:33:34
a psi does 2. i'll just say a psi works
1:33:43
now this is only if a is a constant does not depend on time does not depend
1:33:49
on space if a depended on time i would not be able to divide it out of
1:33:54
this partial derivative because the partial derivative would act on on that a same goes for if a was a function of
1:34:00
space if a was a function of space i wouldn't be able to divide it out of this partial derivative with respect to x
1:34:07
so this only holds if a is a constant that means that i might run into some
1:34:12
problems with time evolution i can choose a constant and i can multiply psi by that constant
1:34:19
such that psi is properly normalized at say time t equals zero but will that
1:34:25
hold for future times it's a question of normalization and time evolution
1:34:30
what we're really interested in here is the integral from minus infinity to infinity of
1:34:37
psi of x and time squared dx
1:34:44
if this is going to always be equal to 1
1:34:50
supposing it's equal to 1 at some initial time what we really want to know is what the
1:34:55
time derivative of this is if the time derivative of this is equal to zero then we'll know that whatever
1:35:02
the normalization of this is it will hold throughout the evolution of the well throughout the evolution of
1:35:08
the wave function now i'm going to make a little bit of simplifying notation here and i'm going
1:35:14
to drop the integral limits since it takes a while to write
1:35:21
and we're going to mult or sorry we're going to manipulate this expression
1:35:26
a little bit we're going to use the schrodinger equation we're going to use the rules of complex numbers
1:35:32
i'm going to use the rules of differential calculus i'm going to get something that will
1:35:37
show that indeed this does hold so let's step through that manipulations of the schrodinger equation like this
1:35:44
are a little tricky to follow so i'm going to go slowly and if it seems like i'm being extra pedantic please bear with me some of the
1:35:51
details are important so the first thing that we're going to
1:35:57
do pretty much the only thing that we can do with this equation is we're going to exchange the order of
1:36:02
integration and differentiation instead of differentiating with respect to time the integral with respect to x
1:36:09
we're going to integrate with respect to x of
1:36:15
the time derivative of this psi of x and t
1:36:21
quantity squared basically i've just pushed the derivative inside the integral
1:36:27
now notationally speaking i'm going to move some stuff around here give myself a little more room
1:36:32
and notationally oops didn't mean to change the colors notationally speaking here
1:36:39
the d dt became a partial derivative with respect to time
1:36:44
the total derivative d by dt is now a partial
1:36:52
what the notation is keeping track of here is just the fact that this
1:36:58
is a function only of time since you've integrated over x and you've substituted in limits
1:37:04
whereas this is a function of both space and time
1:37:11
so whereas this derivative is acting on something that's only a function of time i can write it as a simple d by dt
1:37:18
a total derivative in this case since what the derivative is acting on as a function of both
1:37:23
position and time i have to treat this as a partial derivative now
1:37:31
so the next thing that we're going to do aside from after pushing this derivative
1:37:37
inside and converting it to a partial derivative is rewrite this squared absolute magnitude of psi as
1:37:44
psi star times psi now the squared absolute magnitude of a
1:37:50
complex number is equal to the complex number times its complex conjugate
1:37:56
it's just simple complex analysis rules there so what we've got is the integral of the
1:38:02
partial derivative with respect to time of psi star times psi
1:38:07
integral dx now we have a time derivative applied to a product we can apply the product rule
1:38:16
from differential calculus what we end up with is the integral of the partial derivative with respect to
1:38:22
time of psi star times psi plus psi star
1:38:28
partial derivative of psi with respect to time that's integrated dx
1:38:36
now what i'm going to do is i'm going to notice these partial derivatives with respect to time
1:38:44
and i'm going to ask you to bear with me for a minute while i make a little more space
1:38:51
it's probably a bad sign if i'm running out of space on a computer where i have effectively infinite space
1:38:58
but bear with me the partial derivatives with respect to time appear in the schrodinger equation i h
1:39:05
bar d by dt of psi equals minus h bar squared over 2m partial derivative
1:39:13
second partial derivative of psi with respect to position plus potential times psi
1:39:22
these are the time derivatives that i'm interested in i can use the schrodinger equation to
1:39:28
substitute in say the right hand side for these time derivatives both for psi star and for psi
1:39:35
so first i'm going to manipulate this by dividing through by i h bar which gives me
1:39:41
d partial psi partial time equals i h bar over 2m
1:39:50
second partial of psi with respect to x minus
1:39:55
where did it go i v over h bar
1:40:02
psi so that can be substituted in here i also need to know something for the
1:40:08
complex conjugate of psi so i'm going to take the complex conjugate of this entire equation what that looks like is partial
1:40:14
derivative of psi star with respect to time now i'm taking the complex conjugate of
1:40:20
this so i have a complex part here the sign of that needs to be flipped and i have a complex number here
1:40:26
that needs to be complex conjugated since the complex conjugate of a product is the product of the complex conjugates
1:40:34
what that means is this is going to become minus i h bar over 2 m
1:40:40
d squared psi star dx squared sorry i forgot the squared
1:40:45
there my plus i v over h bar
1:40:51
psi so i've just gone through and changed the signs on all of the imaginary parts of all these numbers psi became psi star
1:40:58
i became minus i minus i became i this can be substituted in for that
1:41:07
what you get when you make that substitution this equation isn't really getting simpler is it it's getting longer
1:41:12
what you get is the integral of something i'll put an open square brackets at the beginning here
1:41:18
i've got this equation minus i h bar over 2m
1:41:25
second partial derivative of psi star partial x squared
1:41:31
plus i v over h bar
1:41:36
psi star that's multiplied by psi from here
1:41:42
so i've just substituted in this expression for this
1:41:48
now the next part i have plus psi star and whatever i'm going to substitute in from
1:41:55
this which is what i get from this version of the schrodinger equation here i h bar over 2m
1:42:03
second partial derivative of psi with respect to x
1:42:08
minus i v over h bar psi
1:42:14
close parentheses close square brackets and i'm integrating dx
1:42:20
now this doesn't look particularly simple but if you notice what we've got here
1:42:26
this term if i distributed this psi in would have i v over h bar psi star times psi
1:42:34
this term if i distributed this psi star in would have an i v over h bar psi star and psi
1:42:42
this term has a plus sign this term has a minus sign so these terms actually cancel out
1:42:51
what we're left with then to rewrite things both of the terms that remain have this minus i h bar over 2m out front
1:43:01
so we're going to have equals to i h bar over 2m
1:43:07
and here i have a minus second partial derivative of psi star
1:43:13
with respect to x times psi and here i have plus
1:43:19
psi star times the corresponding second partial of psi with respect to x
1:43:26
and this is integrated dx is that all right yes now what i'd like
1:43:32
you to notice here is that we've got d by dx and we've got an integral dx
1:43:40
we don't have any time anymore so we're making progress and we're actually almost done
1:43:50
where where did we get so far we started with the time derivative of this
1:43:55
effective total probability which should have been equal to one if
1:44:01
which would be equal to one if this were proper probability distribution but we're just considered with the time evolution
1:44:07
since we know that we whatever psi is we can multiply it by some
1:44:12
constant to make it properly normalized at a particular time now we're interested in the time evolution
1:44:19
we're looking at the time derivative of this and we've gone to this expression which has complex conjugates of psi and
1:44:25
second partial derivatives with respect to x
1:44:31
now what i'd like you to do and this is a check your understanding question
1:44:37
is think about why this statement is true this is the partial derivative with
1:44:43
respect to x of psi star d psi dx
1:44:49
minus d psi star dx so sorry i'm saying d i should be saying partial these are partial
1:44:55
derivatives
1:45:00
this is true and it's up to you to figure out why but since this is true what we're left
1:45:06
with is we have our i h bar over 2m an integral
1:45:12
over minus infinity to infinity of this expression partial with respect to
1:45:17
x of psi star partial psi partial x minus
1:45:22
partial psi star partial x psi
1:45:28
we're integrating dx now and this is nice because we're integrating dx
1:45:34
of a derivative of something with respect to x so
1:45:40
that's easy fundamental theorem of calculus we end up with i h bar
1:45:46
over 2m psi star partial psi partial x minus
1:45:52
partial psi star partial x psi evaluated at the limits of our integral
1:45:59
which are minus infinity to infinity now if psi is going to be normalizable
1:46:07
we know something about the value of psi at negative and positive infinity
1:46:17
if psi is normalizable psi has to go to zero
1:46:22
as x goes to negative and positive infinity
1:46:30
what that means is that when i plug in the infinity here
1:46:36
psi star d psi dx d psi e x and psi they're all everything here is going to
1:46:42
be 0. so when i enter in my limits i'm just going to get 0 and 0.
1:46:47
so the bottom line here after all of this manipulation is that this is equal to 0.
1:46:55
what that means is that the integral from negative infinity to infinity of the squared absolute magnitude of psi
1:47:02
as a function of both x and time is equal to a constant
1:47:12
put another way time evolution
1:47:22
does not affect normalization
1:47:33
what that means is that i can take my candidate wave function
1:47:38
not normalized integrate it find out what i would have
1:47:43
to multiply or divided by to make it normalized and if i'm successful i have my
1:47:49
normalized wave function i don't need to worry about how the system evolves in time the
1:47:54
schrodinger equation does not affect the normalization
1:48:04
so this is that check your understanding question i mentioned the following statement was that crucial
1:48:10
step in the derivation and i want you to show that this is true explain why in your
1:48:15
own words
1:48:20
now to do an example here normalize this wave function
1:48:26
what that means is that we're going to have to find a constant and i've already put the constant in the wave function a
1:48:33
such that the integral from minus infinity to infinity of the squared absolute
1:48:38
magnitude of psi of x in this case i've left the time dependence out is equal to
1:48:44
1. and same as in the last problem the first thing we're going to do is substitute the squared absolute
1:48:51
magnitude of psi for psi star times psi
1:48:56
the other thing i'm going to do before i get started is notice that my wavefunction is zero if the absolute
1:49:02
value of x is greater than one meaning for x above one or below negative one
1:49:09
so instead of integrating from minus infinity to infinity here i'm just going to focus on the part where psi is non-zero and integrate from -1 to 1.
1:49:17
integral from minus 1 to 1 of psi star
1:49:23
which is going to be a e to the ix is going to become e to the minus ix
1:49:29
and 1 minus x squared is still going to be 1 minus x squared
1:49:34
now i have a complex conjugated a because part of the assumption about normalization constants like this is
1:49:40
usually that you can choose them to be purely real so i'm not going to worry about taking the complex conjugate of a just to make
1:49:47
my life a little easier psi well that's just right here a e to the
1:49:52
ix 1 minus x squared i'm integrating dx
1:50:00
this is psi star this is psi integral dx from -1 to 1 should be equal to 1.
1:50:07
so let's do this we end up with a squared times the integral from -1 to 1 of
1:50:14
e to the minus ix and e to the ix what's e to the minus ix times e to the ix
1:50:20
well thinking about this in terms of the geometric interpretation we have e to the ix which is cosine theta plus i
1:50:27
sine theta you can think about that as being somewhere on the unit circle at an angle
1:50:32
theta minus i x or minus i theta would just be
1:50:38
in the exact opposite direction so when i multiply them together i'm going to get something that has the product of the magnitudes
1:50:45
the magnitudes are both one and it's purely real
1:50:51
you can see that also by looking at just the the rules for multiplying exponentials like this
1:50:57
e to the minus ix times e to the plus ix is e to the minus ix plus ix or e to the
1:51:02
zero which is one so i can cancel these out
1:51:08
and what i'm left with is 1 minus x squared quantity squared dx
1:51:14
plugging through the algebra a little further a squared integral minus 1 to 1 of 1 minus 2x squared
1:51:22
plus x to the fourth dx
1:51:28
you can do this integral equals a squared 2
1:51:34
sorry x
1:51:39
minus two thirds x cubed plus
1:51:45
x to the fifth over five we know in quantum mechanics that all of the information about the physical
Position, velocity and momentum from the wave function
1:51:51
system is encapsulated in the wave function psi psi then ought to be related to
1:51:56
physical quantities for like like example for example position velocity and momentum of the particle
1:52:01
we know a little bit about the position we know how to calculate things like the expected value of the position
1:52:07
and we know how to calculate the probability that the particles within a particular range of positions
1:52:12
but what about other dynamical variables like velocity or momentum the connection with velocity and
1:52:18
momentum brings us to the point where we really have to talk about operators operators are one of our
1:52:24
fundamental concepts in quantum mechanics and they connect the wave function with physical quantities but let's take a step back
1:52:31
first and think about what it means for a quantum system to move
1:52:37
um the position of the particle we know say the integral from a to b of the squared
1:52:43
magnitude of the wave function dx gives us the probability that the
1:52:49
particle is between a and b and we know that the expected position
1:52:56
is given by a similar expression the integral from minus infinity to infinity of psi star of x times x times psi of x
1:53:05
dx now these expressions are related you know by the fact that the squared magnitude of psi is the probability
1:53:11
density function describing position and this is really just the calculation of the expected value of x
1:53:18
given that probability density function now what if i want to know what the
1:53:23
motion of the particle is
1:53:28
one way to consider this is suppose i have a box and
1:53:34
if i know the particle is say here at time t equals zero what can quantum
1:53:40
mechanics tell me about where the particle is later physically speaking you could wait
1:53:46
until say t equals one second and then measure the position of the particle
1:53:52
maybe it would be here you could then wait a little longer
1:53:58
and measure the particle again maybe at that point it would be here
1:54:04
that say t equals two seconds or if i wait a little bit longer and measure the particle yet again
1:54:11
at say t equals three seconds maybe the particle would be up here
1:54:17
now does that mean that the particle followed a path that looked something like this no we know that the position
1:54:23
of the particle is not something that we can observe at any given time with impunity because
1:54:29
of the way the observation process affects the wave function back when we talked about measurement we
1:54:34
talked about having a wave function that looks something like this a probability density that looks
1:54:41
something like that and then after we measure the problem measure the position of the particle the probability density has changed if we say measure the
1:54:47
particle to be here the new wave function has to accommodate that new probability density function
1:54:53
the fact that measurement affects the system like this means that we really can't imagine repeatedly
1:55:00
measuring the position of a particle in the same system what we really need
1:55:07
is an ensemble that's the technical term for what we need
1:55:14
and what what an ensemble means in this context is that you have many
1:55:19
identically prepared systems
1:55:30
now if i had many identically prepared systems i could measure the position over and over and over and over again
1:55:36
once per system if i have you know 100 systems i could measure this measure the position 100 times and that would give
1:55:41
me a pretty good feel for what the probability density for position measurements is at the particular time
1:55:47
when i'm making those measurements if i wanted to know about the motion of the particle i could do that again
1:55:52
except instead of taking my 100 measurements all at the same time i would take them at slightly different times
1:55:58
so instead of this being the same system this would be these would all be excuse me these would all be different
1:56:04
systems that have been allowed to evolve for different amounts of time and as such the motion of the particle
1:56:11
isn't going to end up looking something like that it's going to end up looking like some sort of probabilistic
1:56:18
motion of the wave function in space what we're really interested in here
1:56:25
sorry i should make a note of that many i'm sorry
1:56:32
single measurement per system
1:56:41
this notion of averaging over many identically prepared systems is important in quantum mechanics because
1:56:47
of this effect that measurement has on the system
1:56:53
so what we're interested in now in the context of something like motion
1:56:59
is well can we predict this can we predict where the particle is likely to be as a function of time
1:57:12
and yes we can and what i'd like to do to talk about that is to consider
1:57:19
a quantum mechanical calculation that we can actually do the time derivative of the expected value of position
1:57:29
this time derivative tells us how the center of the probability distribution
1:57:35
if you want to think about it that way how the center of the wave function moves with time
1:57:43
so this time derivative d by dt
1:57:49
of the expected value of x that's d by dt of let's just write out the
1:57:55
expected value of x integral from minus infinity to infinity of x times
1:58:01
psi star of x psi of x where this is the probability density function
1:58:07
that described given by the wave function and this is x we're integrating dx
1:58:14
now if you remember when we talked about normalization whether the normalization of the wave function changed as the wave
1:58:20
function evolved in time we're going to do the same sort of calculation with this we're going to do some calculus
1:58:26
with this expression we're going to apply the schrodinger equation but as before the first thing we're
1:58:32
going to do is move this derivative inside the equation this is a total time derivative of something that's a
1:58:37
function of in principal position and time i should write these as functions of x
1:58:44
and t
1:58:49
and what you get when you push that in is as before the integral or the
1:58:55
total derivative becomes a partial derivative since x is just the coordinate x in
1:59:01
these contexts of functions of both space and time the total time derivative will not
1:59:06
affect the coordinate x even when it comes becomes a partial derivative so what we'll end up with is x times the
1:59:12
partial time derivative of psi star psi
1:59:18
integral dx i'm not going to write the integral from minus infinity to infinity here just to save myself some time
1:59:25
now if you remember this expression the integral or sorry not the not the full integral
1:59:33
just the partial time derivative of psi star psi that was what we worked with in the
1:59:38
lecture on normalization so if we apply the result from the electron normalization and it's equation
1:59:44
126 yes in the book if we apply that
1:59:51
you can simplify this down a lot right off the bat
1:59:57
and what you end up with is i h bar over 2 m times this integral
2:00:04
x and then what we substitute in the equation 126 is gives an expression for this highlighted part here in orange
2:00:13
and what you get is the partial derivative with respect to x of psi star
2:00:18
partial of psi with respect to x minus partial of psi star with respect to x
2:00:25
times psi integral still with respect to dx of course
2:00:34
now if we look at this equation we're making the same sort of progress we made when we
2:00:39
did the normalization derivation we had time derivatives here now we have
2:00:44
only space derivatives and we have only space derivatives in an integral over space so this is definitely progress now we
2:00:51
can start thinking about what we can do with integration by parts the first integration by parts i'm going
2:00:57
to do has the non-differential part just being x and the differential part being
2:01:04
dv is equal to you know i'm not going to have space to write this here
2:01:10
i'm going to move stuff around a little bit
2:01:18
so the differential part is dv is the partial derivative well what's
2:01:23
left of this equation the partial derivative with respect to x of psi star d psi
2:01:29
dx minus d psi dx psi sorry d psi star dx
2:01:37
psi and then there's the dx from the integral sorry i'm running out of space this
2:01:42
differential part here is just this part of the equation
2:01:49
now i can take this derivative dudx in my integration by parts procedure d u equals dx
2:01:55
and dv here is easy to integrate because this is a derivative
2:02:01
so when i integrate the derivative there i'll just end up with v equals psi star d psi
2:02:08
dx minus d psi star dx psi
2:02:16
now when i actually apply those that integration by part the boundary term here with the without
2:02:22
the integral in it is going to involve these two so i'm going to have x times
2:02:30
psi star partial psi partial x minus partial psi star
2:02:35
partial x psi and that's going to be evaluated between minus infinity and infinity the limits
2:02:42
on my integral the integral part which comes in with the minus sign is going to
2:02:48
be composed of these bottom two terms integral of
2:02:53
psi star partial psi partial x minus partial psi star
2:02:58
partial x psi and it's integral dx
2:03:04
from minus infinity to infinity now what's nice oh you know i forgot something here what
2:03:09
did i forget my leading constants i still have this i h bar over 2m out
2:03:15
there i h bar over 2m is multiplied by this
2:03:21
entire expression now the boundary terms here vanish
2:03:27
boundary terms in integration by parts and quantum mechanics will often vanish because if you're evaluating something at say infinity
2:03:33
psi has to go to zero at infinity so this term is going to vanish psi star has to go to zero at infinity so this is
2:03:38
going to vanish so even though x is going to infinity psi is going to zero
2:03:44
and if you dig into the mathematics of quantum mechanics you can show convincingly that the limit as x times
2:03:50
psi goes to infinity is going to be zero so this boundary term vanishes both at
2:03:56
infinity and at minus infinity and all we're left with is this
2:04:04
yes all you're left with is that so i'll write that over i h bar
2:04:11
over 2m times the integral of psi star
2:04:17
partial psi partial x minus partial psi star partial x psi
2:04:24
integral dx i'm actually going to split that up into two separate
2:04:30
integrals so i'll stick another integral sign in here and i'll put a dx there
2:04:36
and i'll put parentheses around everything so my leading constant gets multiplied in properly and now i'm going to apply integration
2:04:42
by parts again but this time just to the second integral here so here
2:04:48
we're going to say u is equal to psi and dv is equal to again using the fact that
2:04:55
when we do this integral if we can integrate a derivative that potentially simplifies things so this is going to be partial psi star partial x dx
2:05:05
so when we derivative take the derivative of this we're going to get d u is equal to partial psi
2:05:11
partial x and when we integrate this we're going to get v equals
2:05:16
psi star now when we do the integration when we write
2:05:22
down the answer from this integration by parts the boundary term here psi star times psi is going to vanish again because we're
2:05:29
evaluating it at a region where both psi star and psi well vanish so the boundary term
2:05:35
vanishes
2:05:42
and you notice i have a minus sign here when we do the integration by parts the
2:05:47
integral term has a minus sign in it here so we're going to have the partial psi with respect to x and psi star
2:05:53
with a minus sign coming from the integration by parts and a minus sign coming from the leading term here
2:06:00
so we're going to end up with a plus sign there so we get a minus
2:06:06
from the integral part um what that means though is that i have
2:06:13
psi star and partial psi partial x in my integration by parts i end up with
2:06:18
partial psi partial x and size star it's the same the fact that i had a minus and another minus means i get a plus so i have two
2:06:25
identical terms here the result of this then is i h bar
2:06:32
over m i'm adding a half and a half and getting one basically times
2:06:37
the integral of psi star partial psi partial x
2:06:43
dx and this is going to be something that i'm going
2:06:49
to call now the expectation of the velocity vector velocity operator
2:06:57
this is the sort of thing that you get out of operators in quantum mechanics you end up with expressions
2:07:03
like this and this i'm sort of equating just by analogy with the expectation of a
2:07:09
velocity operator this is not really a probability distribution anymore at least not obviously we started with the
2:07:16
probability distribution due to psi the absolute magnitude of psi squared and we end up with
2:07:22
the partial derivative on one of the size so it's not obvious that this is a probability distribution anymore and
2:07:28
well it's the probability distribution in velocity and it's giving you the expected velocity
2:07:34
in some sense in a quantum mechanical sense so this is really a more general sort of
2:07:41
thing we have the velocity operator
2:07:46
the expectation of the velocity operator oh and operator wise i will try to put hats on things
2:07:52
i will probably forget i don't have that much attention to detail when i'm making lectures like this
2:07:57
the hat notation means operator if you see something that you really serve as an operator but it doesn't have a hat that's probably just because i made a
2:08:03
mistake but this expression for the expectation of the velocity operator is the one we just derived minus
2:08:10
i h bar over m times the integral of psi star
2:08:15
partial derivative of psi with respect to x integral dx now it's customary to talk about
2:08:22
momentum instead of velocity momentum has more meaning because it's a conserved quantity under
2:08:27
you know most physics so we can talk about the momentum operator the expectation of the momentum operator
2:08:33
and i'm going to write this momentum operator expression in a slightly more suggestive way the integral of psi star times something
2:08:40
in parentheses here which is minus i h bar partial derivative with respect to x i'm
2:08:45
going to close the parentheses there put a psi after it and a dx for the integral
2:08:52
it had the same sort of expression for the position operator we were just writing that as the expected value of
2:08:58
position without the hat earlier but that's going to be the integral of psi star what goes in the parenthesis
2:09:05
now is just x psi dx
2:09:10
so this you recognize is the expectation of the variable x uh subject to the probability
2:09:16
distribution given by psi star times psi this is slightly more subtle you have
2:09:22
psi star and psi which looks like a probability distribution but what you have in the parentheses now is very obviously an operator that does
2:09:28
something it does more than just multiply by x it multiplies by minus i h bar and takes the derivative of psi
2:09:38
operators in general do that
2:09:43
we can write them as say x hat equals x times
2:09:49
where there's very obviously something that has to go after the x in order for it to be considered an operator
2:09:55
or we can say the same for v hat it's minus i h bar over m times the
2:10:00
partial derivative with respect to x where there obviously has to be something that goes here
2:10:06
likewise for momentum um minus i h bar partial derivative with respect to x something has to go there
2:10:14
another example of an operator is the kinetic energy operator usually that's written as t
2:10:20
and that's minus h bar squared over 2m you can think of it as the momentum
2:10:25
operator squared it's got a second derivative with respect to x
2:10:32
and again there very obviously has to be something that goes there the operator acts on the wave function that's what i
2:10:37
said back when i talked about the fundamental concepts of quantum mechanics and this is what it means for the operator to act on the wave function
2:10:42
the operator itself is not meaningful it's only meaningful in the context when it's acting on away function
2:10:50
in general in general
2:10:57
the expectation value of some has an introduction to the uncertainty principle we're going to talk about
Introduction to the uncertainty principle
2:11:04
waves and how waves are related to each other we'll get into a little bit of the context of fourier analysis which is
2:11:11
something we'll come back to later but the overall context of this lecture is the uncertainty principle and the
2:11:17
uncertainty principle is one of the key results from quantum mechanics and it's related to what we discussed earlier in
2:11:23
the context of the boundary between classical physics and quantum physics quantum mechanics has these inherent uncertainties that
2:11:30
are built into the equations built into this state built into the nature of reality
2:11:37
that we really can't surmount and the uncertainty principle is one way in which those or is the mathematical
2:11:43
description uh it's those relationships that i gave you earlier delta p delta x is greater
2:11:49
than about equal to h bar over 2. i think i just said greater than about equal to h bar
2:11:54
earlier we'll do things a little more mathematically here and it turns out there's a factor of 2 there
2:12:00
to start off though conceptually think about position and wavelength
2:12:07
and this really is now in the context of a wave so say i had
2:12:14
a coordinate system here something like this and if i had some wave
2:12:20
with a very specific wavelength you can just think about it as a sinusoid
2:12:26
if i asked you to measure the wavelength of this wave you could take a ruler and you could plop it down there
2:12:34
and say okay well how many inches are there from peak to peak or from zero crossing to zero crossing
2:12:40
or if you really wanted to you could get a tape measure and measure
2:12:45
many wavelengths one two three four wavelengths in this case that would allow you to very accurately
2:12:51
determine what the wavelength was if on the other hand
2:12:57
the wave looked more like this give you another coordinate system here
2:13:03
the wave looks something like this
2:13:09
you wouldn't be able to measure the wavelength very accurately you could as usual put your ruler down
2:13:17
on top of the wave for instance and count up the number of inches or centimeters from one side to the other but that's just one wavelength it's not
2:13:23
nearly as accurate as say measuring four wavelengths or ten wavelengths or a hundred
2:13:28
wavelengths you can think of some limiting cases suppose you had a wave with many many many many many
2:13:35
oscillations it looks like i'm crossing out the wave underneath there so i'm going to erase this in a moment but if you had a wave
2:13:41
with many wavelengths and you could measure the total length of many wavelengths you would have a very
2:13:46
precise measurement of the wavelength of the wave the opposite is the case here you only
2:13:52
have one wavelength you can't really measure the wavelength very accurately what you can do however is measure the
2:13:58
position very accurately here i can say pretty certainly the wave is there you know plus or minus a very short spread
2:14:05
in position the other hand here i cannot measure the position of this wave accurately at all
2:14:11
you know if this thing continues i can't really say where the wave is it's not really a sensical question to ask where
2:14:17
is this wave this wave is everywhere these are the sorts of built-in
2:14:22
uncertainties that you get out of quantum mechanics where is the wave the wave is everywhere it's a wave it doesn't have a local position
2:14:30
it turns out if you get into the mathematics of fourier analysis that there is a relationship between the
2:14:36
spread of wavelengths and the spread of positions if you have a series of waves of all different
2:14:42
wavelengths and they're added up the spread in the wavelength
2:14:48
will is related to the spread in positions of the sun
2:14:53
and we'll talk more about fourier analysis later but for now just realize that this product
2:14:58
is always going to be greater than or equal to about one wavelength is something with units of inverse length
2:15:04
and link when the position of course is something with units of length so the dimensions of this equation are
2:15:10
sort of a guideline wavelength and position have this sort
2:15:15
of relationship and this comes from fourier analysis
2:15:22
so how do these waves come into quantum mechanics well waves in quantum mechanics really first
2:15:28
got their start with louis de bruy i always thought his name was pronounced de broglie but it's uh
2:15:34
well he's french so there's all sorts of weird pronunciations in french is my best guess at how it would
2:15:39
probably be pronounced de voy proposed that matter could travel
2:15:45
in waves as well and he did this with a interesting argument
2:15:50
on the basis of three fundamental equations that had just recently been discovered when he was doing his analysis
2:15:56
this was in his phd thesis by the way e equals
2:16:01
m c squared you all know that equation you all hopefully also know this
2:16:07
equation e equals h f planck's constant times the frequency of
2:16:13
a beam of light is the energy associated with a quanta of light this was another one of einstein's
2:16:18
contributions and it has to do with his explanation of the photoelectric effect
2:16:24
the final equation that de bruy was working with was c c
2:16:30
equals f lambda the speed of light is equal to the frequency of the light times the wavelength of the light and
2:16:36
this is really not true just for light this is true for any wave phenomenon the speed the frequency and the
2:16:41
wavelength are related now if these expressions are both equal
2:16:47
to waves or are both equal to energy then i ought to be able to say m c
2:16:52
squared equals h f and this expression tells me something
2:16:57
about f it tells me that f equals c over lambda so i can
2:17:04
substitute this expression in here and get m c squared equals h c
2:17:11
over lambda now i can cancel out one of the c's
2:17:16
and i'm left with m c equals h over lambda now what the voice said was
2:17:23
this this is like momentum
2:17:28
so i'm going to write this equation as p equals h over lambda
2:17:34
and then i'm going to wave my hands extraordinarily vigorously and say while this equation is only true
2:17:41
for light and this equation is only true for waves this is also true
2:17:48
for matter how actually this
2:17:54
happened in the context of quantum mechanics in the early historical development of
2:18:00
quantum mechanics is de broglie noticed that
2:18:05
the spectrum of the hydrogen atom this bright line spectra that we were talking about where a hydrogen atom emits
2:18:11
light of only very specific wavelengths
2:18:16
intensity as a function of wavelength looks something like this but that could be explained if he
2:18:22
assumed that the electrons were traveling around the nucleus of the hydrogen atom as waves and that only an
2:18:28
integer number of waves would fit the one that i just drew here didn't end up back where it started so
2:18:34
that wouldn't work if you had a wavelength that looked something like this going
2:18:40
around say three full times in a circle that that would potentially count for these
2:18:46
allowed emission energies that was quite a deep insight and it was one of the things
2:18:52
that really kicked off quantum mechanics at the beginning the bottom line here for our purpose is
2:18:59
that we're talking about waves and we're talking about matter waves
2:19:04
so that uncertainty relation or the relationship between the spreads of wavelengths and the spreads in positions
2:19:10
that i mentioned in the context of fourier analysis will also potentially hold
2:19:16
for matter and that gets us into the position momentum uncertainty relation
2:19:25
the wave momentum relationship we just derived on the last slide was p equals h
2:19:31
over lambda this tells you that the momentum and the wavelength are related
2:19:37
from two slides ago we were talking about waves and whether or not you could say exactly where a wave was we had a relationship
2:19:44
that was something like delta lambda the spread in wavelengths times the spread and positions of the wave is always
2:19:50
greater than about equal to one combining these relationships together in quantum mechanics and this is not
2:19:56
something that i'm doing rigorously now i'm just waving my hands gives you
2:20:02
delta p delta x is always greater than about equal to h bar
2:20:08
over two and this is the correct mathematical expression of the heisenberg uncertainty
2:20:14
principle that we'll talk more about and derive more formally in chapter three
2:20:21
but for now just realize that the position of a wave the position of a particle
2:20:27
are on certain quantities and the uncertainties are related by this which in one perspective results from
2:20:34
consideration of adding many waves together in the context of fourier analysis which is something we'll talk
2:20:39
about later as well extended through the use of or the
2:20:45
interpretation of matter as also a wave phenomenon
2:20:51
to check your understanding here are four possible wave packets and i would like to rank i would like you to
2:20:57
rank them in two different ways one according to the uncertainties in their positions and two according to the uncertainties in their momentum
2:21:04
so if you consider say wave b to have a very certain position you would rank that one highest in terms of the
2:21:10
certainty of its position perhaps you think wave b has a very low uncertainty in position you would put it
2:21:16
on the other end of the scale i'm looking for something like the uncertainty if b is greater than the uncertainty of a is greater than the
2:21:21
uncertainty of d is greater than the uncertainty of c for both position and momentum
2:21:28
the last comment i want to make in this lecture is on energy time uncertainty this was the other equation i gave you
2:21:34
when i was talking about the boundary between classical physics and quantum physics we had delta p delta x is greater than
2:21:42
or equal to h bar over 2 and now we also had
2:21:47
excuse me for a moment here delta e delta t greater than about equal to h bar over
2:21:53
two same sort of uncertainty relation except now we're talking about spreads in energy and spreads in time
2:22:00
i'd like to make an analogy between these two equations delta p and delta x
2:22:06
delta p according to deploy is related to the wavelength
2:22:14
which is sort of a spatial frequency it's a the frequency
2:22:21
of the wave in space
2:22:27
delta x of course is just well i'll just say that's a space
2:22:32
and these are related according to this equation
2:22:37
in the context of energy and time we have the same sort of thing delta t well that's pretty clear that's time
2:22:44
and delta e well that then therefore by analogy here has
2:22:50
to have something to do with the frequency of the wave
2:22:56
now in time and that's simple that's just the frequency
2:23:06
the fact that these are also related by an uncertainty principle
2:23:12
tells you that there's something about energy and frequency and time
2:23:20
and this is something that we'll talk about in more detail in the next lecture when we start digging into the schrodinger equation
2:23:26
now the time dependent sure on your equation and deriving the time independent schrodinger equation which will give us the relationship exactly
2:23:34
but for now position and momentum energy and time we're all talking are both talking about
2:23:40
sort of wave phenomenon except in the context of position and momentum you're talking about wavelength frequency of the wave
2:23:48
in space whereas energy and time you're talking about the frequency of the wave in time how quickly it oscillates
2:23:55
that's about all the uncertainty principle as i've said is something that we'll treat in much more detail
2:24:00
in chapter three but for now the uncertainty principle is important because you have these equations and these are fundamental
2:24:07
properties of the universe if you want to think of them that way and
2:24:12
there's something that we're going to be working with as a way of checking the validity of quantum mechanics
2:24:19
throughout the rest of the next throughout chapter two um that's all for now you just need to
2:24:25
conceptually understand how these wave lengths and positions or frequencies and
2:24:30
times are interrelated the last few lectures have been all
Key concepts of QM - revisited
2:24:36
about the wave function psi since psi is such an important concept
2:24:41
in quantum mechanics really the first entire chapter of the textbook is devoted to the wavefunction
2:24:48
and all of its various properties since we've reached the end of chapter one now now is a good opportunity to go
2:24:54
and review the key concepts of quantum mechanics in particular the wave function and how it is related to the
2:25:00
rest of quantum mechanics the key concepts as i stated them earlier were operators the schrodinger
2:25:07
equation and the wave function operators are used in the schrodinger
2:25:14
equation
2:25:19
and act on the wave function
2:25:26
your friend and mine psi
2:25:33
what we haven't really talked about a lot yet is how to determine the wave function and the wave function is
2:25:38
determined as solutions to the schrodinger equation
2:25:46
that's what chapter 2 is all about solving the schrodinger equation for various circumstances
2:25:54
the key concepts that we've talked about so far operators and the wave function
2:25:59
conspire together to give you observable quantities
2:26:06
things like position or momentum or say the kinetic energy
2:26:12
of a particle but they don't give us these properties with certainty in particular the wave
2:26:17
function really only gives us probabilities
2:26:24
and these probabilities don't give us really any certainty about what will happen uncertainty is one of the key concepts that we have
2:26:31
to work with in quantum mechanics
2:26:36
so let's take each of these concepts in turn and talk about them in a little more detail since now we have some
2:26:41
actual results that we can use some mathematics we can put more meat on
2:26:47
this concept map than just simply the concept map
2:26:52
first the wave function the wave function psi
2:26:57
does not tell us anything with a with certainty and it's a good thing too because psi as
2:27:03
a function of position and time is complex it's not a real number
2:27:09
and it's hard to imagine what it would mean to actually observe a real number so the wave function is already on
2:27:15
somewhat suspect ground here but it has a meaningful connection to
2:27:20
probability distributions if we more or less define
2:27:25
the squared modulus the absolute magnitude of the wave function
2:27:31
to be equal to a probability distribution this is the probability distribution for
2:27:37
what well it's the probability distribution for outcomes of measurements of position for instance
2:27:43
you can think about this as a probability distribution for where you're likely to find the particle should you go looking for it
2:27:50
this interpretation as a probability distribution requires the wave function to be normalized
2:27:55
namely that if i integrate the squared magnitude of the wave
2:28:01
function over the entire space that i'm interested in i have to get one this means that if i look hard enough
2:28:08
for the particle everywhere i have to find it somewhere
2:28:15
the probability distributions as i mentioned earlier don't tell you anything with certainty in particular
2:28:21
there is a good deal of uncertainty which we express as a standard deviation or variance for instance if i'm
2:28:27
interested in the standard deviation of the uncertainty or standard deviation of the position excuse me
2:28:33
it's most easy to express as the variance which is the square of the standard deviation
2:28:40
and the square of this standard deviation or the variance is equal to the expectation value of the square
2:28:46
of the position minus the square of the expectation value of the position and we'll talk about expectation values
2:28:53
in a moment expectation values are calculated using expressions with operators that look
2:28:58
a lot like these sorts of integrals in fact i can re-express this as
2:29:04
the expectation of the square in terms of a probability distribution is just the x squared times multiply multiplied
2:29:11
by the probability distribution with respect to x integrated overall space this is the
2:29:16
expectation of x squared i can add to that or subtract from that
2:29:23
sorry the square of the expectation of x which has a very similar form
2:29:30
and that gives us our variance so our wave function which is complex gives us probability distributions
2:29:36
which can be used to calculate expectation values and uncertainties
2:29:42
this probabilistic interpretation of quantum mechanics gets us into some trouble pretty quickly i'm going to move
2:29:48
this up now give myself some more space
2:29:54
namely with the concept of wave function collapse
2:30:01
now collapse bothers a lot of people and it should this is really a
2:30:07
philosophical problem with quantum mechanics that we don't really have a good interpretation of what quantum mechanics really means for the nature of
2:30:13
reality but the collapse of the wave function is more or less a necessary consequence of the interpretation of the
2:30:20
wave function as a probability distribution if i have some
2:30:25
states some space some coordinate system and i plot on this coordinate system
2:30:32
the squared magnitude of psi this is related to our probability distribution with respect to position
2:30:40
if i then measure the position of the particle
2:30:46
what i'm going to get is say i measure the particle to be here
2:30:52
now if i measure the position of the particle again immediately i should get a number that's not too
2:30:58
different than the number that i just got this is just sort of to make sure that if i repeat a measurement it's
2:31:03
consistent with itself that i don't have particles jumping around truly randomly if i know the position i know the
2:31:09
position that's a reasonable assumption what that means is that the new probability distribution for the
2:31:15
position of the particle after the measurement is very sharply peaked about the position of the measurement
2:31:24
if this transition from a wave function for instance that has support here to a
2:31:29
wavefunction that has no support here did not happen instantaneously
2:31:36
it's imaginable that if i tried to measure the particle's position twice in very rapid succession that i would have one
2:31:43
particle measured here and another particle measured here does that really mean i have one
2:31:48
particle or do i have two particles these particles could be separated by quite a large distance in space and my
2:31:54
measurements could be not separated by very much in time so i might be getting into problems with special relativity in the speed of light
2:32:01
and these sorts of considerations are what leads to the copenhagen interpretation of quantum mechanics
2:32:06
which centers on this idea of wave functions as probability distributions and wave function collapse as part of
2:32:13
the measurement process now i mentioned operators in the context
2:32:19
of expectation values operators are our second major concept in quantum mechanics
2:32:25
what about operators in the wave function well operators let's just write a general
2:32:30
operator as q hat hats usually signify operators operators always act on
2:32:36
something you can never really have an operator in isolation and what the operators act on is usually the wave function
2:32:44
we have a couple of operators that we've encountered namely the position operator x hat which is
2:32:49
defined as x times and what's it multiplied by well it's multiplied by the wave function
2:32:56
we also have the momentum operator p hat and that's equal to minus i h bar times
2:33:02
the partial derivative with respect to x of what well of the wave function
2:33:09
we also have the kinetic energy which i'll write as k e hat you could also write it as t hat that
2:33:15
operator is equal to minus h bar squared over 2m times the second derivative with
2:33:21
respect to position of what well of the wave function
2:33:27
and finally we have h hat the hamiltonian which is an expression of the total
2:33:33
energy in the wave function it's a combination of the kinetic energy operator here
2:33:39
which you can see first of all as p squared we have a second derivative with respect to position and minus h bar
2:33:46
squared this is just p squared divided by 2m p squared over 2m is a classical kinetic energy the analogy is reasonably
2:33:53
clear there you add a potential energy term in here and you get the hamiltonian
2:33:59
now expectation values of operators like this are calculated as
2:34:05
integrals the expectation value of q for instance is the integral
2:34:11
of psi star times q acting on psi overall space
2:34:19
this bears a striking resemblance to our expression for instance for the
2:34:24
expectation of the position which was the integral of just x times
2:34:29
rho of x where rho of x is now given by the absolute magnitude of psi squared
2:34:36
which is given by psi star times psi now
2:34:42
basically the pattern here is you take your operator and you sandwich it between psi star and psi
2:34:47
and you can think about this position as being sandwiched between psi star and psi as well because we're just multiplying by it doesn't really matter
2:34:54
where i put it in the expression the sandwich between psi star and psi of the operator is more significant when
2:35:00
you have operators with derivatives in them but i'm getting a little long-winded
2:35:06
about this perhaps suffice it to say
2:35:11
that operators in the wave function allow us to calculate meaningful physical quantities like
2:35:16
x the expectation of position this is more or less where we would expect to find the particle or
2:35:22
the expectation of p and i should be putting hats on these since technically they're operators the expectation of p is more or less the expected value of
2:35:30
the momentum the sort of sorts of momentum momenta that the system can have
2:35:35
or the expectation value of h the typical energy the system has
2:35:41
and all of these are tied together in the context of uncertainty for instance if i wanted to calculate the uncertainty
2:35:46
in the momentum i can do that with the same sort of machinery we used when we were talking
2:35:51
about probability that i calculate the expectation of p squared and i subtract the expectation
2:35:57
of p squared so the expectation of the square minus the square of the expectations is
2:36:03
directly related to the uncertainty so that's a little bit about operators and a little bit about the wave function
2:36:09
and a little bit about how they're used operators acting on the wave function calculating expectations in the context
2:36:15
of the wave function being treated as a probability distribution now
2:36:21
where are we all going with this we're going towards the schrodinger equation in the schrodinger equation to write it
2:36:26
out is i h bar partial derivative with respect to time of the wave function and that's equal to
2:36:32
minus h bar squared over 2m second partial derivative
2:36:38
with respect to position of the wave function plus some potential function
2:36:43
function of x times the wave function now the wave function psi here i've left
2:36:48
it off as a function of position and time so this is really the granddaddy of them
2:36:53
all this is the equation that we will be working with throughout chapter two we will be writing this equation for
2:36:59
various scenarios and solving it and describing the properties of the solutions
2:37:05
so hopefully now you have a reasonable understanding of the wave function and the schrode and enough understanding of
2:37:11
operators to understand what to do with the wave function the sorts of questions you can ask of the wave function are
2:37:17
things like what sorts of energy does this system have how big is the spread in momenta where am i likely to find the
2:37:23
particle if i went looking for it but all of that relies on having the wave function and you get the wave
2:37:29
function by solving the schrodinger equation so that's where we're going with this
2:37:36
and that's all of the material for chapter one and without further ado moving on to the next lecture we'll
2:37:41
start solving the schrodinger equation we're going to move now in to actually
Separation of variables and Schrodinger equation
2:37:49
solving the schrodinger equation this is really the main meat of quantum mechanics
2:37:54
and in order to start tackling the schrodinger equation we need to know a little bit about how equations like the
2:38:00
schrodinger equation are solved in general one of those solution techniques is separation of variables and that's the
2:38:05
solution technique that we're going to be applying repeatedly to the schrodinger equation
2:38:11
first of all though let's talk a little bit about ordinary and partial differential equations the schrodinger equation is a partial differential
2:38:17
equation which means it's a good deal more difficult than an ordinary differential equation but what does that actually
2:38:23
mean first of all let's talk about ordinary differential
2:38:28
equations what an ordinary differential equation
2:38:35
tells you is how specific coordinates change with time at least that's most applications so you have something like
2:38:43
x as a function of time y as a function of time sorry not y is a function of x y
2:38:48
is a function of time z as a function of time for example the position of a projectile moving through
2:38:55
the air could be determined by three functions x y and z
2:39:03
if you're only working in two dimensions for instance let me drop the z but we might have a velocity as well say
2:39:09
v x of t and v y of t these four coordinates position in two
2:39:16
dimensions and velocity in two dimensions fully specifies the state of a projectile moving in two dimensions
2:39:22
what an ordinary differential equation might look like to govern the motion of this projectile would be something like
2:39:27
the following dx dt is vx
2:39:33
dy dt is vy nothing terribly shocking there the
2:39:38
position coordinates change at a rate of change given by the velocity well the velocity change velocities
2:39:45
change dv x dt is given by let's say minus k
2:39:51
v x and d v y d t is minus k v y sorry k v subscript y now
2:40:01
k v y minus g this tells you that
2:40:06
um well where i got these equations this is a effectively damped
2:40:11
frictional motion in the plane uh xy where gravity is pulling you down
2:40:16
so in the absence of any velocity gravity leads to an acceleration in the negative y direction
2:40:22
and the rest of this system evolves accordingly
2:40:27
what that tells you though in the end is the trajectory of the particle if you launch it
2:40:33
as a function of time tick tick tick tick tick tick tick tick tick as the projectile moves through the air
2:40:39
in say x y space partial differential equations on the
2:40:45
other hand pdes
2:40:51
you have several independent variables
2:40:59
so where an ordinary differential equation we only had time and everything was a function of time
2:41:06
in a partial differential equation what you're trying to solve for will have several independent variables
2:41:13
for example the electric field the vector electric field in particular
2:41:19
as a function of x y and z the electric field
2:41:24
has a value both a magnitude and a direction at every point in space so x y
2:41:29
and z potentially vary over the entire universe
2:41:35
now you know how excuse me you know a few equations that pertain to
2:41:41
the electric field that maybe you could use to solve to determine what the electric field is one of these is gauss's law which we usually give an
2:41:47
integral form the electric field the integral of the electric field dotted with an area vector over a closed surface
2:41:54
is equal to the charge enclosed by that surface over epsilon not
2:42:00
now hopefully you also know there is a differential form for gauss's law and it usually is written like this
2:42:12
this upside down delta is read as del so you can say this is del dot e and this is a vector differential operator
2:42:20
i'm going to skip the details of this because this is all electromagnetism and if you go on to take advanced electromagnetism courses you will learn
2:42:26
about this in excruciating detail perhaps suffice to say here that most of
2:42:31
the time when we're trying to solve equations like this we don't work with the electric field we work with the potential let's call that v
2:42:38
and this system of equations here if you treat the electric field as minus the gradient
2:42:45
of the potential gives you this equation or this equation gives you
2:42:51
the laplace equation del squared v equals rho
2:42:56
over epsilon naught what that actually writes out to if you go through all the vector algebra is the
2:43:04
second derivative of v with respect to x plus the second derivative of v with respect to y plus the second derivative
2:43:11
of v with respect to z and i've left off all my squares in the denominator here is equal to rho
2:43:17
over epsilon naught this is a partial differential equation and if we had some machinery for solving
2:43:22
partial differential equations we would be able to determine the potential at every point in space and that would then allow us to
2:43:28
determine the electric field at every point in space this is just an example hopefully you're familiar with some of the terms i'm
2:43:34
using here the main solution technique that is used for partial differential equations is
2:43:40
separation of variables and separation of variables is fundamentally a guess
2:43:47
suppose we want to find some function in the case of electromagnetism it's the
2:43:54
potential x y and z the potential is a function of x y and z
2:43:59
let's make a guess that v of x y and z can be written as
2:44:05
x of x times y of y times z of z
2:44:11
so instead of having one function of three variables we have the product of three functions of one variable each
2:44:19
does this guess work well it's astonishing how often this guess actually does work
2:44:24
this is a very restrictive sort of thing but under many realistic circumstances this actually
2:44:30
tells you a lot about the solution for example
2:44:36
the wave equation the wave equation is what you get mathematically if you
2:44:41
think about say having a string stretched between two solid objects
2:44:48
now under those circumstances if you zoom in on if you say pluck the string
2:44:54
you know it's going to vibrate up and down mathematically speaking if you zoom in
2:45:01
on a portion of that string say it looks like this you know the center of this string is
2:45:07
going to be accelerating downwards and the reason it's going to accelerate downwards is because there is tension in the string and the tension force pulls
2:45:13
that direction on that side and that direction on that side so it's being pulled to the right and pulled to the
2:45:19
left and the net force then ends up being in the downward direction if the string
2:45:25
curved the other direction you would have effectively a net force pulling
2:45:30
up into the right and a net force pulling up into a force pulling up into the right a force pulling up into the left and your net force would be up
2:45:38
this tells you about forces in terms of curvatures and that thought leads directly to the wave equation
2:45:47
the acceleration as a result of the force is related to the curvature
2:45:54
of the string and how we express that mathematically is with
2:45:59
derivatives the acceleration is the second derivative of the position
2:46:05
so if we have the position of this string is u as a function of position and time
2:46:14
then the acceleration of the string at a given point and at a given time is going to be equal to
2:46:20
some constant traditionally written c squared times the curvature which is the second derivative of u
2:46:25
with respect to x again u being a function of position and time
2:46:31
so this is the weight equation i should probably put a box around this because the wave equation shows up a lot
2:46:36
in physics this is an important one to know but let's proceed with
2:46:41
separation of variables u as a function of
2:46:47
position and time is going to be x a function of not time x a function of
2:46:54
position and t a function of time so capital x and capital t are functions
2:47:01
of a single variable each and their product is what we're guessing reproduce reproduce the behavior of u
2:47:09
so if i substitute this u into this equation
2:47:14
what i end up with is the second derivative of x of x t of t
2:47:22
with respect to time equals c squared times the second derivative of x of x
2:47:28
t of t with respect to position
2:47:35
so this hasn't really gotten this anywhere yet but what you notice here is we have
2:47:41
derivatives with respect to time and then we have this function of position since these are partial derivatives
2:47:48
they're derivatives taken with everything else other than the variable that you're concerned with held constant which means this
2:47:55
part here which is only a function of position can be treated as a constant and taken outside of the derivative
2:48:02
the same sort of thing happens here we have second derivatives partial second derivatives with respect to
2:48:07
position and here we have only a function of time effectively a constant for this partial
2:48:13
derivative which means we can pull things out and what we've got then is capital x i'm
2:48:18
going to drop the parentheses x because you know capital x is a function of lowercase x
2:48:25
so you've got big x second partial derivative with respect to time of big t
2:48:31
equals c squared big t second partial derivative of big x
2:48:38
with respect to x
2:48:43
that's nice because you can see we're starting to actually be able to pull x and t
2:48:49
out here the next step is to divide both sides of this equation by
2:48:54
x t by basically dividing through by u in order for this to work we need to know that our solution is non-trivial
2:49:00
meaning if x and t are everywhere zero dividing through by this will do bad things to this equation
2:49:07
but what you're left with after you divide by this is one over t
2:49:12
second partial of t big t with respect to little t and c squared one over big x
2:49:20
second partial of big x with respect to little x this is fully separated
2:49:27
what that means is that the left hand side here is a function
2:49:33
only of t the right hand side
2:49:39
is a function only of x that's very interesting
2:49:46
suppose i write this function of t as say f of t
2:49:52
this then this part let's call that g of x i have two different functions of t and
2:49:59
x normally you would say oh i have f of t and i have g of x and i know what those
2:50:05
forms are i could in principle solve for t as a function of x
2:50:11
but that isn't what you're going to do and the reason that's not the case is that this is a partial differential equation both x and t are independent
2:50:19
variables all of this analysis in order for separation of variables to work must hold at every point in space at every x
2:50:26
and at every time so suppose this relationship held for a
2:50:31
certain value of t for a certain value of x i ought to be able to change x and have
2:50:37
the have the relationship still hold so if i change x without changing t the
2:50:42
left-hand side of the equation isn't changing if changing x led to a change in g of x
2:50:50
then my relationship wouldn't hold anymore so effectively what this means is that g of x is a constant in order
2:50:56
for this relationship to hold both f of t and g of x have to be constant
2:51:04
essentially what this is saying in the context of the partial differential equation is that if we look at the x
2:51:09
part here when i change the position any change in the second derivative of
2:51:16
the position function is mimicked by this one over x such that the overall function ends up being a constant
2:51:25
that's nice because that means i actually have two separate equations f of t is a constant
2:51:31
and g of x is a constant what these equations actually look like
2:51:36
this was my f of x this is my g or f of t and this is my g of x
2:51:44
that constant which i've called a here and the notation is arbitrary though you can in principle save yourself some time
2:51:51
by thinking ahead and figuring out what might be a reasonable value for a
2:51:57
what's especially nice about these is that this equation is now only an ordinary differential
2:52:03
equation since t is big t is only a function of little t we just have a function of a single variable we only have a single
2:52:10
variable here we don't need to worry about what variables are being held constant what variables aren't being held constant so we can write this as
2:52:16
total derivative with d instead of uh partial derivative with
2:52:21
the partial derivative symbol so we've reduced our partial differential equation into
2:52:29
two ordinary differential equations this is wonderful and we can write we can rearrange these
2:52:35
things to make them a little more recognizable you've got d squared t dt squared equals a
2:52:42
t and c squared d squared big x d little x squared
2:52:49
equals a times big x multiplying through by big t in this equation and big x in
2:52:55
this equation and these are equations that you should know how to solve
2:53:02
if not you can go back to your ordinary differential equations books
2:53:07
and solution to ordinary differential equations like this are very commonly studied
2:53:15
in this case we're taking the second derivative of something and we're getting the something back with a constant out front anytime you
2:53:22
take the derivative of something and get itself or itself times a constant you should think exponentials and in this case the
2:53:28
solution is t equals e to the square root of a
2:53:34
times time if you take the second derivative of this you'll get two square roots of a
2:53:39
factors that come down a time times e to the root a t which is just big t
2:53:46
you can in principle also have a normalization constant out front and you end up with the same sort of
2:53:52
thing for x big x is going to be
2:53:58
e to the square root of a over c
2:54:04
x with again in principle a normalization constant out front
2:54:10
what that means is if i move things up a little bit and get myself some space
2:54:20
u of x and t what we originally wanted to find is now going to be the product of these two functions so i have a
2:54:27
normalization constant in front and i have e times root a t and e times root a over c x
2:54:36
now if this doesn't look like a wave and that surprises you because i told
2:54:41
you this was the wave equation it's because we have in principle some
2:54:46
freedom for what we want to choose for our normalization constant and for what we want to choose for our separation
2:54:51
constant this constant a and the value of that constant will in
2:54:58
principle be determined by the boundary conditions a and a
2:55:06
are determined by boundary conditions
2:55:15
the consideration of boundary conditions and initial conditions in partial differential equations is subtle and i
2:55:20
don't have a lot of time to fully explain it here but if what you're concerned with is why
2:55:27
this doesn't look like a wave equation what actually happens when you plug in to your initial conditions and your
2:55:33
boundary conditions to find your normalization constants and your actual value for the separation constant you'll find that a is complex
2:55:40
and when you do when you substitute in the complex value for a into these expressions you'll end
2:55:46
up with e to the i omega t sort of behavior which is going to give you effectively
2:55:51
cosine of omega t up to some phase shifts as determined by
2:55:56
your normalization constant and your initial conditions so this is how we actually solve a
2:56:02
partial differential equation the wave equation in particular separates easily into these two ordinary
2:56:10
differential equations which have solutions that you can go and look up pretty much anywhere you want
2:56:17
finding the actual value of the constants that match this general solution to the specific circumstances
2:56:23
you're concerned with can be a little tricky but in the case of the wave equation if what you want is say a
2:56:28
traveling wave solution you can find it there are appropriate constants that produce traveling waves in this
2:56:34
expression so to check your understanding what i'd like you to do is go through that
2:56:39
exercise again performing separation of variables to convert this this equation into again two ordinary differential
2:56:45
equations this equation is called the heat equation and it's related to the diffusion of heat throughout a material
2:56:52
if you have say a hot spot and you want to know how that hot spot will spread out with time
2:57:00
since this is a quantum mechanics course let's move on to the time dependent schrodinger equation
2:57:06
this is the full schroedinger equation in all of its glory except i've just written it in terms of
2:57:12
the hamiltonian operator now h hat
2:57:17
is the hamiltonian
2:57:24
the hamiltonian is related to the total energy i evidently can't spell
2:57:31
total energy of the system
2:57:36
meaning it's you know kinetic energy plus potential
2:57:43
and we have a kinetic energy operator and we have well we will soon have a potential energy operator
2:57:50
what h hat actually looks like is it's the kinetic energy operator which if you recall correctly is minus h
2:57:57
bar squared over 2m times the second derivative with respect to position
2:58:04
and the potential energy operator is just going it looks a lot like the position operator it's just multiplying
2:58:09
by some potential function which here i'll consider to be a function of x
2:58:15
now this is an operator which means it acts on something so i need to substitute in
2:58:21
a wave function here and when you do that in the context of the schrodinger equation you end up with
2:58:27
the form that we've seen before i h bar d psi dt
2:58:34
equals minus h bar squared over 2m
2:58:39
d squared psi dx squared plus v of x psi
2:58:48
so that's our short energy equation how can we apply separation of variables to this
2:58:53
well we make the same sort of guess as we made before namely
2:59:00
psi is going to be x t where x is a big x is a function of
2:59:05
position and big t is a function of time
2:59:11
if i substitute psi equals x t into this equation you get pretty much what you would expect i h bar
2:59:18
now when i substitute x t in here big x
2:59:24
big t big x is a function only of position so i don't need to worry about the time
2:59:31
derivative acting on big x so i can pull big x out and what i'm left with then is a
2:59:37
time derivative of big t this is then going to be equal to
2:59:43
minus h bar squared over 2m times the same thing when i substitute x
2:59:48
t in here the second derivative with respect to position is not going to act on the time
2:59:54
part so i can pull the time part out t
3:00:00
second derivative of big x with respect to position
3:00:06
and substituting in x t here doesn't really do anything there's no derivatives here so this is not a real it's not a particularly interesting term
3:00:14
so we've got we're getting v x t on the right
3:00:19
now the next step in separation of variables is to divide through by your solution x t
3:00:25
assuming it's not zero that's okay and you end up with i h bar one over big
3:00:31
x sorry one over big t canceling out the x and you're just left
3:00:37
with big t one over t partial of t dt
3:00:43
and then on the right hand side we have minus h bar over two m sorry h bar squared over two m one over big x
3:00:49
second partial of x with respect to position plus v
3:00:55
x and t are fully cancelled out in this term now as before this is a function of time
3:01:01
only and this is a function of space only
3:01:07
which means both of these functions have to be constant and in this case the constant
3:01:15
we're going to use is e and you'll see why
3:01:20
once we get into talking about the energy in the context of the wave function
3:01:26
so we have our two equations one i h bar
3:01:32
over t first partial derivative of big t with respect to time
3:01:37
is equal to e and on the right hand side from the right hand side we get minus h bar
3:01:43
squared over 2m one over big x second partial of big x with respect to position
3:01:50
plus v is equal to the energy so these are our two equations
3:01:59
now i've written these with partial derivatives but since as i said before these functions
3:02:04
big t and big x are only functions of a single variable there's effectively no reason to use partial derivative symbols
3:02:10
i could use d's instead of partials essentially there's no difference if you only have a function of a single
3:02:16
variable whether you take the partial different partial derivative or the total derivative
3:02:22
so let's take these equations one by one the first one the time part
3:02:30
this we can simplify by multiplying through by big t as before and you end up with i h bar
3:02:39
d big t d t equals e times t
3:02:46
taking the derivative of something and getting it back multiplied by a constant
3:02:51
again should suggest two exponentials let me move this i h bar to the other
3:02:57
side so we would have divided by i h bar
3:03:03
and 1 divided by i is minus i so i'm going to erase this from here and say
3:03:08
minus i in the numerator so first derivative with respect to time of our function
3:03:15
gives us our function back with this out front immediately this suggests exponentials
3:03:20
and indeed our general solution to this equation is some normalization constant times e to the minus i
3:03:28
e over h bar times time
3:03:36
so if we know what the separation constant capital e is we know the time part of the evolution
3:03:44
of our wave function this is good what this tells us is that our time
3:03:49
evolution is actually quite simple it's in principle a complex number
3:03:56
t is in principle a complex number it has constant magnitude
3:04:05
time evolving this doesn't change the absolute value of capital t
3:04:11
and essentially it's just rotating about the origin in the complex plane so if this is my complex plane real axis
3:04:18
imaginary axis wherever capital t starts as time evolves
3:04:24
it just rotates around and around and around and around in the complex plane
3:04:31
so the time evolution that we'll be working with for the most part in quantum mechanics is quite simple
3:04:38
the space part of this equation is a little more complicated all i'm going to be able to do now is
3:04:44
rearrange it a little bit by multiplying through by capital x just to get things on top and change the
3:04:51
order of terms a little bit to make it a little more recognizable minus h bar squared
3:04:57
over 2m second derivative of capital x with respect to position
3:05:06
plus v times capital x is equal to e times capital x
3:05:15
and this is all the better we can do we can't solve this equation because we don't know what v is yet v is where the physics enters this
3:05:21
equation and where the wave function from one scenario differs from the wave function for another scenario
3:05:27
essentially the potential is where you encode the environment into the schrodinger equation
3:05:35
now if you remember back a ways when we were talking about the schrodinger equation on the very first slide of this lecture what we had was the hamiltonian
3:05:42
operator acting on the wave function and this is that same hamiltonian this is h hat
3:05:49
not acting on psi now just acting on x so you can also express the schrodinger equation as h times x equals e times x
3:05:57
the hamiltonian operator acting on your spatial part is the energy of sorry is the separation constant e
3:06:04
which is related to the energy times the spatial part so this is another expression of the schrodinger
3:06:09
equation this equation itself is called the
3:06:15
time-independent schrodinger equation or t-i-s-e if i ever use that abbreviation
3:06:21
and this is really the hard part of any quantum mechanics problem
3:06:27
to summarize what we've said so far starting with the schrodinger equation
3:06:33
which is this time derivatives with complex parts in terms of hamiltonians and wave functions
3:06:39
gives you this substituting in the actual definition of the hamiltonian including a potential v
3:06:46
and applying separation of variables gets us this pair of ordinary differential equations
3:06:53
the time part here gave us numbers that just basically spun around in the complex plane
3:07:00
not the imaginary part this is traditionally the real part
3:07:05
and this is the imaginary part so the time evolution is basically rotation in the complex plane
3:07:14
and the spatial part well we have to solve this this equation being the time independent schrodinger equation
3:07:20
we have to solve this for a given potential the last comment i want to make in this lecture is a comment about notation
3:07:29
my notation is admittedly sloppy and if you read through the chapter griffiths calls my notation sloppy
3:07:36
um in griffiths since it has the luxury of being a book and not the handicap of
3:07:43
having my messy handwriting they use capital psi to denote the function of x
3:07:48
and time and when they do separation of variables they re-express this as
3:07:56
lowercase psi as a function of position and lowercase phi as a function of time
3:08:02
so for this i used capital x sorry i should put things in the same order i use capital t of t
3:08:08
and capital x of x because i have a better time distinguishing my capital letters from
3:08:14
my lowercase letters than trying to well you saw how long it took me to write that symbol
3:08:20
i'm not very good at writing capital size there is a lot of sloppiness in the notation in quantum mechanics namely
3:08:26
because oops geez i have two functions of time
3:08:31
this is griffith's function of position sorry about that
3:08:37
this here and this here these are really the interesting parts the functions of position the solutions to the
3:08:43
time-independent schrodinger equation
3:08:49
what that gives us well what that means is that a lot of people are sloppy with what they call the wave
3:08:56
function this is the wave function this is the spatial part or the solution
3:09:03
to the time independent schrodinger equation this is not the wave function but i mean i've already made this sloppy
3:09:10
mistake a couple of times in problems that i've given to you guys in class namely i'll ignore the time domain part
3:09:16
and just focus on the spatial part since that's the only interesting part
3:09:23
so perhaps that's my mistake perhaps i need to relearn my handwriting but at any rate be aware that
3:09:31
sometimes i or perhaps even griffis or whoever you are talking to will use the term the wave function when they don't actually
3:09:37
intend to include the time dependence the time dependence is in some sense
3:09:43
easy to add on because it's just this rotation in complex number space but hopefully things will be clear from
3:09:49
the context what is actually meant by the wave function
Stationary solutions to the Schrodinger equation
3:09:56
so we're still moving toward solutions to the schrodinger equation and the topic of this lecture is what
3:10:02
you get from separation of variables and the sorts of properties it has
3:10:09
to recap what we talked about last time the schrodinger equation i h bar
3:10:14
partial derivative of psi with respect to time is equal to minus h bar squared over 2m second partial derivative of psi
3:10:21
with respect to position plus v times psi where this is the essentially the
3:10:27
kinetic energy and this is the potential energy as part of the hamiltonian operator
3:10:33
we were able to make some progress towards solving this equation by writing psi
3:10:38
which is in principle a function of position and time as some function of position multiplied
3:10:46
by some function of time why did we do this
3:10:51
well it makes things easier we can make some sort of progress but haven't we restricted our solution a lot by writing
3:10:58
it this way well really we have but
3:11:04
it does make things easier and it turns out that these solutions that are written as products that result from
3:11:10
solving the ordinary differential equations you get from separation of variables with the schrodinger equation
3:11:17
can actually be used to construct everything that you could possibly want to know
3:11:23
so let's take a look at the properties of these separated solutions
3:11:29
first of all these solutions are called stationary states
3:11:35
what we've got is psi as a function of position and time is equal to
3:11:42
some function of position multiplied by some function of time and i wrote that as capital t on the last slide but if
3:11:48
you remember from the previous lecture the time eq evolution equation was a solvable and what it gave us was a
3:11:55
simple exponential e to the there we go minus e sorry i times e times t
3:12:03
divided by h bar so this is our time evolution part and this is our spatial part
3:12:10
what does it mean for these states to be stationary well consider for instance
3:12:16
the probability density for the outcome of position measurements
3:12:21
hopefully you remember this is equal to the squared absolute magnitude of psi
3:12:26
which is equal to the complex conjugate of psi times psi
3:12:34
now if i plug this in for psi and its complex conjugate
3:12:40
i end up with the complex conjugate of big x as a function of position times the complex conjugate of this the only
3:12:47
part that's complex about this is the i here and the exponent so we need to flip the sign on that and we'll have e to the
3:12:54
i positive i now e t over h bar
3:12:59
that's for the complex conjugate of psi and for the science south well x of x e
3:13:05
to the minus i e t over h bar
3:13:10
now multiplying these things together there's nothing special about the multiplication here and this
3:13:16
and this are complex conjugates of each other so they multiply together to give the magnitude of
3:13:23
the squared magnitude of each of these numbers together which since these are just complex
3:13:28
exponentials is magnitude 1. so what we end up with here
3:13:33
is x star x essentially the squared magnitude of
3:13:39
just the spatial part of the wave function there's now no time dependence here
3:13:49
which means the probability density here does not change as time evolves
3:13:55
so that's one interpretation of the or one meaning of these things being called stationary states
3:14:01
the fact that i can write a wave function as a product like this and
3:14:06
the only time dependence here comes in a simple complex exponential means that that time dependence drops out when i
3:14:12
find the probability distribution another interpretation of these things
3:14:18
as stationary states comes from considering expectation
3:14:23
suppose i want to calculate the expectation value of some generic operator capital q
3:14:30
the expression for the expectation of an operator is an integral of the wave function
3:14:38
times the operator acting on the wave function so complex conjugate wave function operator wave function
3:14:45
now i'm going to go straight to the wave function as expressed in terms of
3:14:51
x and t parts so complex conjugate of the spatial part
3:14:56
times the complex conjugate of the time part which from the last slide is e to the plus i e t over h bar
3:15:04
our operator gets sandwiched in between the complex conjugate of the wave function and the wave function itself
3:15:10
so this is again no no stars anymore come on brett just x and then e to the minus i e t
3:15:18
over h bar this is all integrated dx so this is psi
3:15:24
star and this is psi and this is our operator sandwiched
3:15:29
between them as in the expression for the expectation
3:15:35
now provided this operator does not act on
3:15:40
time it doesn't have anything to do with the time coordinate and that will be true for basically all of the operators
3:15:45
we will encounter in this course now we talked about how the schrodinger equation can be split by separation of
Superposition of stationary states
3:15:52
variables into a time independent schrodinger equation in a relatively simple time dependent part
3:15:59
what that gave us is provided we have solutions to that time independent schrodinger equation
3:16:04
we have something called a stationary state and it's called a stationary state because nothing ever changes the
3:16:10
probability densities are constant the expectation values are constant in the state effectively since it has a precise
3:16:16
exact no uncertainty energy has to live for an infinite amount of time
3:16:21
that doesn't sound particularly useful from the perspective of physics we're often interested in how things interact and how things change with time so how
3:16:29
do we get things that actually change with time in a non-trivial way well it turns out that these stationary
3:16:35
states while their time dependence is trivial the interaction of their time dependence when added together in a
3:16:41
superposition is not trivial and that's where the interesting time dynamics of quantum mechanics comes from
3:16:46
superpositions of stationary states now we can make superpositions of stationary states because of one
3:16:52
fundamental fact and that fact is the linearity of the schrodinger equation so the schrodinger equation as you
3:16:58
hopefully remember it by now is i h bar partial derivative of psi with respect
3:17:04
to time is equal to minus h bar squared over 2m
3:17:10
second derivative of psi with respect to x and that's a really ugly sign
3:17:15
must fix second derivative of psi with respect to position
3:17:20
plus v times psi so this is our hamiltonian operator applied to the wave function and this is
3:17:27
our time dependence part now in order for an equation to be linear
3:17:32
what that means is that if psi solves the equation psi plus some other psi that also solves
3:17:39
the equation we'll solve the equation so if say let's call it a
3:17:46
solves the schrodinger equation and b
3:17:53
solves the schrodinger equation and uh let me write this out in a little more detail first of all i'm talking
3:17:59
about a as a is a function of position and time as is b
3:18:07
if a and b both solve the schrodinger equation then a plus b
3:18:13
must also solve the schrodinger equation and we can see that pretty easily let's substitute psi equals a plus b
3:18:20
into this equation the first step i h bar partial derivative respect to
3:18:27
time of a plus b is equal to minus h bar squared over 2m
3:18:34
second partial derivative with respect to space of a plus b plus the potential v
3:18:40
times a plus b now the partial derivative of the sum is
3:18:46
the sum of the partial derivatives that goes for the second partial derivative as well
3:18:51
and well this is just just the uh product of the potential with the sum is the sum of the product of the potential
3:18:58
with whatever you're multiplying out i'm going to squeeze things a little bit more here
3:19:05
so i can write that out i h bar d by dt of a
3:19:11
plus i h bar db dt equals
3:19:16
minus h bar squared over 2m second derivative of a with respect to space
3:19:23
forgot my squared on the second derivative minus h bar squared over 2m
3:19:29
second derivative of b with respect to position plus v times a plus v times b
3:19:37
that's just following those fundamental rules now you can probably see where this is going
3:19:43
this this and this
3:19:48
this these three terms together make up the schrodinger equation
3:19:54
the time dependent schroedinger equation for a
3:20:00
fo a for a
3:20:08
and this this and this altogether that's the time
3:20:14
dependent schroedinger equation for b
3:20:20
so if a satisfies the time-dependent schrodinger equation which is what we supposed when we got
3:20:25
started here then this term this term and this term will cancel out they will obey the equality
3:20:32
likewise for the parts with b in them so essentially if a solves the
3:20:37
schrodinger equation b solves the schrodinger equation a plus b also solves the schrodinger equation the reason for that is
3:20:44
the partial derivatives here partial derivative of the sum is the sum of the partials and the product with the sum is
3:20:50
the sum with the product these are linear operations
3:20:56
so we have a linear partial differential equation and the linearity of the partial
3:21:01
differential equation means well essentially that if a solves and b solves then a plus b will also solve it
3:21:08
that allows us to construct solutions that are surprisingly complicated and actually the general solution to the
3:21:14
schrodinger equation is psi
3:21:19
of position and time is equal to the sum
3:21:25
and i'm going to be vague about the sum here you're summing over some index j
3:21:31
x sub j as a function of position these are solutions now to the time independent
3:21:36
schrodinger equation the spatial part of the schrodinger equation times
3:21:42
your time part and we know the time part from the well from us back from when we discussed
3:21:48
separation of variables is minus i e now this is going to be e sub j
3:21:55
t over h bar so this is a general expression that
3:22:01
says we're we're summing up a whole bunch of stationary state solutions to the time independent schrodinger equation
3:22:07
and we're getting psi now oh i've left something out and i've left and what i've left out is quite
3:22:14
important here we need some
3:22:19
constant c sub j
3:22:25
that tells us how much of each of these stationary states to add in
3:22:33
so this is actually well it's going to be a solution to the schrodinger equation since it's constructed from solutions to destroying
3:22:39
your equation and this is completely
3:22:46
general that's a little surprising what that
3:22:51
means is that this can be used to express not just a subset of solutions to the schrodinger
3:22:58
equation but all possible solutions to the showing of your schrodinger equation
3:23:07
all the solutions to the short injury equation can be written like this
3:23:18
that's a remarkable fact and it's certainly not guaranteed you can't just write down any old partial
3:23:24
differential equation apply separation of variables and expect the solutions that you get to be
3:23:30
completely general and super posable to make any solution you could possibly want
3:23:36
the reason this works for the schrodinger equation is because the schroeder equation is well
3:23:43
just to drop some mathematical terms if you're interested in looking up information later on the schroedinger
3:23:48
equation is an instance of what's called a sturm liuval problem stormley oval problems are a class of
3:23:55
linear operator equations for instance partial differential equations or ordinary differential equations
3:24:02
that have a lot of really nice properties and this is one of them so
3:24:07
the fact that the schrodinger equation is a sternly oval equation but the fact that the time independent schrodinger
3:24:12
equation is a sternly oval equation means that this will work so if you go on to study you know
3:24:20
advanced mathematical analysis methods in physics you'll learn about this but for now you
3:24:26
just need to sort of take it on faith the general solutions to the schrodinger equation look like this superpositions of stationary states
3:24:35
so if we can superpose stationary states what does that actually give one example i would like to do here is
3:24:42
and this is just an example of the sorts of analysis you can do given superpositions of stationary states is
3:24:47
to consider the energy suppose i have two solutions to the time independent schroedinger equation which
3:24:54
i'm just going to write as h hat x1 equals e1 x1
3:25:00
and hat x2 equals e2
3:25:06
x2 so x1 and x2 are solutions to the time independent schrodinger equation and their distinct
3:25:12
solutions e1 not equal to e2
3:25:18
i'm going to use these to construct a wave function
3:25:24
let's say psi of x and at time t equals 0
3:25:30
let's say it looks like this c1 times x1 as a function of position plus
Potential function in the Schrodinger equation
3:25:39
quantum mechanics is really all about solving the schrodinger equation that's a bit of an oversimplification
3:25:44
though because if there was only one schrodinger equation we could just solve it and be done with it and that would be it for quantum mechanics
3:25:52
the reason this is difficult is that the schrodinger equation isn't just the schrodinger equation there are many schrodinger equations
3:25:58
each physical scenario for which you want to apply quantum mechanics has its own schrodinger equation they're all
3:26:04
slightly different and they all require slightly different solution techniques the reason there are many different
3:26:10
schrodinger equations is that the situation over under which you want to solve the schrodinger equation
3:26:15
enters the schrodinger equation as a potential function so let's talk about potential functions
3:26:20
and how they influence well the physics of quantum mechanics first of all where does potential appear
3:26:27
in the schrodinger equation this is the time dependent schrodinger equation and the right hand side here you know is
3:26:33
given is giving the hamiltonian operator acting on the wave function
3:26:39
now the hamiltonian is related to the total energy of the system and you can see that by looking at the parts
3:26:44
this is the kinetic energy which you can think of as the
3:26:50
momentum operator squared over 2m sort of a quantum mechanical and now
3:26:56
analog of p squared over 2m in classical mechanics and the second piece here
3:27:02
is in some sense the potential energy
3:27:07
this v of x is the potential energy as a function of
3:27:12
position as if this were a purely classical system for instance if the particle was found at a particular
3:27:18
position what would be its potential energy that's what this function v of x encodes now we know in quantum mechanics we
3:27:24
don't have classical particles that can be found at particular positions everything is probabilistic and uncertain but you can
3:27:30
see how this is related this is the time dependent schrodinger
3:27:35
equation which is a little bit unnecessarily complicated most of the time we work with the
3:27:41
time-independent schrodinger equation which looks very similar again we have a left-hand side given by the hamiltonian
3:27:48
we have a kinetic energy here and we have a potential energy here
3:27:56
if we're going to solve this time-independent equation note now that the wave functions here are expressed only as functions of position not as
3:28:03
functions of time this operator gives you the wave function itself back multiplied by e
3:28:09
which is just a number this came from the separation of variables it's just a constant and we
3:28:15
know from considering the expectation value of the hamiltonian operator which is related to the energy
3:28:20
for solutions to this time independent schrodinger equation that we know this is essentially the energy of the state
3:28:32
now what does it mean here in this context or in this potential context
3:28:38
well you have a potential function of position and you have psi the wave function
3:28:44
so this v of x psi of x if that varies as a function of position
3:28:51
and it will if the wave function has a large value a large magnitude in a certain region
3:28:58
and the potential has a large value in a certain region that means that there is some significant probability the particle
3:29:04
will be found in a region with high potential energy that will tend to make the potential
3:29:10
energy of the state higher now if psi is zero in some region where the potential energy is high that means
3:29:16
the particle will never be found in a region where the potential energy is high that means
3:29:22
the state likely has a lower potential energy this is all very sort of heuristic
3:29:28
qualitative argument and we can only really do better once we know what these solutions are and what these actual
3:29:34
potential functions look like um
3:29:40
what i'd like to do here before we move on is to rearrange this a little bit to show you what effect the potential
3:29:45
energy related to the energy and how it's related to the energy of the state what effect that has on the wave function
3:29:52
and in order to do that i'm going to multiply through by this h bar squared over 2m and rearrange terms a little bit
3:30:00
what you get when you do that is the second derivative of psi
3:30:05
with respect to x there's my eraser
3:30:10
with respect to x being equal to 2m over h bar squared times
3:30:18
v of x minus e psi
3:30:24
so this quantity here relates the second derivative of psi to psi itself
3:30:31
for instance if the potential is larger than the energy of the state you'll get one overall sign relating the
3:30:38
second derivative in psi whereas if energy is larger than potential then you'll end up with a negative quantity here
3:30:45
relating the second derivatives of psi with itself so keep this in the back of your mind
3:30:52
and let's talk about some example potential functions this is what we're going to be doing or
3:30:57
this is what the textbook does in all of chapter two write different potential functions and solve the schrodinger
3:31:02
equation the first example potential we do and this is section 2.2 is what i like to
3:31:08
call the particle in a box the textbook calls it an infinite square well
3:31:13
the particle in a hard box for instance you can think of as a potential function that looks like this
3:31:20
get myself some coordinate systems here you have a potential function v of x
3:31:25
oops turn off my ruler that looks something like this
3:31:34
this is v of x as a function of x the potential goes to infinity
3:31:40
for x larger than some size let's call this you know minus a to a if you're inside minus a to a
3:31:47
you have zero potential energy if you're outside of a you have infinite potential energy it's
3:31:53
a very simple potential function it's a little bit non-physical though because while infinite potential energy what does that really mean it means it would
3:31:59
require infinite energy to force the particle beyond a if you had some infinitely dense material that just
3:32:06
would not tolerate the electron ever being found inside that material and you made a box out of that material this is
3:32:12
the sort of potential function you would get much more realistically
3:32:17
we have the harmonic oscillator potential the harmonic oscillator potential
3:32:24
is the same as what you would get in classical physics it's a parabola this is something you
3:32:30
know proportional to x squared uh v of x being proportional to x
3:32:36
squared is what i mean this is what you would get if you had a particle attached to a spring connected
3:32:42
to the origin if you move the particle to the right you stretch the spring put quantum mechanically if you happen
3:32:47
to find the particle at a large displacement from the origin the spring would be stretched quite a
3:32:53
large amount and would have a large amount of potential energy associated with it from a more physical down to earth sort
3:33:00
of perspective this is what happens when you have any sort of equilibrium position for a particle to be in the
3:33:06
particle is sitting here near the origin where there is a flat potential but any displacement
3:33:11
from the origin makes the potential tend to increase in either direction this is a like a
3:33:16
an electron in a particle trap or an atom in a particle trap harmonic oscillator potentials show up
3:33:22
all over the place and we'll spend a good amount of time talking about them
3:33:28
the next potential that we consider is the delta function potential
3:33:33
and what that looks like now i'm starting going to start at zero and draw it going negative but
3:33:40
it's effectively an infinitely sharp infinitely deep
3:33:45
version of this particle in a box potential instead of going to infinity
3:33:50
outside of your realm it's at zero and instead of being a zero inside your realm it goes to minus infinity there
3:33:58
this now continues downwards it doesn't bottom out here
3:34:03
the overall behavior will be different now because the particle is no longer disallowed from being outside of the
3:34:08
domain there is no longer an infinite potential energy here and we'll talk about that as well these
3:34:14
are all sort of weird non-physical potentials the particle in a soft box potential is
3:34:20
a little bit more physical if i have my coordinate system here
3:34:25
the particle in a soft box potential looks something like this
3:34:31
to keep things simple it still changes instantaneously at say minus a and a but
3:34:37
the potential energy is no longer infinity this is for instance a box made out of a
3:34:42
material that has some pores in it the electron or whatever particle you're considering to be in the box doesn't like being in those pores
3:34:50
so there's some energy you have to add in order to push the particle in once it's in it doesn't really matter where
3:34:55
it is you've sort of made that energy investment to push the particle into the box and we'll talk about the quantum
3:35:01
mechanical states that are allowed by this potential as well finally we will consider what happens when
3:35:08
there's no potential at all essentially your potential function is constant
3:35:16
that actually has some interesting implications for the form of the solutions of the schrodinger equation
3:35:22
and we'll well we'll talk about that in more detail to map this onto textbook sections
3:35:29
this is section 2.2 the harmonic oscillator section 2.3
3:35:35
the delta function potential is section 2.5 the particle in a box is section 2.6
3:35:40
particle in a soft box is 2.6 and particle with no potential or an overall constant potential everywhere in space
3:35:47
is section 2.4 so these are some example potentials that we'll be talking about in this
3:35:52
chapter what do these potentials actually mean though how do they influence the
3:35:58
schrodinger equation and its solutions well the way i wrote the schrodinger equation
3:36:04
a few slides ago second derivative of psi with respect to x
3:36:09
is equal to two m over h bar squared just a constant times
3:36:15
v of x minus e psi this is now the time independent
3:36:21
schrodinger equation so we're just talking about functions of position here and e keep in mind is really is the
3:36:27
energy of the state if we're going to have a solution to the time independent schrodinger equation this e exists and it's just a number
3:36:36
so what does that actually mean let's think about it this way we have a
3:36:42
left-hand side determined by the right-hand side of this equation the left-hand side is just the second derivative with respect to position of
3:36:48
the wave function this is related to the curvature of the wave function i could actually write this as a total
3:36:53
derivative since this is just psi is only a function of position now
3:36:59
so there's no magic going on with this partial derivatives it's going to behave same as the ordinary derivative that you're used to from calculus class
3:37:06
the second derivative is related to the concavity of a function whether something's concave up or concave down
3:37:14
so let's think about what this means if you have a potential v of x that's
3:37:20
greater than your energy if v of x
3:37:25
is greater than e what does that mean that means v of x
3:37:30
minus e is a positive quantity that means the right hand side here will
3:37:37
have whatever sign psi has and i'm being a little sloppy since psi here is in general complex function
3:37:44
but if we consider it to just be say positive which isn't as meaningful for a complex number as it is for a real number
3:37:52
you would have psi of x if psi of x is positive and this number is positive then the second derivative
3:37:58
is positive which means that if we're say if psi is say here psi is positive
3:38:05
when it's multiplied by is positive then the second derivative is positive it curves like this
3:38:11
whereas if psi is down here psi is negative this is positive second derivative of
3:38:18
psi is negative it curves like this what this means is that
3:38:23
psi curves away
3:38:29
from our axis away from this psi equals 0 line
3:38:36
on the other hand if v of x is less than the energy
3:38:41
this quantity will be negative and we get the opposite behavior if psi is up here positive it's
3:38:48
multiplied by a negative number and the second derivative is negative you
3:38:54
get something that curves downwards if psi is on the other side of the axis it curves upwards
3:38:59
psi curves toward the axis
3:39:08
so this helps us understand a little bit about the shape of the wave function
3:39:14
for instance let me do an example here in a little bit more detail
3:39:21
suppose i have i'll do it over here
3:39:27
coordinate system if i have a potential function let's do the sort of soft particle in a
3:39:34
box i can do better than that
3:39:40
soft particle in a box so v of x is constant outside your central region and constant
3:39:47
inside your central region and has a step change at the boundaries of your region
3:39:52
let's think about what our wave function might look like under these circumstances
3:40:02
so we have our boundaries of our region here
3:40:12
the other thing that we need to know to figure out what the wave function might look like is a hypothetical energy and
3:40:18
i'm just going to set an energy here i'm going to do the interesting case
3:40:24
let's say this is the energy i'm plotting energy on the same axis as the potential which is fine this
3:40:31
is the energy of the state this is the potential energy as a function of position so they have the same units
3:40:37
what this energy hypothetically means is that outside here the potential energy is greater than the energy of the state
3:40:43
and inside here the potential energy is less than the energy of the state
3:40:49
so we'll get different signed sort of behaviors different curvatures of the wave function
3:40:55
so do my wave function in blue here if i say
3:41:00
start my wave function this is all hypothetical now this may not work if i start my wave function here
3:41:07
at some point on the positive side of the axis at the origin we know the energy of the
3:41:13
state is larger than the energy of or than the potential energy
3:41:18
so this quantity is negative and psi curves towards the axis so since
3:41:23
psi is positive here i'm looking at this sort of curvature so i could draw my wave function out sort of like this
3:41:33
maybe that's reasonable maybe that's not this is obviously not a quantitative calculation this is just sort of the
3:41:38
sort of curvature that you would expect now i only continue these curving lines
3:41:43
out to the boundaries since at the boundaries things change
3:41:50
outside our central region here the potential energy is larger than the energy of the state and you get
3:41:56
curvature away from the axis what might that look like
3:42:02
well something curving away from the axis it's going to look sort of like that
3:42:08
but where do i start it do i start it going like that do i start it going like that what does this actually look like
3:42:14
well if you think about this we can say a little bit more about what
3:42:19
happens to our wave function when it passes a boundary like this
3:42:25
and the key fact is that if v of x is finite
3:42:31
then while we might have the second derivative of psi
3:42:37
with respect to x being discontinuous
3:42:44
maybe might not be in this case the second derivative of
3:42:50
psi is just set by this difference so when we have a discontinuous discontinuity in the potential we have a discontinuity in the second derivative
3:43:00
the first derivative of psi will be continuous
3:43:11
think about integrating a function that looks like this
3:43:16
i integrate it once i get something maybe with large positive slope going to slightly smaller
3:43:22
positive slope there will be no discontinuity in the first derivative
3:43:29
what this means for psi is that it's effectively smooth and that i just by that i just sort of
3:43:35
mean no corners the first derivative psi won't ever show a corner like this it
3:43:41
will be something like that for example
3:43:46
no sharp corners to it what that means in the context of a boundary like this is that if i have psi
3:43:53
going downwards at some angle here i have to keep that angle as i cross the boundary
3:43:59
now once i'm on the other side of the boundary here i have to curve
3:44:06
and i have to curve according to the rules that we had here so depending on what i actually chose for my initial
3:44:11
point here and what the actual value of the energy was and what the actual value of the potential is outside in this region i may get differing degrees of
3:44:18
curvature i may get something that happens like this curves up very rapidly or i may get something that doesn't
3:44:24
curve very rapidly at all perhaps it's curving upwards very slowly
3:44:31
but it crosses the axis now as it crosses the axis the sine on
3:44:36
psi here changes the curvature is also determined by psi
3:44:41
as psi gets smaller and smaller the curvature gets smaller and smaller the curvature becoming zero as psi crosses the axis
3:44:47
then when psi becomes negative the sine of the curvature changes so this would start curving the other
3:44:54
direction curving downwards
3:44:59
it turns out that there is actually a state right in the middle sort of a happy medium state
3:45:05
where psi curves curves curves curves curves and just kisses
3:45:10
the axis
3:45:16
comes towards the axis and when it comes towards the axis and reaches the axis with zero slope and zero curvature it's
3:45:23
stuck it will never leave the axis again and these are the sorts of states that you might actually associate with
3:45:28
probability distributions you know if psi is blowing up like this going to positive infinity or to negative
3:45:34
infinity that your your wavefunction will not be normalizable but the wavefunction here denoted by
3:45:40
these green curves has finite area therefore is sort of normalizable
3:45:47
so these are the sorts of things that the potential function tells you about the wave function
3:45:54
in general what direction it curves how much it curves and how quickly of course doing this quantitatively
3:45:59
requires a good deal of mathematics but i wanted to introduce the mat or before i introduced the math i wanted to
3:46:05
give you some conceptual framework with which to understand what exactly this potential means
3:46:11
if the potential is larger than the energy you expect things that curve upwards and
3:46:18
when you get things that curve upwards you'll have a curve away from the axis you tend to have things blow up unless they just sort of go down and kiss the
3:46:25
axis like this so there will be a lot of things approaching the axis and never leaving
3:46:30
so that we have normalizable wave functions on the other hand if the potential energy is less than the energy of the
3:46:36
state you get things that curve towards and well if you have something that curves
3:46:42
towards it tends to do this always curving towards always curving towards always
3:46:47
curving towards the axis you get these sort of wave-like states
3:46:55
so that's a very hand-waving discussion of the sorts of behavior you get from
3:47:02
in this case uh step discontinuous potential
3:47:07
and we'll see the sort of behavior throughout this chapter to check your understanding take this
3:47:14
discontinuous potential and tell me which of these hypothetical wave functions
3:47:20
is consistent with the schrodinger equation now i did not actually go through and solve the
3:47:25
schrodinger equation here to make sure these things are quantitatively accurate
3:47:31
they're probably all not quantitatively accurate what i'm asking you asking you to do here is identify the sort of
3:47:37
qualitative behavior of these systems is the curvature right
3:47:46
and let's see yeah is the are the boundary conditions
3:47:52
right
3:47:59
uh in particular does the wave function behave as you would expect as it passes
3:48:05
from the sort of interior region to the exterior region
Infinite square well (particle in a box)
3:48:12
we've been talking about solving the schrodinger equation and how the potential function encodes
3:48:18
the scenario under which we're solving the schrodinger equation the first real example of a solution to
3:48:24
the schrodinger equation and a realistic wave function that we will get comes from this example
3:48:30
the infinite square well which i like to think of as a particle in a box
3:48:35
the infinite square well is called that because its potential is infinite and well square
3:48:43
what the potential ends up looking like is if i plot this
3:48:49
going from zero to a the potential is infinity
3:48:56
if you're outside the ray the region between 0 and a and
3:49:02
at 0 if you're in between the region if you're in between 0 and a
3:49:07
so what does this look like when it comes to the schrodinger equation well what we'll be working with now is
3:49:14
the time independent schrodinger equation the t i s e
3:49:19
which reads minus h bar squared over 2m times the second derivative of sorry i'm getting
3:49:26
ahead of myself the second derivative of psi with respect to x plus potential as a function of x times
3:49:34
psi is equal to the energy of the stationary state that results from the solution of
3:49:40
this equation times psi now this equation doesn't quite look
3:49:46
right if we're outside the region
3:49:53
bad things happen you end up with an infinity here for v of x if x is not between 0 and a
3:50:03
the only reason this the only way this equation can still make sense under those circumstances
3:50:09
is if psi of x is equal to zero if
3:50:15
x is less than zero or x is greater than a
3:50:20
so outside this region we already know what our wavefunction is going to be it's going to be zero and that's just a requirement on the basis of
3:50:27
infinite potential energy can't really exist in the real world now what if we're inside
3:50:35
then v of x is zero and we can cancel this entire term out of our equation
3:50:41
what we're left with then is minus h bar squared over two m
3:50:46
second partial derivative of psi with respect to x is equal to e times psi just dropping
3:50:53
that term entirely so this is the time independent schrodinger equation that we want to
3:50:59
solve so how do we solve it well
3:51:06
we had minus h bar squared over 2m times the second derivative of psi
3:51:11
with respect to x being equal to e times psi we can simplify that
3:51:17
just by rearranging some constants what we get
3:51:22
minus second derivative of psi with respect to x equal to minus
3:51:29
k squared psi and this is the sort of little trick
3:51:34
that people solving differential equations employ all the time knowing what the solution is you can define a constant that makes a little more sense
3:51:41
in this case using a square for k instead of just some constant k but in this circumstance k
3:51:48
is equal to root would go root 2 m times e
3:51:55
over h bar so this is our constant which you just get from rearranging this equation
3:52:05
this equation you should recognize this is the equation for a simple
3:52:11
harmonic oscillator a mass on a spring for instance now as i said before the partial
3:52:16
derivatives here don't really matter we're only only talking about one dimension and we're talking about the time independent schrodinger equation so
3:52:22
the wave function here psi is just a function of x not a function of x and time
3:52:29
so this is the ordinary the ordinary differential equation that you're familiar with for things like masses on springs and
3:52:35
what you get is oscillation psi as a function of x is going to be a
3:52:42
sine kx plus b cosine
3:52:47
kx and that's a general solution a and b here are constants
3:52:56
to be determined by the actual scenario under which you're trying to solve this equation
3:53:01
this equation now not the original schrodinger equation so these are our
3:53:06
solutions sines and cosines sines
3:53:14
and cosines that's all well and good but that doesn't actually tell us what the wave function is because well we don't know
3:53:20
what a is we don't know what b is and we don't know what k is either
3:53:25
we know k in terms of the mass of the particle that we're concerned with plunk's constant and the
3:53:31
e separation constant we got from deriving the time independent schrodinger equation
3:53:36
while that might be related to the energy we don't know anything about these things these are free parameters still
3:53:42
but we haven't used everything we know about the situation yet in particular we haven't used the boundary conditions and
3:53:49
one thing the boundary conditions here will determine is the form of our solution now what do i mean by boundary
3:53:54
conditions well the boundary conditions are what you get from considering the actual domain of
3:54:01
your solution and what you know about it in particular at the edges
3:54:06
now we have a wave function that can only be non-zero
3:54:12
between zero and a outside that it has to be zero so we
3:54:18
know right away our wave function is zero here and zero
3:54:24
here so whatever we get for those unknown constants a b and k it has to somehow
3:54:31
obey this we know a couple of things about the general form of the wave function
3:54:37
in particular just from consideration of things like the hamiltonian operator or the momentum operator we know that the
3:54:46
wave function itself psi must be continuous
3:54:55
we can't have wave functions that look like this
3:55:00
the reason for that is this discontinuity here would do very strange things to any sort of physical operator
3:55:06
that you could think of for example the momentum operator is defined as minus i h bar partial
3:55:12
derivative with respect to x the derivative with respect to x here would blow up and we would get a very strange value for the momentum
3:55:19
that can cause problems by sort of contradiction then the wave function itself must be continuous
3:55:27
we'll come back to talking about the boundary conditions on the wave function later on in this chapter but for now all we need to know is that the wave
3:55:32
function is continuous what that means is that since we're zero here
3:55:38
we must go through zero there and we must go through 0 there since we're 0 here
3:55:45
so what that means wrong color
3:55:52
means psi of 0 is equal to 0. and psi of a
3:55:57
is equal to zero what does that mean for our hypothetical solution psi of x equals a sine
3:56:06
kx plus b cosine kx
3:56:12
well first of all consider this one the wave function at 0 equals
3:56:17
0. when i plug 0 into this the sine of 0 k times 0 is going to be 0. the sine of 0
3:56:24
is 0. but the cosine of 0 is 1. so what i'll get if i plug in 0 for psi is 1 times b
3:56:33
so i'll get b now if i'm going to get 0 here that means
3:56:39
b must be equal to 0. so we have no cosine solutions
3:56:51
no cosine part to our solutions so everything here is going to start
3:56:57
like sines it's going to start going up like that
3:57:02
that's not the whole story though because we also have to go through
3:57:08
zero when we go through a so if i plug a into this
3:57:14
what i'm left with is psi of a
3:57:21
is equal to capital a times the sine of k a
3:57:26
if this is going to be equal to zero then i know something about ka
3:57:32
in particular the sine function goes through 0 for particular values of k
3:57:37
particular values of its argument sine of x is 0 for x equals integer multiples of pi
3:57:44
what that actually looks like on our plot here is
3:57:49
things like this our wave functions are going to end up looking like this
3:57:59
so let me spell that out in a little more detail our
3:58:04
psi of a wave function is a times the sine of k times a
3:58:13
and if this is going to be equal to zero ka has to be
3:58:18
either 0 plus or minus pi plus or minus 2 pi plus or minus 3 pi
3:58:26
etc this is just coming from all of the places where the sine of something
3:58:32
crosses zero crosses the axis now it turns out this
3:58:40
this is not interesting this means psi is 0 everywhere
3:58:46
since the sine of 0 is well sine k times a if ka is going to be 0 then everything
3:58:51
if ka is 0 k is 0. so the sine of k times x is going to be
3:58:56
0 everywhere so that's not interesting this is not a wavefunction that we can work with
3:59:02
another fact here is that these plus or minuses the sine of minus x is equal to minus
3:59:10
the sine of x sine is an odd function since what we're looking at here has a
3:59:15
normalization constant out front we don't necessarily care whether there's a plus or a minus sign coming from the
3:59:21
sine itself we can absorb that into the normalization constant
3:59:28
so essentially what we're working with then is that ka equals
3:59:34
pi 2 pi 3 pi et cetera which i'll just write as n
3:59:41
times pi now if k times a
3:59:47
is going to equal n times pi we can figure out what
3:59:53
um well let's substitute in for k which we had a few slides ago was root 2
4:00:00
m capital e over h bar so that's k k times a
4:00:06
is equal to n pi this is interesting we now have
4:00:11
integers coming from n here as part of our solution so we're no
4:00:16
longer completely free we in fact have a discrete set of values
4:00:22
now a that's a property of the system we're not going to solve for that m that's a property of the system h bar
4:00:28
that's a physical constant the only thing we can really solve for here is e so let's figure out what that tells us
4:00:33
about e and if you solve this for e you end up with n squared pi squared
4:00:40
h bar squared over 2m a squared
4:00:47
this is a discrete set
4:00:52
of allowed energies
Infinite square well states, orthogonality - Fourier series
4:01:00
i keep talking about solutions to the time independent schrodinger equation and how they have nice mathematical properties
4:01:06
what that actually means is well what i'm referring to are the orthogonality and completeness of
4:01:12
solutions to the time-independent schrodinger equation what that actually means is the topic of
4:01:17
this lecture to recap first of all these are what our stationary states
4:01:23
look like for the infinite square well potential this is the potential such that v of x
4:01:30
is infinity if x is less than 0 or x is greater than a
4:01:36
and 0 for x in between 0 and a
4:01:44
so if this is our potential you express the time independent schrodinger equation you solve it you
4:01:50
get sine functions for your solutions you properly apply the boundary conditions mainly that psi has to go to
4:01:56
zero at the ends of the interval because the potential goes to infinity there
4:02:02
and you get n pi over a times x as your argument to the sine functions
4:02:08
and you normalize them properly you get a square root of 2 over a out front
4:02:13
the energies associated with these wave functions and this energy now is the separation constant in from in the
4:02:19
conversion from the time dependent schrodinger equation to time independent schrodinger equation are proportional to n that index the
4:02:28
wave functions themselves look like sine functions and they have an integer number of half wavelengths or half
4:02:34
cycles in between 0 and a so this orange curve this is n equals 1.
4:02:41
blue curve is n equals two the purple curve is n equals three and the
4:02:49
green curve is n equals four if you calculate the squared magnitude
4:02:55
of the wavefunctions they look like this one hump for n equals one two humps for the blue curve n equals
4:03:01
two three humps for the purple curve n equals three and four humps
4:03:06
for the green curve n equals four so you can see just by looking at these wave functions that there's a lot of
4:03:12
symmetry one thing we talked about in class is that these wave functions are either
4:03:18
even or odd about the middle of the box and this is a consequence of the potential being an even function about
4:03:24
the middle of the box if i draw a coordinate system here going between 0 and a
4:03:31
either the wave functions have a maximum or they have
4:03:36
a 0. at the middle of the box
4:03:41
so for n equals one we have a maximum for n equals two we have a zero and this pattern continues
4:03:47
the number of nodes is another property that we can think about and this is the number of points
4:03:53
where the wave function goes to zero for instance the blue curve here for n equals two has one node
4:04:00
this trend continues as well if i have a wave function that for instance let me draw it in some absurd
4:04:06
color has one two three four five six seven nodes
4:04:13
you know this would be for n equals eight this would be sort of like the wave function for n equals eight
4:04:19
these symmetry properties are nice they help you understand what the wave function looks like but they don't really help you calculate
4:04:25
what helps you calculate are the orthogonality and completeness of these wave functions
4:04:31
so what does it mean for two functions to be orthogonal let's reason to at this from a
4:04:37
perspective which are more familiar the orthogonality of vectors we say two vectors are orthogonal if
4:04:44
they're at 90 degrees to each other for instance so if i had a two-dimensional coordinate system
4:04:49
and one vector pointing in this direction let's call that a and another vector pointing in this direction let's
4:04:54
call that b i would say those two vectors are orthogonal if they have a 90 degree angle separating them
4:05:01
now that's all well and good in two dimensions it gets a little harder to visualize in three dimensions and well
4:05:07
what does it mean for two vectors to be separated by 90 degrees if you're talking about a 17 dimensional space
4:05:13
in higher dimensions like that it's more convenient to define orthogonality in terms of the dot product and we say two
4:05:19
vectors are orthogonal in that case if the dot product of those two vectors is zero
4:05:25
now in two dimensions you know the dot product is given by the x components of both vectors ax times bx plus the y
4:05:32
component of so those two vectors multiplied together a y times b y
4:05:38
if this is zero we say these two vectors are orthogonal in three dimensions we can say plus
4:05:45
a z times b z and if this is equal to zero we say the vectors are orthogonal and you can
4:05:51
continue this multiplying together like components or same dimension of the components of
4:05:58
vectors in each dimension multiplying them together a1 b1 a2 b2 a3 b3 a4 b4
4:06:05
all added up together and if this number is zero we say the vectors are orthogonal
4:06:12
we can extend this notion to functions but what does it mean to multiply two
4:06:18
functions like this in the case of vectors we were multiplying like components both x components both y components both z
4:06:25
components in the case of functions we can multiply both functions values at
4:06:33
particular x coordinates and add all those up and what that ends up looking like is an integral say the
4:06:40
integral of f x g of x dx
4:06:45
so i'm scanning over all values of x instead of scanning over all dimensions
4:06:51
and i'm multiplying the function values at each individual point
4:06:56
at each individual x together and adding them all up instead of multiplying the components of each
4:07:02
vector together at each individual dimension and adding them all up
4:07:08
the overall concept is the same and you can think about this as in some sense a dot product of two functions
4:07:16
now in quantum mechanics since we're working with complex functions it turns out that we need to put a complex
4:07:21
conjugate here on f in order for things to make sense this should start to look familiar now
4:07:29
you've seen expressions like the integral of psi star of x times psi of x
4:07:35
dx is equal to one our normalization condition this is essentially the dot product of psi with itself
4:07:44
psi of course is not orthogonal to itself but it is possible to make a fun pair of
4:07:50
functions that are orthogonal and we say functions are orthogonal if
4:07:56
orthogonal or forgone
Infinite square well example - computation and simulation
4:08:10
so we've been working with solutions to the time independent schrodinger equation for the infinite
4:08:17
square well potential the particle in a box case how do these things actually work though
4:08:24
in order to give you guys a better feel for what the solutions actually look like and how they behave i'd like to do
4:08:29
some examples and use a simulation tool to show you what the time evolution of
4:08:34
the schrodinger equation in this potential actually looks like so
4:08:40
the general procedure that we've followed or will be following in this lecture is once we've solved the
4:08:46
time independent chosen schrodinger equation we get the form of the stationary states
4:08:51
knowing the boundary conditions we get the actual stationary states the stationary state wave functions and
4:08:57
their energies these can then be normalized to get true stationary state wave functions that we
4:09:02
can actually use these stationary state wave functions
4:09:08
will for the most part form an orthonormal set psi sub n of x we can add the time part
4:09:16
knowing the time dependent schrodinger equation or the time part that we got when we separated variables
4:09:22
in the time dependent schrodinger equation we can then express our initial conditions as a sum of these
4:09:30
stationary state wave functions and use this sum then to determine the
4:09:35
behavior of the system so what does that actually look like in the real world
4:09:40
not like not like very much unfortunately because the infinite square will potential is not very realistic but
4:09:46
a lot of the features that we'll see in this sort of potential will appear in more realistic potentials as well
4:09:53
so this is our example these are our stationary state wave functions this is
4:10:00
what we got from the solution to the time independent schrodinger equation this was the form of the stationary
4:10:07
states these were the energies and then this was the normalized solution with the time dependence added back on since
4:10:14
the time dependence is basically trivial the initial conditions that i'd like to
4:10:20
consider in this lecture are the wave function evaluated at zero
4:10:25
is either zero if you're outside the sorry
4:10:30
this should be a if you're outside the domain you're zero if you're inside the domain
4:10:36
you have this properly normalized wave function
4:10:41
we have an absolute value in this which means this is a little difficult to work with
4:10:46
but what the plot actually looks like if i draw a coordinate system here
4:10:52
going from zero to a is this
4:11:01
it's just a tent a properly built tent with straight
4:11:06
walls going up to a nice peak in the middle
4:11:12
our general procedure suggests that we express this initial condition in terms of these stationary states with their
4:11:18
time dependence and that will tell us everything we need to know one thing that will make this a little
4:11:23
easier to work with is getting rid of the absolute values we have here so let's express psi
4:11:29
of x time t equals 0 as a three part function
4:11:35
first we have root three over a one minus now what we should substitute in here is
4:11:42
what we get if say zero is less than x is less than a over two sort of the first half integral interval
4:11:49
going out to a over 2 here in this case we have something sloping upwards
4:11:56
which is going to end up in this context being 1 minus a over 2 minus x over a over 2.
4:12:05
so to say another word or two about that
4:12:10
if x is less than a over 2. this quantity here will be negative
4:12:18
so i can get rid of the absolute value if i know that this quantity in the numerator is positive so i multiply the
4:12:24
quantity in the numerator by a minus sign which i can express more easily just by writing it as a over 2 minus x a over 2
4:12:30
minus x that will then ensure that this term here this term here is positive for x is
4:12:38
in this range 1 minus that is then
4:12:43
this term in our wave function for the other half of the range root 3
4:12:49
over a 1 minus something and this is now from a over 2
4:12:55
is less than x is less than a the second half of the interval for the second half of the interval x is larger than a over
4:13:02
2. so x minus a over 2 is positive so i can take care of this absolute value just by leaving it as x minus a over 2.
4:13:08
i don't need to worry about the absolute value in this range so this is x minus a over two all over a over two
4:13:14
and of course if we're outside that we get zero
4:13:20
this technique of splitting up absolute values into separate ranges makes the integrals a little easier to express and a little easier to think about
4:13:28
so that is our initial conditions how can we express
4:13:34
these initial conditions as a sum of stationary state wave functions evaluated at time t equals 0.
4:13:40
this is where fourier's trick comes in if i want to express my initial conditions
4:13:46
as a sum of stationary state wave functions i know i can use
4:13:52
this sort of an expression this is now my initial conditions
4:13:59
and my stationary state wave functions are being left multiplied complex conjugated integrated over the domain
4:14:05
and that gives us our constants c sub n that go in this
4:14:11
expression for the initial conditions in terms of the stationary state wave functions the notation here is that if psi appears
4:14:17
without a subscript that's our initial condition that's our actual wave function
4:14:22
and if psi appears with a subscript it's a stationary state wave function so what does this actually look like
4:14:28
well we know what these functions are first of all we know that this function
4:14:36
which has an absolute value in it is best expressed if we split it up in two so we're going to split this integral up
4:14:41
into a one going from zero to a over two and one going from zero to a so let's do that we have c sub n equals
4:14:48
the integral from 0 to a over 2 of
4:14:54
our normalized stationary state wave function
4:15:00
which is root 2 over a times the sine of
4:15:05
n pi x over a that's this psi sub n star
4:15:11
evaluated at time t equals zero i'm ignoring time for now so even if i had my time parts in there i
4:15:18
would be evaluating e to the zero where time is zero so i would get one from those parts
4:15:24
then you have psi our initial conditions and our initial conditions for the first half of our interval
4:15:31
plus root 3 over a 1 minus a over 2 minus x
4:15:37
over a over 2. and i'm integrating that dx the second half of my integral
4:15:44
integral from a over 2 to a looks much the same root 2 over a
4:15:49
sine n pi x over a that part doesn't change the only part
4:15:55
that changes is the fact that we're dealing with the second half of the interval so the absolute value gives me
4:16:00
a minus sign up here more or less root 3 over a 1 minus x minus a over 2
4:16:07
over a over 2 dx so substitute in for n
4:16:15
and do the integrals this as you can imagine
4:16:21
is kind of a pain in the butt so what i'd like to do at this point is give you a demonstration of one way that
4:16:28
you can do these integrals without really having to think all that hard and that's doing them on the computer
4:16:33
you can of course use all from alpha to do these you can of course use mathematica but the tool that i would like to demonstrate is called sage
4:16:41
sage is different than wolfram alpha and mathematica and that sage is entirely open source and it's
4:16:47
entirely freely available you can download a copy install it on your computer and work with it whenever you want
4:16:53
it's a very powerful piece of software unfortunately it's not as good as the commercial alternatives of course but it
4:16:59
can potentially save you a couple hundred dollars the interface to the software that i'm using is their notebook web page so you
4:17:07
can use your google account to log into this notebook page and then you have access to this sort of
4:17:12
an interface so if i scroll down a little bit here
4:17:17
i'm going to start defining the problem a here that's our
4:17:24
domain our domain goes from 0 to a h bar i'm defining equal to 1 since that number is a whole lot more convenient
4:17:30
than 10 to the minus 31st n x and t those are just variables and
4:17:37
i'm defining them as variables given by these strings and x and t now we get into the physics
4:17:43
the energy that's a function of what index you have what your
4:17:50
which particular stationary state you're talking about this would be psi sub n this would be e sub n e sub n is equal
4:17:55
to n squared pi squared h bar squared over 2 m a squared that's an equation that we've derived
4:18:01
psi of x and t psi sub n of x and t in particular
4:18:07
is given by this it's square root of 2 over a times the sine function times this complex exponential which now uses
4:18:14
the energy which i just defined here psi star is the complex conjugate of psi
4:18:20
which i've just done by hand by removing the minus sign here more or less just to copy paste
4:18:27
g of x is what i've defined the initial conditions to be which is
4:18:32
square root of 3 over a times this 1 minus absolute value expression
4:18:37
and c sub n here that's the integral of g of x times psi
4:18:45
from 0 to a over 2 plus g of x times psi going from a over 2 to a
4:18:51
that's all well and good now i've left off the psi stars but since i'm evaluating at time t equals 0
4:18:57
it doesn't matter psi is equal to psi star at t equals 0. i did have to split up the integral from
4:19:03
0 to a over 2 and a over 2 to a because otherwise sage got a little too complicated in terms of what it thought
4:19:09
the integral should be but given all this i can plot for instance g and if i click evaluate here
4:19:16
momentarily a plot appears this is the plot of g of x as a function of x now i define a to be
4:19:23
equal to one so we're just going from zero to one this is that tent function i mentioned
4:19:29
if i scroll down a little bit we can evaluate c of n this is what you would get if you
4:19:35
plugged into that integral that i just wrote on the last slide
4:19:40
you can make a list evaluating c of n for x going from one to ten
4:19:46
and this is what you get you get these sorts of expressions four times square root of six over pi squared
4:19:52
or minus four root six over pi squared divided by nine four root six over pi squared over 25
4:19:58
4 root 6 over pi squared over 49 you can see the sort of pattern that we're working with
4:20:03
some number divided by an odd number raised to the nth power squared
4:20:10
we can approximate these things just to get a feel for what the numbers are actually like
4:20:16
and we have 0.99 minus 0.11 plus 0.039 etc
4:20:22
moving on down so that's the sort of thing that we can
4:20:28
do relatively easily with sage get these types of integral expressions
4:20:34
and their values um you can see i've done more with this sage notebook and we'll come back to it in a moment but for now
4:20:41
these are the sorts of expressions that you get for c sub n
4:20:48
so our demo with sage tells us c sub n equals
4:20:54
some messy expression and it can evaluate that messy expression and tell us what we need to know
4:21:02
now the actual form of the evaluated c sub n was not actually all that complicated and if we truncate our sum
4:21:10
instead of summing from now this is expressing psi of x t our
4:21:15
wave function as an infinite sum n equals 1 to infinity of c sub n
4:21:21
psi sub n of x and t if i truncate this sum at say n equals 3
4:21:27
i'll just have a term from psi 1 and psi 3. recall back from the sage results that
4:21:32
psi 2 the coefficient of psi 2 c sub 2 was equal to 0.
4:21:38
so let's find the expectation of x squared knowing the form of these functions and
4:21:44
now knowing the values of these c sub n from sage you can write out what x squared should
4:21:51
be this is the expected value of x squared and it's going to be an integral
4:21:57
of these numbers 4 root 6 over pi squared times psi 1
4:22:04
which was root 2 over a sine n so they're just dealing with psi 1.
4:22:10
now we have pi x over a we have to include the time dependence
4:22:16
now since i'm looking for the expected value of x squared as a function of time now
4:22:22
then we have e to the minus i times
4:22:27
pi squared h bar squared t over 2 m a squared
4:22:34
all divided by h bar or i could just cancel out one of the h bars here that's our first term
4:22:42
in our first term of our expression the next term we have
4:22:48
4 root 6 over 9 pi squared from this coefficient now psi 3
4:22:54
is root 2 over a sine of
4:22:59
3 pi x over a times again complex exponential e to the
4:23:05
minus i pi squared h bar squared t over 2
4:23:10
m sorry 9 pi squared h bar squared t over 2 m a squared all divided by h bar
4:23:18
now what is this this whole thing needs to be complex conjugated because
4:23:24
this is psi star what's next well i need to multiply this
4:23:31
by x squared and i need to multiply that by
4:23:37
the same sort of thing e to the plus this minus
4:23:43
same sort of thing e to the plus this
4:23:48
so these this is the term in orange brackets here is psi star
4:23:54
this is our x the term in blue brackets here is our psi so we're just using the same sort of
4:23:59
expression only you can certainly see just how messy it is this is
4:24:05
the integral of psi star
4:24:12
x squared psi
4:24:18
this is psi star this is x squared and this stuff is psi
4:24:24
we have to integrate all of this dx from 0 to a
4:24:31
this is pretty messy as well messy but doable now since i was working with sage anyway
4:24:37
i thought let's see how the time dependence in this expression plays out in sage
4:24:44
so going back to sage we know these
4:24:49
c sub n's these these are the c sub n's that i chose for c sub one and c sub three
4:24:58
and c sub n of x gives me some digits or um sorry c sub n
4:25:08
evaluated gave me these numbers
4:25:13
in uh just in decimal form now i can use these c sub n's to express
4:25:18
that test function where i truncated my sum at psi sub 3.
4:25:24
so this is our test function in fact if you evaluate it it's a lot more simple when you plug in
4:25:29
the numbers sine 3 pi x and sine pi x when h bar is one and a is one these
4:25:35
these expressions are a lot easier to work with which gives you a feeling for why quantum mechanics
4:25:40
quantum mechanics often we assign h bar equal to one
4:25:45
the expected value of x squared here is then the integral of the conjugate of
4:25:52
my test function times x squared times times my test function
4:25:57
integrated from 0 to a and sage can do that integral
4:26:03
it just gives you this sage can also plot what you get as a result
4:26:08
now you notice sage has left complex exponentials in here if you take this expression and manually
4:26:15
simplify it you can turn this into something with just a cosine there is no complex part to this expression but sage
4:26:21
isn't smart enough to do that numerically so if i ha so i have to take the absolute value of this expression to make the complex parts the tiny tiny
4:26:28
complex parts go away and if i plot it over some reasonable range this is what it looks like
4:26:34
it's a sinusoid or a cosine you saw it actually and what we're looking at here on the y
4:26:40
axis is the expected value of x squared this is related to the variance
4:26:45
in x so it's a measure of more or less the uncertainty in position so our
4:26:52
uncertainty in position is oscillating with time what does this actually look like in the
4:26:57
context of the wave function well
4:27:02
the wave function itself is going to be a sum you know c sub 1 times psi 1
4:27:08
c sub 3 times psi c 3 c sub 5 times 5 c sub 7 times psi 7 etc i can do that in
4:27:14
general by making this definition of a function where i just add up all of the c sub n's all the size of n's for n in
4:27:21
some range f of x if i go out to 7 looks like this
4:27:29
you get you can get a feel for what it would look like if i added more terms as well
4:27:34
now the plot that i'm showing you here is a combination of four things first it's the initial conditions shown
4:27:40
in red that's the curve that's underneath here the tent
4:27:46
i'm also you showing i'm also showing you this approximate wave function when i truncate the sum at two just the first
4:27:53
term that's this poor approximation here smooth curve
4:27:59
the function if i truncate the approximation at 4 that will include psi 1 and psi 3.
4:28:05
that's this slightly better approximation here this one
4:28:11
and if i continue all the way up to 20 that's this quite good approximation the
4:28:18
blue curve here that comes almost all the way up to the peak of the tent
4:28:23
so that's what our approximate wave functions look like but these are all evaluated at t equals 0.
4:28:29
what does that look like for instance in terms of the probability density and as a function of time
4:28:37
so let's define the probability density rho of x t as the absolute value of our approximate
4:28:44
function and i'll carry the approximation all the way to n equals 20. absolute value squared
4:28:50
and i'm getting the approximate form with this dot n at the end
4:28:55
so this is our approximate form of the probability density calculated with the first um 20
4:29:01
uh stationary state wave functions this plot then shows you what that time
4:29:07
dependence looks like i'm plotting the probability density at time t equals
4:29:13
0. probability density at time t 0.04 0.08 0.12 0.16
4:29:19
we start with blue dark blue that's this sort of peaked curve
4:29:26
which should be more or less what you expect because we did a problem like this for this sort of wave function in class
4:29:33
then you go to dark green which is under here underneath the yellow
4:29:40
it seems to have lost the peak and it spread out slightly
4:29:46
red is at time 0.08 and if i scroll back up
4:29:53
to our uncertainty as a function of time plot 0.08 is here
4:29:58
so it's pretty close to the maximum uncertainty you expect the uncertainty the width to start decreasing thereafter
4:30:06
if i scroll back down here this red curve then is more or less as wide as this distribution will ever get
4:30:14
and if we continue on in time now going to 0.12 that was the orange curve here
4:30:21
and the orange curve is back on top of the green curve the wave function has effectively gotten narrower again
4:30:26
if you keep going all the way up to 0.16 you get the cyan curve the light blue curve which is more or less back on top
4:30:31
of the dark blue curve so the wave function sort of spilled outwards and then sloshed back
4:30:38
inwards you can sort of imagine this is ripples in a tank of water radiating out and then coming back to the center
4:30:46
this is what the time evolution would look like as calculated in sage
4:30:52
you can make definitions of functions like this you can evaluate them you can plot them and you can do all of that
4:30:58
relatively easily now i'll give you all a handout of this worksheet so that you get a feel for the
4:31:04
syntax if you're interested in learning more about sage please ask me some questions i think sage is a great tool
4:31:09
and i think it has a promising future especially in education like this for for students the fact that this is free
4:31:16
is a big deal so that's what the time variability
4:31:22
looks like we had our wave function which started off sort of sharply peaked
4:31:27
our probability density excuse me rho of x which i should actually write as row of
4:31:34
x and t which sort of got wider
4:31:39
and then sloshed back in so we sort of had this outwards motion followed by inwards
4:31:45
motion where our expectation of x squared related to
4:31:50
our uncertainty oscillated oh sorry it didn't oscillate
4:31:58
about x equals zero it oscillated about some
4:32:04
larger value or sorry it didn't oscillate about zero it oscillated about some some larger value so there's some sort of mean uncertainty here sometimes
4:32:11
you have less uncertainty sometimes you have more uncertainty that's the sort of time dependence you get
4:32:16
from quantum mechanical systems
4:32:21
to get an even better feel for what the time variability looks like there's a simulation that i'd like to
4:32:27
show you and this comes from falstad.com which as far as i can tell is a guy who
4:32:33
was sick of not being able to visualize these things so he wrote a lot of software to help him visualize them
4:32:41
so here's the simulation and i've simplified the display a little bit to make things easier to understand
4:32:48
these circles on the bottom here each circle represents a stationary state wave function
4:32:54
and he has gone all the way up to stationary state wave functions that oscillate very rapidly in this case but this is our ground
4:33:01
state this is our first excited state second excited state third excited state etc n equals 1 2 3 4 5 6 7 etc
4:33:11
now in each of these circles there may or may not be a line the line the length of the line represents the magnitude of
4:33:18
the time part of the evolution of that particular stationary state and the angle going around the circle here
4:33:24
represents the phase as that evolution proceeds so if i unstop this simulation
4:33:32
you can see this slowly rotating around
4:33:37
you're also probably noticing the color here changing the color of this represents the phase
4:33:43
this the vertical size of this represents the probability density and the color
4:33:48
represents the phase so it's a representation of where you're likely to find it and a represent and a sort of
4:33:54
color based representation of how quickly it's evolving the vertical red line here in the center tells you what
4:34:00
the expectation value for position is and in this case it's right down the
4:34:06
middle if i freeze the simulation and add
4:34:14
a second wave function this is now adding some
4:34:19
component of the first excited state and by moving my mice around here i can add varying amounts either adding none or a
4:34:25
lot and i can add it at various phases i'm going to add a lot of it an equal amount
4:34:30
is the ground state and i'm going to do it at the same phase and i'm going to release and let that evolve
4:34:37
so you can see now the probability density is sort of sloshing to the left
4:34:42
and sloshing back to the right and if you look at
4:34:47
our amplitude and phases you can see the ground state is still rotating the first excited state is rotating but
4:34:53
the first excited state is rotating four times faster
4:35:00
so when they align you have something on the right when they anti-align something on the left
4:35:06
they're aligned they're anti-aligned and this sloshing back and forth is one
4:35:13
way where we can actually get motion out of
4:35:18
stationary states you notice the phase is no longer constant you have some red parts and
4:35:24
purple parts and things are sort of moving around in an awkward way the colors are hard to read but you know now
4:35:30
that the phase of your wave function is no longer going to be constant as a function of position
4:35:37
so those exponential time parts may be giving you a wave function that's purely real here and purely imaginary here or
4:35:44
some combination of purely real and real and imaginary some general complex number and that complex number is not
4:35:51
simply e to the i omega t it's e to the i omega something that's a function of position as well as time it's it's
4:35:57
complicated i can of course add some more wave functions here
4:36:05
and you get even more complicated sorts of evolution
4:36:10
our expected value of x is now bouncing around fairly erratically our phase is bouncing around even more
4:36:16
erratically but what we're looking at here is just the sum of the first one two three four five six stationary states each evolving
4:36:24
with the same amplitude and different phases
4:36:29
now i'm going to stop the simulation and clear it now another thing i can do with this simulation tool is put a gaussian
4:36:37
into the system so i'm going to put a gaussian in here
4:36:44
so this is sort of our initial conditions and the
4:36:49
simulation has automatically figured out well i want this much i want a lot of the first
4:36:55
of the ground state side one a lot of psi 3
4:37:01
a lot of psi 5 a lot of size 7 a little bit of psi 9 a little bit of psi 11 etc
4:37:08
and if i play this i'll slow this down a little bit first if i play this
4:37:14
you see the wave function gets wide becomes two gets narrower again and sloshes back where it started
4:37:21
if you watch these arrows down here you can tell when it comes back together the arrows
4:37:27
are all pointing in the same direction and when it's dispersed the arrows are sort of pointing in opposite directions
4:37:34
since our initial conditions were symmetric there's no reason to expect the expected value to ever be non-zero non ever move
4:37:41
away from the center of this well but as your
4:37:48
say psi one psi three psi five psi seven etc oscillate at their own rates in time the
4:37:56
superposition results in a relatively complicated dynamics for the overall probability density
4:38:03
and of course i can make some ridiculously wacky excited era initial conditions
4:38:09
that just sort of oscillate all over the place in a very complicated way
4:38:15
there are a lot of contributions to this wave function now and not no any no one contribution is
4:38:22
particularly winning to occasionally see little flashes of order in the wave function
4:38:28
i highly encourage you to play with these simulations just to get a feel for how time evolution the schrodinger
4:38:33
equation works there are a lot more than just the square well here there's a finite well
4:38:39
harmonic oscillator pair of wells there are lots of things to play with so you can get a reasonably good feel with how
4:38:44
the schrodinger equation behaves in a variety of physical circumstances
4:38:54
so that's our simulation and hopefully you have a better feel now for
4:38:59
what solutions to the schrodinger equation actually look like to check your understanding
4:39:05
explain how these two facts are related time variability in quantum mechanics
4:39:10
happens at frequencies given by differences of energies whereas in classical physics you can set
4:39:16
the reference level for potential energy to whatever you want sort of equivalent to saying i'm measuring gravitational
4:39:21
potential from ground at level versus from the bottom of this well
Quantum harmonic oscillators via ladder operators
4:39:29
the system we're considering in this lecture is the quantum harmonic oscillator there are a few ways to solve the
4:39:35
schrodinger equation for the quantum harmonic oscillator but what we're going to do in this lecture is a solution more
4:39:40
or less by pure cleverness the solution is called the solution by ladder operators and we'll see what that
4:39:46
means in a few minutes just to set the stage the potential that we're working with here is the potential
4:39:52
of a harmonic oscillator the amount of energy essentially that you get if you displace a particle attached to a
4:40:00
spring from equilibrium if you remember spring potential energy the potential as a function of x is one half the spring
4:40:07
constant times the displacement x squared but it's traditional to write this instead
4:40:13
in terms of the angular frequency the angular frequency of the oscillations that result when a mass m is on a spring
4:40:20
with spring constant k is the square the square root of the spring constant divided by the mass of the
4:40:26
particle and if you substitute this in here and mess around with the simplification a
4:40:31
little bit you end up with one half m omega squared x squared so this is the form of the potential that we'll be
4:40:37
using what this looks like if i plot it
4:40:43
is a parabola
4:40:48
not the world's prettiest parabola but you get the idea and we know a little bit about what solutions to
4:40:54
the schrodinger equation should look like under circumstances like this let me draw this a little lower so i
4:41:00
have room if i have some energy e in this combined
4:41:07
energy wave function axis making a diagram of what the wave function looks like
4:41:12
if i start my wave function here you know in this region the energy is above the potential so the
4:41:18
schrodinger equation solutions have to curve downwards and what they end up looking like
4:41:24
is well something like this say now in the regions outside here where the
4:41:31
potential is above the energy the schrodinger equation solutions curve upwards in the case of
4:41:36
the harmonic oscillator solutions they curve just down to kiss the axis
4:41:44
and you end up with a nice sort of hump shaped wave function if you have a higher energy
4:41:51
say up here it's entirely possible to get solutions that look different suppose i
4:41:57
started my wavefunction here pointed at some angle the energy now is higher relative to the
4:42:04
potential so the wave function is going to curve more and it's possible to make it curve down to the point where when it
4:42:09
reaches this point now where the potential is higher than the energy and it starts curving back upward
4:42:14
you again get away function that just smoothly joins in with the
4:42:20
well with the axis giving you a sort of nice normalizable wave function so these are the sorts of solutions that
4:42:26
we expect to get if you want to get these solutions just by well like drawing them like i just
4:42:33
did you can conceptually understand what they look like but quantitatively you'll have to do a lot of fine tuning to get
4:42:38
these energy levels exactly right and to get the initial conditions here i just started my wave function how high up
4:42:44
should i start my wave function or in this case should i start it at the middle should i just place it what should this angle here be
4:42:50
fine tuning like that is hard and we'll see how to do that in the next lecture but in this case we're going to make a
4:42:57
solution by cleverness instead of fine tuning
4:43:03
to set that up let's go back to the time independent schrodinger equation this is the general time independent
4:43:09
schrodinger equation where now we're going to be substituting in the harmonic oscillator potential one half m omega
4:43:15
squared x squared that means the harmonic oscillator time
4:43:20
independent schrodinger equation that we actually have to work with minus h bar squared over 2m times partial derivative
4:43:27
of psi with respect to x squared plus one half m omega squared x squared
4:43:36
psi is equal to e psi
4:43:43
so this here is the hamiltonian operator the time independent schrodinger
4:43:49
equation is also often just written as h psi equals e psi
4:43:55
and that's fine this let's take a look closer look at this hamiltonian operator maybe we can do something with it
4:44:04
the cleverness comes in in this step consider factoring the hamiltonian
4:44:11
well i can simplify this a little bit by pulling out for instance a 2m here
4:44:17
and writing this as the momentum operator squared this is essentially p squared over 2m the
4:44:23
kinetic energy part this is the potential energy part if i pull out one
4:44:28
over two m what i get one over two m p hat squared
4:44:35
plus m omega x quantity squared
4:44:40
this is suggestive if we had numbers
4:44:45
and i had something like a squared plus b squared i could factor that over the complex
4:44:51
numbers as i a plus b times minus i a
4:44:57
plus b if you expand this out you'll end up getting a plus a squared for multiplying
4:45:03
these a plus b for multiplying these and similar to the uh the cross terms in say
4:45:10
a minus b times a plus b the cross terms end up canceling out
4:45:15
and we would end up with what we started now this is suggestive
4:45:21
you can't actually factor operators like this because they're not
4:45:27
numbers they're operators and operators don't necessarily behave the same way numbers behave
4:45:32
we'll see what that means in a minute but for now let's just suggest
4:45:40
looking at things like this plus or minus i times the momentum operator plus
4:45:46
m omega x where x now is the position operator now x the position operator just entails multiplying by x so perhaps
4:45:53
i should put a hat here perhaps i shouldn't doesn't really matter this is
4:45:59
what we're considering now i haven't justified this in any way beyond saying
4:46:04
it kind of looks like maybe it would factor well
4:46:10
does it factor these things are called ladder operators
4:46:15
and they're traditionally defined just to make the notation a little bit simpler a hat and there's either a plus
4:46:21
or minus on this let me draw this a little bigger a hat plus or minus in the subscript
4:46:28
and these are defined to be 1 over the square root of 2 h bar m omega
4:46:37
the constant just makes things more nice overall times minus or plus
4:46:43
i p hat plus m omega x this is now the position operator x hat
4:46:52
so this is the traditional definition let's see if we have something that properly factors
4:46:59
what we should have is that a hat minus times a hat plus
4:47:05
is our hamiltonian is this true this is an operator algebra problem and
4:47:13
operator algebra problems are tricky to do without test functions but initially we can just write this out we have
4:47:19
two a's being multiplied together so we're going to have a 1 over 2 h bar m
4:47:24
omega out front and then we're going to have i p hat plus m omega x
4:47:32
times minus i p hat plus m omega x
4:47:39
once again hats on the x's if you prefer so so far we've just written down our
4:47:46
operators in order now if i actually tried to expand these out
4:47:52
1 over 2 h bar m omega now this term
4:48:00
i times minus i that's just plus one so we would get p hat squared
4:48:09
so far so good for this term this is okay as well plus
4:48:16
m squared omega squared x hat squared that's still okay we're still on track
4:48:23
this was more or less what our hamiltonian looked like the cross terms get a little more interesting though
4:48:29
we have a term like this which gives us
4:48:35
let's see we're going to end up with a minus i from this minus i
4:48:40
m omega and we have x hat p hat
4:48:46
we're going to end up with something very similar from this term we're going to have an i we're going to
4:48:52
have an m we're going to have an omega except in this case we're going to have p hat x hat not x hat p hat as we had
4:48:58
here so i'm going to factor
4:49:04
the constants out and do that in the right color
4:49:11
that means we're going to have minus p hat x hat here
4:49:17
so this is what we get when we expand this out this part here
4:49:23
looks a lot like the hamiltonian so we're on the right track it's actually like 1 over h bar omega times the
4:49:29
hamiltonian this part though this is a little more difficult to work with and it turns out
4:49:36
that this piece right here this sort of thing appears a lot in quantum mechanics and we have a name for
4:49:43
it and a notation for it and the notation is x hat comma p hat
4:49:48
in square brackets this is called a commutator
4:49:54
and fundamentally the fact that i can't just subtract these two things from each other and get zero is one of the most
4:50:00
fundamental parts of quantum mechanics one of the most fundamental features of quantum mechanics
4:50:07
so let's talk about commutators in a little more detail the commutator in general
4:50:13
is defined for two operators a and b to be what you just saw on the last page
4:50:18
first i have a b and then i subtract a sorry and then i subtract the opposite
4:50:25
order b a so if i acted on this or if i used this
4:50:31
operator this combined operator to act on a wave function i would first let a act and then let b
4:50:37
act and i would subtract that from what i get if i let b act and then a act
4:50:45
just to make that a little more explicit if i had a b
4:50:50
minus b a acting on some wave function i would say
4:50:55
that's a b psi minus b a
4:51:01
psi you don't necessarily get the same answer for both of these things because
4:51:07
the order in which operators act is important
4:51:13
so let's look then at our commutator the commutator we had in the last
4:51:18
slide was x and p commutator of x and p is x hat p hat
4:51:24
minus p hat x hat and let's
4:51:30
allow this to act on some wave function psi
4:51:35
in order to make my notation correct i ought to have the same sort of psi here
4:51:44
so if i allow this to act on psi first we're going to have x hat p hat psi
4:51:51
minus p hat x hat psi and what this means
4:51:56
is x hat is acting on p hat acting on psi and this is p hat acting on x hat acting
4:52:02
on psi we have definitions for these things
4:52:08
x hat is just x multiplied by something
4:52:13
and p hat is minus i h bar times the derivative
4:52:18
of something in this case psi our second term here
4:52:24
is minus i h bar times the derivative of with respect to x of x times psi
4:52:34
when i apply the derivative here i have to use the product rule since i have a product of two terms i'll have to hit x we want in one term
4:52:41
and psi in the other term so on the left the left most term here is
4:52:48
easy to deal with though it's just minus i h bar x d psi
4:52:53
dx actually let's factor out a minus h bar
4:52:59
i h bar from both terms since they both have it so x d side x is my first term here
4:53:07
then i have minus if i use the derivative on the x derivative of x with respect to x is
4:53:13
just one so all i'll be left with is the psi remaining untouched in the product rule
4:53:20
and if i let the derivative hit the psi i'll leave the x untouched
4:53:25
and i'll have the derivative of psi with respect to x
4:53:30
this is good because here i have an x decide ex minus x d psi
4:53:35
dx so i can let these terms subtract out and cancel
4:53:41
and what i'm left with i have a minus i h bar times minus psi which is just going to be
4:53:46
i h bar psi so i started with the commutator acting on the wave function and i just got
4:53:52
constant multiplied by the wave function so i can drop my hypothetical wave function now and just write an equation
4:53:59
involving the operators again the commutator of x and p is
4:54:04
i h bar it's a weird looking equation
4:54:11
but you can see if you recall from the last slide what we're going to end up with when we evaluated a minus hat a
4:54:19
plus hat we ended up with 1 over h bar omega
4:54:25
times the hamiltonian plus some constants and if you flip back
4:54:31
a slide the ih bars end up actually canceling out and we just end up with plus
4:54:36
a half for our constant so while we did not succeed in fully factoring the hamiltonian we did get the
4:54:43
hamiltonian back plus a constant
4:54:48
and if you actually if you reverse the order and repeat the algebra a hat plus
4:54:55
a hat minus you end up with the same sort of thing it looks very similar you get one over h bar omega times the
4:55:01
hamiltonian minus a half instead
4:55:07
what this means is we can express the hamiltonian in terms of these ladder operators
4:55:12
and these constants what we get for the hamiltonian
4:55:19
h hat is h bar omega times a minus a plus operators
4:55:27
minus a half or alternatively the hamiltonian is equal to h bar omega
4:55:33
a plus a minus plus a half
4:55:40
so these are the sorts of things that we got from our operator algebra after attempting to factor the hamiltonian
4:55:46
that was pretty clever but it didn't actually get us a solution it just got us a different expression of the problem
4:55:53
the cleverness really comes in considering ladder operators and energy the time independent schrodinger
4:55:59
equation here is h hat psi equals e psi so suppose we have some solution psi to
4:56:06
the schrodinger equation we can then express the hamiltonian in terms of these ladder operators h bar omega
4:56:15
times a plus a minus operators plus one half
4:56:22
acting on the wave function should be equal to e times the wave function
4:56:29
the clever part is this what if i consider h hat times
4:56:37
a plus psi what happens to
4:56:43
the wave function if i allow a plus to act on it before i allow the hamiltonian to act on it
4:56:50
now assuming this is the case maybe we can manipulate our expressions here involving the hamiltonian and the ladder
4:56:55
operators to get something with which we can apply our solution
4:57:01
let's see what happens expressing the hamiltonian now as ladder operators h bar omega a hat plus a hat minus
4:57:10
plus one half now acting on a plus hat psi
4:57:16
forgot my hat there sorry looking at this
4:57:22
you can take a plus psi and distribute it in to the expression in parentheses
4:57:27
here h bar omega a plus hat a minus hat
4:57:32
a plus hat psi plus a half psi
4:57:40
put another way i'm really just distributing the operator in and that's actually a more convenient
4:57:46
way to look at it so i'm going to erase my size here and i'm going to leave my psi outside
4:57:51
the expression oops and i forgot an a plus hat here
4:57:57
sorry about that just distributing the a plus in here you'll end up with plus minus plus and just plus on the one half
4:58:06
now you notice i have an a plus here and an a plus here
4:58:12
if you think if you think about factoring this out to the left that's actually allowed as well
4:58:19
i can rewrite this as h bar omega a plus hat in front of the expression a
4:58:25
minus hat a plus hat plus one half
4:58:31
all acting on psi that's okay
4:58:37
what's nice about this is if you look we have here now an h bar omega and an a minus a plus
4:58:44
if i had the appropriate constant here which would turn out to be minus a half i would have the hamiltonian back and
4:58:50
getting the hamiltonian back means we might be able to apply our schrodinger equation so let's rewrite this as h bar omega
4:58:58
a plus hat times a minus a plus
4:59:04
minus a half plus one i haven't changed anything now except
4:59:10
this piece this is my hamiltonian
4:59:17
i had two expressions for the hamiltonian that i got from calculating the product of ladder operators one if i
4:59:22
did a plus first and then a minus one if i did a minus first and then a plus and they were different by the sign that
4:59:28
appeared here so the fact that this is the hamiltonian
4:59:34
allows me to rewrite things a little bit turns out i can rewrite this whole expression as a plus hat acting on
4:59:42
the hamiltonian and you have to distribute the h-bar omega in hamiltonian operator plus h-bar omega
4:59:51
acting on psi so i'm i'm starting to lose my ladder
4:59:56
operators which is a good sign because i don't actually want expressions with lots of ladder operators in them i'd like expressions with things that i know
5:00:02
in them and it turns out you know what happens when the hamiltonian acts on psi so if i
5:00:09
distribute psi in here i'll just have psi times h bar omega and the hamiltonian acting on psi
5:00:15
but you know the hamiltonian acting on psi is e times psi
5:00:20
so we're definitely making progress now this is going to become a plus hat
5:00:26
times e plus h bar omega psi
5:00:33
this now is all constant so it doesn't matter if i put it in between the ladder operator and the wave
5:00:40
function or not so i can pull that out and make this e plus h bar omega
5:00:47
times ladder operator a plus acting on the wave function psi
5:00:53
if i rewrite my entire equation then i end up with h hat acting on
5:00:59
ladder operator psi a plus psi is equal to e plus
5:01:05
h bar omega ladder operator acting on psi a plus acting on psi
5:01:11
this looks a lot like the schrodinger equation for a wave function given by a plus psi
5:01:18
so if psi is a solution to the time independent schrodinger equation
5:01:23
a plus psi is also a solution to the time independent schrodinger equation
5:01:29
with this new energy that's really the clever part
5:01:36
if psi is a solution a plus psi
5:01:42
is also a solution
5:01:48
that's really quite interesting what that means is if i have one solution psi
5:01:53
i can apply the ladder operator which i've just been writing as a plus hat here but we know what the ladder
5:01:59
operator a plus is it's a combination of the momentum operator and multiplication by x with appropriate constants thrown
5:02:05
in we know about a plus i if we knew the wave function we could actually do this it would involve some taking some
5:02:11
derivatives and multiplying by some constants we can do that so this gives us some machinery for
5:02:17
constructing solutions from other existing solutions we haven't actually solved the system
5:02:23
yet there's a little bit of cleverness left and this has to do with ladder operators and the ground state
5:02:30
what we showed on the last slide was that if psi was a solution
5:02:36
then a plus hat psi was a solution
5:02:42
with energy e plus h bar omega
5:02:47
it turns out a minus hat psi you can follow through the same algebra
5:02:54
is also a solution but it has energy e minus h-bar omega
5:03:00
so suppose we have some solution psi and i'll call it psi sub n now
5:03:08
if we apply the ladder operator a plus psi
5:03:14
we'll end up with some wave function psi n plus 1. it's another solution to the schrodinger equation it has a slightly
5:03:19
higher energy the energy has been increased by the amount h bar omega here
5:03:25
i can repeat that process and i'll get say something i would call
5:03:31
psi n plus 2. and you can keep keep applying ladder operator over and
5:03:37
over and over and you'll generate an infinite number of solutions with higher and higher and higher
5:03:43
energies we can also apply the ladder operator a minus
5:03:48
hat and you'll get something i'll call psi sub n minus 1
5:03:54
with slightly lower energy the energy has been lowered by an amount h bar omega
5:04:00
you can apply the ladder operator a minus hat as many times as you want of course as well and
5:04:06
you'll get psi sub n minus two or psi sub n minus three or size of n minus four psi sub n minus five every
5:04:12
time you apply the lab the lowering operator the ladder operator a minus hat you get another solution
5:04:18
with lower and lower and lower energy but we know if we have a wave function
5:04:24
with very very low energy it's going to behave very strangely
5:04:30
if your potential for instance is your harmonic oscillator potential it looks like this and your energy e
5:04:37
is below your potential v of x then if i start my wavefunction say
5:04:43
anywhere really let's start it here the fact that the energy is below the
5:04:49
potential for the entire domain of the potential means that
5:04:55
over the entire domain of the wave function the wave function is going to be curving away from the axis
5:05:01
the wave function is going to be blowing up that's a problem i cannot have solutions with arbitrarily
5:05:07
low energy what that means
5:05:14
cannot have solutions
5:05:20
with very low energy what that means
5:05:25
is that if i apply this lowering operator over and over and over again sooner or later i have to get something that i can no longer apply the latter
5:05:32
the lowering operator to something will no longer give me a meaningful solution
5:05:38
and it turns out the best way of thinking about that is there is some wave function such that a
5:05:45
minus acting on that wave function is equal to zero if we have a state like this this will
5:05:51
be our lowest energy state and i'll call it psi sub zero
5:05:58
this is a necessary condition for getting a normalizable wave function
5:06:03
if we had if we did not have this condition we'd be able to keep applying the lowering operator and we would sooner or later get solutions that were
5:06:10
not allowed that's a problem so let's figure out what this
5:06:17
actually implies we know what the lowering operator is we know
5:06:22
what zero is we have to be able to solve this this is going to be an ordinary differential equation just given by the
5:06:28
definition of the ladder operator 1 over 2 m h bar omega and the square root
5:06:36
times the momentum operator h bar d by dx
5:06:42
plus m omega x acting on psi sub zero
5:06:47
is equal to zero
5:06:53
this we can solve this is a relatively easy ordinary differential equation to solve in fact
5:06:59
because it's actually separable if you mess around with the constants
5:07:05
you can convert this into the differential equation d psi dx
5:07:10
is equal to minus m omega over h bar x psi
5:07:16
and these are now size zeros sorry this
5:07:21
can be directly integrated i can rewrite this as the psi over psi
5:07:27
is equal to minus m omega over h bar x dx
5:07:33
and if i do this integral integrating both sides of this equation what you end up with after you simplify
5:07:40
is that psi sub 0 is equal to e to the minus
5:07:46
m omega over two h bar x
5:07:51
squared e to the minus x squared for our ground state for our lowest
5:07:57
energy psi sub zero for our lowest energy solution there's a normalization constant here
5:08:03
and i'll save you the trouble of calculating the normalization constant out it's m omega over pi
5:08:09
h bar to the one fourth power so this is our ground
5:08:14
state now it's off to the races by consideration of the hamiltonian and
5:08:21
attempting to factor it and defining ladder operators and exploring the consequences of these ladder operators
5:08:28
in particular that we ended up with any single solution giving us an infinite number of solutions
5:08:34
by repeatedly applying a plus and a minus the necessity of a normalizable wave
5:08:40
function the necessity of having a lowest energy state meant
5:08:45
that we got an equation that was simple enough that we could solve it with just simple ordinary differential
5:08:50
equations now there's really no such thing as a simple ordinary differential equation but this was a lot easier to solve than
5:08:56
some ordinary differential equations what that ended up giving us in the end
5:09:02
was psi 0 our lowest energy state we can then apply the raising operator a
5:09:07
plus over and over and over again to construct an infinite number of states
5:09:13
to summarize here's a slide with all of the definitions the raising and lowering operators the ladder operators a plus and a minus
5:09:21
the expressions that you get from simplifying the hamiltonian in terms of the latter operators
5:09:28
i want to highlight these two expressions because i have not completely derived
5:09:35
them i have argued that the latter operator a plus applied to some wave function psi sub n gives you psi sub n
5:09:41
plus one but i haven't told you anything about the normalization you could apply this operator over and
5:09:47
over again and re-normalize all of the wave functions you get as a result but it turns out there's a pattern to them
5:09:52
and that pattern is that what you get by applying the latter operator a plus to psi n is not psi n plus one but psi n plus one
5:09:59
times this square root of n plus one likewise for the lowering operator
5:10:04
there's a nice explanation in the textbook of how you can use still more cleverness to derive what these normalization multiple multiplicative
5:10:12
factors are our ground state we got from applying the lowering operator to some hypothetical wavefunction which when we
5:10:18
solve it we ended up with this our psi sub 0 our lowest energy wave
5:10:23
function putting all this together you can come up with an expression for
5:10:29
the nth wave function psi sub n in terms of psi sub 0. you have to apply a plus
5:10:37
n times this superscript n here means to apply a plus m times for instance a plus
5:10:43
hat cubed would be a plus a plus a plus all acting on say if there's a sign here
5:10:50
all acting on the psi just one after the other and
5:10:56
if you calculate the energies that we get you know
5:11:02
applying the hamiltonian to our lowest energy wave function and then knowing that the raising the operator a plus
5:11:10
gives you a new solution with an energy that's increased by the amount h bar omega you end up with the energies
5:11:16
so we actually know everything about the solutions now we know the lowest energy solution we have a procedure for
5:11:22
calculating higher energy solutions and we know the energies of all of these solutions
5:11:28
so that's wonderfully good to give an example of how these things are actually used let's calculate psi one
5:11:35
we know psi one oops i'm black a little easier to read psi one
5:11:41
is going to be equal to a plus acting on psi zero and there's that normalization constant the square
5:11:48
root of n plus one except in this case n is zero so this is just going to be one
5:11:55
if i substitute in the definition of the operator a plus that's one over that
5:12:00
square root of two h bar m omega minus i p hat
5:12:07
plus m omega x
5:12:14
where p hat now is minus i h bar d by dx
5:12:22
this is my raising operator that's all acting on psi sub zero we know psi sub zero given in normalized form
5:12:29
m omega over pi h bar to the one fourth power e to the minus
5:12:35
m omega over two h bar x squared
5:12:41
we just have to evaluate this taking derivatives of this exponential
5:12:46
and multiplying it by x so let's continue with that
5:12:52
moving our normalization constant out front m omega over pi h bar to the 1 4 power
5:12:59
over this square root factor 2 h bar m omega
5:13:05
simplifying this expression out we end up with minus h bar d by dx
5:13:11
plus m omega x all acting on
5:13:16
e to the minus m omega over 2 h bar x squared
5:13:22
now this term with the m omega x that's going to be easy the derivative here is going to be
5:13:28
relatively straightforward as well and what we end up with is the constants
5:13:34
we had out front and taking the derivative of an x of an
5:13:40
exponential we're just going to get the exponential back so we're going to have an h bar e to the minus m omega over 2 h
5:13:47
bar x squared times the inner derivative the derivative of what's in the exponent
5:13:53
itself which is minus 2 x
5:13:58
sorry let me actually write this out minus m omega over 2 h bar times 2x
5:14:08
that's okay the minus sign here and the minus sign i had out front
5:14:14
will end up cancelling out i can simplify i can cancel out my twos i can cancel out an h bar
5:14:22
that's all i'm going to do with that term for now the other term is easy m omega x e to
5:14:28
the minus m omega over 2 h bar x squared
5:14:34
so that's our result we have an e to the m minus m omega et cetera over x squared in both of these
5:14:41
terms so i'm going to pull that out to the right and if i pull my constants out
5:14:48
to the left i have an m omega and an m omega in both of these terms so i can factor that out
5:14:54
and what you end up with at the end after all is said and done
5:14:59
the only skip step i'm skipping now is to simplify the constants what you end up with is m omega over pi
5:15:07
h bar to the one fourth power there's not much we can do about that square root of two m omega over h bar
5:15:15
x e to the minus m omega over 2 h bar
5:15:21
x squared both of these terms had
5:15:27
x and x in them so these terms just add up and this is what we end up with at
5:15:33
the end this is your expression for psi one
5:15:39
the algebra here gets a little bit complicated but fundamentally what we're doing is calculus taking derivatives multiplying
5:15:46
manipulating functions applying the chain rule and turning the crank more or less
5:15:54
the formula we started with here does give us machinery that we can use to calculate
5:16:00
any wave function that we might want as a solution to the time independent schrodinger equation for the quantum
5:16:06
harmonic oscillator to check your understanding here is an
5:16:11
operator algebra problem given that x hat is the position operator and t hat is the kinetic energy
5:16:18
operator essentially p squared over 2m calculate the commutator of x and t
5:16:25
that's just defined as this the one tip i have for you here is to be
5:16:31
sure to include a test function when you expand out these terms
5:16:36
and when you take second derivatives do it as a sequence of two steps don't just
5:16:41
try and take the second derivative twice in one step you may have to apply the product rule
Quantum harmonic oscillators via power series
5:16:49
we've heard about the solution to the harmonic oscillator time independent schrodinger equation by cleverness with
5:16:54
ladder operators this is a different the differential equation we have to work with is something that can be solved by other
5:17:00
techniques in particular it can be solved by power series power series is a common solution
5:17:07
technique for ordinary differential equations so it's useful to see how it applies to the time independent
5:17:12
schrodinger equation the equation we have to solve is this essentially
5:17:18
h operator psi equals e psi where we're now only talking about psi as a function of x
5:17:26
we have a second partial derivative with respect to x that comes from the kinetic energy part of the hamiltonian operator
5:17:32
and we have a potential energy part here where the potential function we're now working with v of x is the potential in
5:17:39
a harmonic oscillator one half m omega squared x squared basically proportional to the square of the displacement of a
5:17:45
particle from some equilibrium position often the first step in solving an
5:17:51
ordinary differential equation like this is to make some change of variables to simplify the structure of the equation
5:17:58
basically what we're looking to do is get rid of some of these constants and it turns out the change of variables
5:18:03
that we want to use here and you can determine this with a little bit of trial and error knowing how change of variables works
5:18:09
is we want to instead of x we want to use x is the square root of h bar over m omega times some new coordinate c
5:18:18
now what happens when we substitute in new coordinates here well we have to worry about psi of x here and here and
5:18:25
here psi is going to have to in some sense change a little bit in order to be represented as a function of c instead
5:18:30
of x we also very clearly have an x here and we have to worry about the second
5:18:35
partial derivative with respect to x so let's work through this step by step
5:18:41
and you'll see how how these substitutions can be made
5:18:46
first of all we can pretty easily handle these psi as a function of x because we know what x is x is the square root of h
5:18:52
bar over m omega times c so we'll have minus h bar squared over two m
5:18:58
don't have to worry about the constants second derivative of psi now square root of h bar over 2 over m omega
5:19:07
c is the argument for psi but we're still second differentiating with respect to x
5:19:14
dx squared pardon me
5:19:20
plus one half m omega squared now substituting in for x this is relatively
5:19:26
easy we're going to get this squared h bar over m omega
5:19:31
c squared times psi and the argument of size again going to be this root h bar over m omega c
5:19:40
equals e times the psi where again the argument of psi is this function of c
5:19:48
you can see there's going to be some cancellation here i can get rid of some m's and some omegas but i'll leave that until later the only difficult term to
5:19:54
deal with here is the second partial derivative with respect to x of psi which is now a function of c
5:19:59
now when you're taking the derivative of something with respect to a function of something else you have to
5:20:04
use the product rule sorry not the product rule the chain rule so i'm going to apply the chain
5:20:10
rule to this derivative term and i'm going to split it up into two steps
5:20:16
two first derivatives instead of one second derivative just to see how each of those steps applies
5:20:21
so first of all minus h bar squared over 2m times
5:20:27
the derivative with respect to x of the derivative with respect to x of psi of c now i can take the derivative of
5:20:34
psi with respect to c that i know how to do that's just d psi d c because psi is a function of c
5:20:39
but in order to turn this into a partial derivative with respect to x i have to multiply by the derivative of c
5:20:45
with respect to x so this is the chain rule at work here
5:20:51
and i know how to take the derivative of c with respect to x because i know c is a function of x
5:20:57
this is just going to give me square root of m omega over h bar
5:21:04
what i get if i solve for c and then just differentiate with respect to x
5:21:10
this can then be pulled out front it's a constant doesn't contribute anything
5:21:16
minus h barbs i want to be in orange minus h bar squared over 2m
5:21:22
times our constant root m omega over h bar times
5:21:27
partial derivative again with respect to x but now i'm taking the partial derivative of the partial derivative of
5:21:34
psi with respect to c so again i have to apply the chain rule what i'm going to get differentiating
5:21:40
psi with respect to c is the second derivative of psi with respect to c now
5:21:46
times again a partial derivative of c with respect to x you can do this all in one step
5:21:53
if you know that the partial derivative of c with respect to x is simple if the partial derivative of c with respect to
5:22:00
x had some problems in it some some dependents you would have two separate functions
5:22:05
here you wouldn't be able to factor it out as a constant and you'd have to apply the product rule to this term
5:22:11
so be careful when you're doing this don't just assume that you can take a second partial derivative
5:22:16
with the chain rule in one step but the second step here again partial
5:22:21
derivative of c with respect to 2x gives me the square root of m omega over h bar
5:22:28
which as a constant i can pull out front and combine what i'm left with for this term then is
5:22:33
minus h bar squared over 2m times m omega over h bar again giving me some
5:22:39
nice cancellations times the second derivative of psi with respect to c
5:22:44
so this converts my derivative with respect to x into a derivative with respect to c
5:22:50
i've converted my x into c and all of my other x's into c is just by changing the arguments of psi
5:22:56
so the overall equation i get now minus h bar squared over 2m m omega over h bar
5:23:03
second partial of psi with respect to c plus one half m omega squared h bar over
5:23:11
m omega c squared psi equals e sine
5:23:19
this is good because we can do some cancellations we can for instance cancel one of the omegas here
5:23:24
and we can cancel the m we can also cancel an m here and one of
5:23:29
the h bars what's nice about this is i have
5:23:35
h bar omega over 2 here and h bar omega over 2 here so i have the
5:23:43
same constant and i'm going to move both of these constants factor them out move them over with the e to lump all of my constants
5:23:49
together i'm also going to change the ordering of the terms to get my two size together
5:23:55
and mess with the signs a little bit but the final equation you get is the second derivative of psi
5:24:01
with respect to c is equal to c squared minus some constant k
5:24:08
psi where k is what we got when we aggregated all these constants together it's
5:24:14
an equal sign there k is equal to 2 e over h bar omega
5:24:20
so this is a differential equation that's substantially simpler than the differential equation we had here
5:24:26
just by rearranging constants we haven't actually changed the structure of the solution any
5:24:33
this differential equation isn't something that we want to just go ahead and try and solve with power series though and you'll see why in a moment
5:24:40
solutions that are most easily represented by power series are solutions that are only interesting near
5:24:45
the origin and this equation tends to be difficult to represent with power series because
5:24:52
of what happens for value for large values of c so let's look for something called an
5:24:57
asymptotic solution let's look for a solution for large c
5:25:04
c much much greater than one what happens when c is much much greater than one
5:25:09
well if c is much much greater than one i don't care about k here it's going to be about equal to c squared z squared
5:25:16
minus k is about c squared that means the actual differential equation we have to solve is
5:25:22
second derivative of psi with respect to c is c squared psi
5:25:28
oh and i've unintentionally changed notation here this is a derivative of psi not a partial derivative of psi that
5:25:35
doesn't really matter the partial derivative and the total derivative are the same because psi now
5:25:41
is only a function of x likewise i should also probably write capital psi here instead of lowercase
5:25:47
psi that's just an error apologies
5:25:52
this approximate equation has solutions in the case of
5:25:58
an asymptotic solution we don't really care about the exact solution an approximate solution is good enough
5:26:04
if we can still use this approximation in our solution so our approximate solution and you can check this
5:26:11
is that the wave function is approximately equal to a times e to the minus z squared over 2
5:26:18
plus b e to the c squared over 2. rewrite that or look like make it more
5:26:25
look like a c so this equation is an approximate solution to this equation
5:26:32
and you can see that by taking the second partial derivative of psi i'll just look at this term for instance
5:26:39
the second partial derivative of this term d squared dx or d c squared
5:26:47
of e to the minus c squared over 2
5:26:53
is and you can plug this into whatever computational algebra tool you want c squared minus 2 times e to the minus c
5:27:01
squared over 2. so this again approximately for large values of c
5:27:08
is going to be about equal to c squared so second derivative of this effectively pulled down x c squared and gave us our
5:27:14
function back and that's what our approximate differential equation is now if you had a minus sign in front of
5:27:20
the c and the exponent here you'd end up with much the same sort of expression so you can see this
5:27:25
is effectively an approximate solution to our approximate differential equation
5:27:30
this is useful in a couple of ways first of all there will be large values of c
5:27:35
unlike the case of the infinite square well there's no sound reason for believing that the wave function will go
5:27:40
to zero for large values of c it's certainly not required by the laws of physics
5:27:46
it is however required by the laws of mathematics in order to have a normalizable wave
5:27:51
function this asymptotic behavior can't have any of this in it
5:27:58
so if we want our wave function to be normalizable
5:28:08
then b must equal 0. that's a requirement
5:28:14
what that tells us then if we have something that's going to be a solution to the time independent schrodinger equation its asymptotic
5:28:21
behavior will be given by this so psi is approximately equal for large c to some constant e to the minus c
5:28:29
squared over two that's an approximate solution this is the story all about how the
Free particles and Schrodinger equation
5:28:35
schrodinger equation applies to the free particle what do we mean by a free particle
5:28:41
imagine an electron for instance floating in the vacuum of space it never encounters anything
5:28:47
it never runs into anything how that enters the schrodinger equation is that there is effectively no
5:28:52
potential anywhere so the time independent schrodinger equation we're back to one dimension now so don't think about a particle floating
5:28:58
around in the vacuum of three-dimensional space it's floating around in the vacuum of one-dimensional space
5:29:04
the left-hand side of our time-independent schrodinger equation is the hamiltonian operator applied to the wave function
5:29:09
this is in some sense the total energy which breaks down into a kinetic energy component here with the momentum of the
5:29:17
particle squared divided by twice the mass and a potential energy part here where v
5:29:22
of x is the potential energy that the particle would have to have to be found at a particular location
5:29:29
in the context of free particle where there is no potential what that means is that v of x
5:29:35
is equal to zero everywhere that means we can just cross out this
5:29:41
term entirely we don't have to worry about it what we're left with then for our time independent schrodinger equation
5:29:48
is minus h bar squared over 2m times the second partial derivative of psi with
5:29:53
respect to x equal to e psi now we have some constants here and we
5:29:59
have a constant here so let's lump them all together and i'm going to shift the signs around a little bit as well so
5:30:04
that we've what we've got is the second derivative of psi with respect to x is equal to minus
5:30:10
2 m e over h bar squared times the wave function
5:30:16
so just lumped all our constants together and multiplied through by a minus sign now you notice the second derivative
5:30:21
here of the wave function giving you the wave function back the fact that we're taking a second derivative suggests that
5:30:26
the constant here perhaps is squared so what i'm actually going to write this
5:30:31
as is the second derivative of psi with respect to x is equal to minus some constant k
5:30:38
squared times the wave function where k our constant is the square root
5:30:44
of two m e over planck's constant
5:30:50
so this is the differential equation and we ought to be able to solve this this is relatively simple compared to the
5:30:57
structure of the differential equations we got from the harmonic oscillator
5:31:02
so how do we solve this well what we have second partial of psi with respect to x is minus some constant squared
5:31:10
times the wave function taking the second derivative gives you a constant squared that immediately
5:31:15
suggests we look for exponential solutions and it turns out the general solution to this equation is some constant times e
5:31:22
to the minus k sorry i k x plus
5:31:28
b e to the plus i k x if i take the second derivative of this
5:31:33
exponential term i'll get a minus i k squared minus i k
5:31:38
squared which you know is just minus k squared applying the rules of complex numbers
5:31:44
which is what we get here so when i take the second derivative of this term i'll end up with minus k squared times this
5:31:49
term and i get the same sort of thing here if i take plus i k squared from the second
5:31:55
derivative that again gives me a minus k squared so we're okay this is our general solution
5:32:01
when we include time in this since you know this is a solution to the time independent schrodinger equation it's
5:32:07
going to have time dependence given by the time part the time equation that we got when we did separation of variables
5:32:14
what you end up with is psi of x and time now is equal to a e to the minus i
5:32:20
k x times e to the i energy t
5:32:26
over h bar plus b times e to the i k x
5:32:35
e to the i energy time over h bar and i've left off my minus signs here in the energy
5:32:41
dependence just to conventional to include minus signs there
5:32:46
we can rewrite this a little bit as a e to the i k
5:32:53
now what i'm doing here is substituting in the definition of k which if you remember was the square root of 2 m e
5:33:02
all over h bar expressing energy in terms of this k
5:33:08
and when i do that what i end up with is this term ends up looking like h bar k squared
5:33:16
over 2m substituting that in here is what we get from from from
5:33:22
manipulating our constants here if i do that manipulation the first term here instead of having
5:33:27
this product of two exponentials i'm going to write it as a sum in the exponent x minus
5:33:33
h bar k over two m t plus b times something looks very
5:33:39
similar e to the minus i k x plus
5:33:45
h bar k over 2 m t so these are our general solutions to
5:33:51
the full schrodinger equation our full wave function as a function of both position and time
5:33:56
and these solutions are traveling waves you can think about this as a traveling
5:34:02
wave in the context of looking at this as a complex number if i look at e to the ikx for instance
5:34:11
as a function of x you know what that does in the complex plane it just rotates around in the
5:34:16
complex plane if i look at this as e to the i kx
5:34:21
let me redo that a little mine sorry i k
5:34:27
times x minus h bar k over 2 m t and i treat this as a function of time
5:34:34
again we just get rotation in the complex plane we get rotation in the other i promised in the last lecture that the
Free particles wave packets and stationary states
5:34:40
solutions we got to the time independent schrodinger equation for the free particle though they are not themselves
5:34:46
normalizable and therefore cannot represent physically realizable states
5:34:51
could be used to construct physically realizable states what that means is that we can take
5:34:57
those solutions which themselves are not real and can add them up in a way that we can
5:35:02
make something that is real this is a little subtle we're constructing something called a wave
5:35:08
packet and basically what that amounts to is adding up a bunch of infinities and
5:35:13
getting something finite taking these traveling wave solutions to the time independent schrodinger
5:35:19
equation for the free particle which extend from minus infinity to infinity in the spatial domain
5:35:24
and from minus infinity to infinity in the temporal domain and summing them up somehow to get something that is localized in the
5:35:31
spatial domain what that means is that we're making a wave packet
5:35:37
a wave packet the features that work here that we care about is that it's going to be zero
5:35:43
for say large negative values of x zero for large positive values of x
5:35:49
and only non-zero over some domain what it might look like is well zero
5:35:59
some wave activity over a relatively limited region and then going back to zero
5:36:06
we will see wave packets that look like this later on i'll give a more concrete example and show some animations
5:36:12
but for now let's think about the math how would we go about constructing something like this
5:36:17
what we did in the case of the particle in a box the infinite square well potential
5:36:24
was when we solved the schrodinger equation we got solutions if our potential looks like this going
5:36:29
to infinity at regions outside of a box our solutions looked like this we got
5:36:35
sinusoids with an integer number of half wavelengths
5:36:41
fitting in our box
5:36:46
that was nice because it allowed us to construct our overall solution to the schrodinger equation psi of x and t
5:36:54
as an infinite sum of these stationary state wave functions the
5:36:59
integer number of half wavelengths fitting in the box plus the essentially trivial time dependence that you get
5:37:05
from the time equation when you do separation of variables with the general schrodinger equation
5:37:11
this isn't going to work for the case of the free particle for a couple of reasons first of all
5:37:17
instead of having a discrete sum over you know states which have an index n for
5:37:24
instance this is our psi sub n where n goes from one to infinity
5:37:30
we now have wave functions psi that are continuous we did not have quantized
5:37:36
states so our stationary states now are going to have to look like our traveling waves
5:37:42
they're going to have to look like e to the i k and then x minus
5:37:47
uh where did it go h bar k over 2 m t
5:37:54
this was our traveling wave solution from the last lecture so instead of having our discrete set of
5:38:01
states indexed by n we have our continuous set
5:38:06
where the parameter is k k is a completely free parameter not fixed to be an integer
5:38:13
the second reason our machinery for the particle in a box won't quite work is this coefficient c sub n
5:38:20
c sub n is also going to have to somehow become a function of k
5:38:26
okay now being unrestricted we can't just treat it as a set of discrete entities we have to have some function
5:38:32
and that function is conventionally written as phi of k
5:38:37
and finally this sum out front again we can't do a sum if we have a
5:38:43
continuous set of functions that we're working with that we want to add up we have to do an
5:38:49
integral the integral now is going to be an integral over k
5:38:54
so our sum over n became an integral over k our coefficient subscript n became a function of k
5:39:01
and our discrete set of functions psi sub n became these traveling wave solutions with the parameter k in them
5:39:09
our integral decay goes over all the possible values of k from minus infinity to infinity
5:39:15
and this is what the expression is overall going to look like we have an integral we have this continuous function and we have our
5:39:23
traveling wave states the main problem with this expression
5:39:29
is this guy how do we know
5:39:34
how do we find phi of k phi of k is a general function
5:39:42
what we had done to find the analog of this the analog of this was that c sub n
5:39:47
in the case of the particle in a box what we did for the case of the particle in a box was use
5:39:53
fourier's trick to collapse the sum instead of a sum now we have an integral
5:39:59
and it's not immediately clear from looking at this what it means for an integral to collapse
5:40:04
we'll see what that means in a second but first of all let's go back to what we did in the case of the particle in a
5:40:10
box and spell out some of the details so that we can make an analogy on the left hand side here now we have the results
5:40:16
for the particle in a box whereas on the right hand side we have the results as i have
5:40:22
outlined what they might look like for the free particle so the first thing we did
5:40:30
for our particle in a box was to express the initial conditions as an infinite sum of the time t equals zero form of
5:40:37
our stationary state wave functions the second thing we did in manipulating
5:40:43
this expression to attempt to find a formula for the c sub n was to multiply on the left by a
5:40:50
particular stationary state wave function not n m
5:40:56
so we multiplied by root 2 over a sine
5:41:02
m pi over a x psi of x 0. this is now looking at the
5:41:08
left hand side so we multiplied by this and we integrated from zero to a
5:41:15
this integral is taken dx it's important to note now that this is
5:41:21
not the wave function psi this is the complex conjugate of the wave function psi and we'll come back to that in a moment
5:41:28
this integral this is our left hand side if we do the same thing to the right
5:41:33
hand side you end up with an integral dx that you can push inside the sum
5:41:38
you can pull out some constants and all you're left with then the only x dependence comes from the sine function
5:41:44
here and the sine function you're multiplying in so we end up ended up with the sum from
5:41:49
n goes n equals 1 to infinity of c sub n our two root 2 over a factors from our
5:41:56
two wave functions multiply together just give us 2 over a
5:42:02
and what we're left with inside the integral is
5:42:08
sine of m pi over ax sine of n pi
5:42:14
over a x dx so that was our expression
5:42:20
and the nice feature about this is that the sine functions had an orthogonality condition on them
5:42:27
that allowed us to take this integral from 0 to a and express it as delta m
5:42:34
n the sine functions if m is not equal to n will integrate to zero over this integral
5:42:40
interval and if m is equal to n you just end up with one
5:42:45
i should be including this factor out front in the expression for the orthogonality
5:42:51
what that means is that the sum collapses the only remaining term is the term from
5:42:58
cm so our right hand side just becomes cm
5:43:05
this gave us our formula for cm being equal to the integral from 0 to a
5:43:11
of essentially root 2 over a sine m pi
5:43:17
over a x times our initial conditions psi of x zero
5:43:23
this was a very brief overview of what we did back when we were talking about the particle in a box
5:43:30
now continuing this analogy into our free particle case
5:43:37
again the first thing we're going to do is left multiply by the complex conjugate of the wave function
5:43:43
now the wave functions that we're working with now are stationary state solutions to the time independent schrodinger equation for the free
5:43:49
particle and what those look like if i evaluate them at t equals 0
5:43:56
is e to the minus i k
5:44:02
x now i'm leaving off normalization constants because i don't know what they are at this point
5:44:08
but while i have a k in this integral i shouldn't use k here
5:44:14
this is the same as saying i have an n in this sum so i shouldn't use n in the
5:44:20
function that i'm multiplying through things will just get confusing so i'm going to call this k prime
5:44:27
so i've left multiplied by k prime i have my wave function my initial conditions
5:44:33
and again i'm integrating now i'm integrating from minus infinity to infinity
5:44:39
and i'm integrating dx this is what i get for the left hand side just following by analogy from what
5:44:45
we did for particle in a box the right hand side in this case
5:44:51
now instead of having a sum over n i have an integral over k
5:44:56
what i'm multiplying by from the left is again the e to the minus i k prime x but this
5:45:03
integral that i'm doing that's an integral dx so i can exchange the order of integration
5:45:10
by k and integration by x so i'm going to write this right hand side now a little differently we have
5:45:15
the integral of minus infinity to infinity dk
5:45:22
then we have phi of k which is not a function of x so i can pull it out of my integral
5:45:28
over x same as i could pull my c sub n out of this integral dx
5:45:36
sorry phi of k not v of x
5:45:42
what i'm left with then is the integral from minus infinity to infinity dx
5:45:49
e to the minus i k prime x e to the i k x
5:45:58
now in order for this term to be meaningfully or to or in order for this
5:46:05
integral to collapse like the sum collapsed here we have to have some sort of orthogonality
5:46:10
condition the orthogonality condition for the sine functions from 0 to a was fairly
5:46:16
straightforward the orthogonality condition that applies here for this where we are integrating over an
5:46:21
infinite domain of something with a continuous parameter k prime and k are continuous parameters that can take on
5:46:27
any value is not a simple chronic or delta it's a little different
5:46:33
but it looks very much the same what you end up with here is called a dirac delta function
5:46:43
and we will meet these dirac delta functions in more detail later
5:46:49
if you're interested there is a video lecture posted on the dirac delta function and what its properties are
5:46:54
but for our purposes here this expression evaluates to a dirac delta function
5:47:01
a dirac delta function is defined essentially as an infinitely narrow distribution
5:47:08
if you treat this as a distribution that only is non-zero at a particular value the delta function
5:47:15
by default is defined to be non-zero only for its argument equal to 0. this is effectively a distribution that
5:47:22
only has non-zero values only has support for k equal to k prime
5:47:29
if you treat this as a distribution and you examine the expression integral from minus infinity to infinity d k of
5:47:36
phi of k delta of k minus k prime
5:47:42
if this is a distribution we're integrating a distribution times a function this is the expected value of phi of k
5:47:50
subject to the distribution given by the delta function the delta function acting like an infinitely narrow
5:47:56
distribution then simply pulls out the value that phi
5:48:02
of k has when k equals k prime since this is infinitely narrow phi of k is effectively a constant over
5:48:09
the non-zero domain of the delta function
5:48:14
so it's just effectively averaging a constant over this domain
5:48:19
so this a whole integral here is equal to phi
5:48:25
of k prime that's what it means for an integral to
5:48:30
collapse and like i said if you're not entirely clear on how the delta function works there's another video lecture on
5:48:37
how to go about or how to understand what the delta function can do for you
5:48:42
for now notice that we can re-express this phi of k prime then
5:48:49
in terms of our left-hand side phi of k prime is equal to the integral from minus
5:48:55
infinity to infinity of e to the minus i k prime x
5:49:00
psi of x 0 integral dx
5:49:06
this completely determines psi sorry phi of k
5:49:14
this is the real genius behind what's called fourier analysis
5:49:22
what we were talking about in the case of the particle in a box was really fourier series and now we're talking about fourier
5:49:28
analysis the way these the math behind this is usually defined is in terms of something
5:49:33
called the fourier transform the top two equations here are essentially definitions of the fourier
5:49:39
transform we have some function of x this is like our wave function as a
5:49:44
function of time and it's being expressed as an integral of some function of k
5:49:50
multiplied by e to the i kx integral dk
5:49:56
this function f capital f of k can be determined by
5:50:01
essentially what we did in the previous slide an integral from minus infinity to infinity dx
5:50:08
of the function lowercase f of x times e to the minus i kx
5:50:14
the 1 over root two pi factors here are customary some authors use them some authors define them slightly differently
5:50:20
it depends on the specific definition of the fourier transform that you're using but you can see the nice symmetry
5:50:26
between these two equations you have your 1 over root 2 pi in both equations you have an integral from minus infinity
5:50:32
to infinity in both equations you have e to the ikx here positive and e to the
5:50:38
minus ikx here negative that's the only difference then you have a function of k integral decay function of x integral dx
5:50:47
up to labeling x and k differently the only difference between these two equations is the sign in the exponent
5:50:56
there's a lot of really nice math that comes from using fourier transforms
5:51:01
um just to give a very brief example if you're interested in processing astronomical images for example or any
5:51:07
images really treating the image as a function of this k parameter which is a spatial frequency
5:51:15
parameter instead of treating the image as a function of x as a function of which pixel you're looking at you can do
5:51:20
some very powerful analysis to identify features for instance high
5:51:26
spatial frequency features versus low spatial frequency features smoothly varying backgrounds versus the
5:51:32
boundaries between objects where the image varies rapidly will have different behavior when expressed in terms of this
5:51:41
function of the spatial frequency from the perspective of quantum mechanics what we're interested in is
5:51:46
how to express our wave function as a function of position and time well
5:51:52
using the fourier transform definition here we can find this phi of k
5:51:58
by the same sort of same sort of equation phi of k is determined by an integral dx of our initial conditions
5:52:04
times a complex exponential knowing what fee of k is we can then determine what phi of x and t is
5:52:11
so again our initial conditions determine our constant multiples essentially of
5:52:17
our stationary states these complex exponentials which then gives us
5:52:24
our overall wave function and how it behaves
5:52:30
to check your understanding here is a simple example problem that requires you to apply the formulas on
5:52:35
the previous page to go from a particular initial condition in this case it's a constant
5:52:42
our initial wave function looks something like this 0
5:52:47
everywhere except for a region between minus a and a
5:52:53
your task find the fee of k that goes with this
5:52:58
particular function that's about it
5:53:04
but before we finish talking about how to superpose these solutions i want to look at the solutions themselves in a
5:53:10
little more detail let's talk about the wave velocity in particular
5:53:16
this is our traveling wave solution and we can figure out what its velocity is
5:53:22
by looking at this argument which direction is this wave going
5:53:28
well if we look at a particular point on this spiral
5:53:34
on this e to the ikx as time evolves we can figure out where that point on the
5:53:39
spiral is by setting this argument equals to a constant since i don't really care about what that constant is i'm just going to set
5:53:45
that equal to zero so let's say kx minus h bar k squared
5:53:50
over two m t is equal to zero
5:53:55
if i continue along these lines it's clear that in this case if t increases
5:54:04
this part of the function of this expression is getting more negative this part expression of the expression has to
5:54:09
get more positive so that means x has to increase as well so as t increases x increases that means
5:54:16
this wave is moving to the right
5:54:25
the next question i can ask is how fast how fast is it moving
5:54:33
and if you look at this again as setting this expression equal to zero i can solve this and say x is equal to h bar k
5:54:41
over 2m t and in this case the velocity
5:54:47
is pretty clear we have this constant
5:54:53
x equals some constant times time position equals something times time this
5:54:59
is our velocity what this actually is in terms of the
5:55:05
energy of the particle requires knowing what the definition of k is so we have h bar
5:55:11
over 2 m and the definition of our k was root 2 m e
5:55:18
over h bar so our h bars cancel out and if we finish this expression moving
5:55:24
the 2m effectively under the square root we get the square root of e over 2m t
5:55:31
so the velocity we get here is square root of e over 2m
5:55:41
now classically what we get
5:55:48
we have a particle moving at some velocity and has some energy we know the relationship between those it's the
5:55:53
kinetic energy one-half mv squared gives me e and if i solve this i get v squared
5:56:01
equals two e over m or v equals root 2e
5:56:08
over m these expressions are not equal to each other
5:56:13
that's a little strange the velocity that we got from quantum mechanics looking at how fast features
5:56:20
on this wave function move is not equal to the classical velocity
5:56:25
will this hold true regardless do quantum mechanical particles have a different propagation behavior that
5:56:32
doesn't really make a lot of sense this is actually not a problem because what we're measuring here is the
5:56:38
velocity of a feature on this wave it's not actually the velocity of a wave
5:56:43
packet and since wave packets are the only real states that we can get that we expect to
5:56:48
observe in the physical universe what we need to figure out is the wave packet velocity
5:56:56
in order to figure out the wave packet velocity consider this wave packet this is just a
5:57:03
sum of two wave two traveling waves with different k's which i've now
5:57:08
indexed k1 and k2 what i'd like you to do is think about expressing
5:57:14
k1 and k2 as if they were near each other so k1 is slightly less than k2 for
5:57:22
example or k1 is slightly greater than k2 under these circumstances it makes sense
5:57:28
to rewrite these things i'm going to define alpha as
5:57:33
k1 plus k2 over 2 the average
5:57:40
times x minus h bar k 1 squared plus k 2 squared
5:57:48
over 2 m t essentially the difference or sorry the
5:57:54
sum of the argument of this and the argument of this
5:57:59
i'm also going to define a parameter delta which is k1 minus k2 over 2x
5:58:07
minus h bar k1 squared minus k2 squared over 2m
5:58:13
t um
5:58:19
actually sorry i don't mean two m's here i mean four m's here
5:58:24
because i have a factor of 2 from the over 2m and i have a one half essentially from
5:58:30
the way i'm combining the two terms so given these definitions
5:58:35
you can express this as
5:58:43
not writing it there as e to the i alpha
5:58:48
plus delta plus e to the i alpha minus delta
5:58:55
so you see what i've done here i've just re-expressed the arguments here as sums and differences
5:59:01
this is getting into the idea behind sum and difference and product identities in trig functions except i'm doing this
5:59:08
with complex exponentials instead if i write this
5:59:14
function as alpha plus delta when i add alpha and delta for instance this first term gets me k1 plus k2 plus
5:59:22
k1 minus k2 the k2's drop out and end up with 2 k1 over 2 which is just k1 times
5:59:28
x just k1 x essentially what you want to get from
5:59:34
this if i express these exponentials in that way you can factor out
5:59:40
the delta the alpha part get an e to the i alpha times an e to
5:59:45
the i delta plus e to the minus i delta
5:59:51
if you're familiar with the complex exponential form of trig functions you can probably see where i'm going with
5:59:57
this this is going to end up equal to e to the i
6:00:02
alpha times cosine actually not just cosine 2 cosine
6:00:09
of delta what this looks like in the context of
6:00:14
our discussion of wave packets is if we have an axis there
6:00:20
we have this cosine factor and it's the cosine of delta if k1 and k2 are near
6:00:25
each other this will be a small number this will also be a relatively small number so
6:00:30
delta evolves much more slowly with space and time than alpha so if i was going to write if i was
6:00:37
going to draw this wave function i would have some slowly varying envelope
6:00:43
like this and superposed on top of that multiplied
6:00:50
by that slowly varying envelope is e to the i alpha which is the sum so if k1 is close to k2
6:00:56
this is going to evolve much more rapidly so my overall wave packet
6:01:02
is going to look something like this where you have zeros
6:01:09
and areas with large amplitude areas with small amplitude areas with large amplitude areas with small amplitude
6:01:17
as time evolves this wave packet will propagate
6:01:24
and if what we're interested in is the velocity with which the overall packet propagates
6:01:30
you can consider a point on delta not a point on alpha
6:01:37
if we're interested in the velocity with which these rapidly moving peaks rapidly oscillating peaks evolve then we would
6:01:44
look at alpha but since what we're interested in now is the wave packet we want to look at delta we want to look at the slowly varying envelope how quickly
6:01:50
the slowly varying envelope moves now i haven't actually constructed a
6:01:55
fully formed physically realizable wave packet here because i have this cosine term which again extends all the way
6:02:01
from minus infinity to infinity but hopefully conceptually you can think about this as a sort of rudimentary wave
6:02:08
packet the question is how fast does the rudimentary wave packet move
6:02:13
well if i look at delta and if i assume that k1 is near k2
6:02:21
we can see how that works out so what i'm looking at here is delta
6:02:26
is equal to zero say the same sort of argument that i was using to determine how fast a figure a feature on a single
6:02:32
wave moved setting this delta equal to a constant not caring what the constant was and setting it equal to 0.
6:02:39
what i get then is k1 minus k2 over 2
6:02:45
x being equal to h bar over 4m
6:02:51
and then k1 squared minus k2 squared i'm going to look at this as the difference of two squares which i can factor
6:02:58
k1 plus k2 times k1 minus k2
6:03:06
i can then cancel out this and this
6:03:12
and what i'm left with is just x
6:03:17
over 2 equals to h over 4 m
6:03:24
k1 plus k2 if i assume that k1 is about equal to k2
6:03:32
then i can pretend that this is some effective
6:03:38
average k k bar
6:03:44
if i write that out sorry this is 2 k bar twice k bar since i have k1 and k2
6:03:51
and they're added together i can then
6:03:57
look at this i have a 1 over 2 here a 1 over 4 here and a 2 here
6:04:02
what i end up with at the end is just going to be x equals h bar
6:04:08
over m times k bar this is different than the expression we
6:04:14
got before k bar now is going to be our average sort of our
6:04:19
average k h bar over m to copy that over and our k was root 2 m
6:04:26
e bar now for k bar instead of k bar i'll have e bar for my average
6:04:33
and then i have plox constant i can cancel out flux constant in the
6:04:38
denominator i can again push my mass into the square root here and what i'm left with then is
6:04:46
root 2e over m times time i forgot my times time here
6:04:52
all of these have a times time so x equals something times time
6:04:59
this is our velocity so here for the wave packet velocity
6:05:08
we get root 2e over m this is the classical velocity
6:05:17
so problem solved whereas the features on each individual
6:05:23
peak for instance in our wave function traveled at one velocity
6:05:28
the overall wave packet traveled at another velocity for the case of
6:05:34
this particular wave packet or wave packets in general the wave packet itself travels at the velocity you would
6:05:40
expect except i have to be clear here now let me rewrite this the velocity we get
6:05:46
for a wave packet now this is only approximate so i should write it as approximately
6:05:51
equals and it's not twice the energy
6:05:57
it's twice the average energy divided by mass in the square root
6:06:02
so this is not exactly the classical formula because now we don't necessarily have a single energy if we had a single
6:06:08
energy we would be stuck with one of those solutions to the time independent schrodinger equation which have definite
6:06:14
energy in the case of this part of this free particle those definite energy solutions extended
6:06:20
throughout all space and that was a problem so we don't actually have a definite energy so we'll have some
6:06:26
spread in energies here and if you have a large spread in energies you'll effectively get a large spread in
6:06:31
velocities and what starts off as a wave packet will not stay a wave packet very long it will propagate at different speeds
6:06:38
different parts of the wave packet will propagate faster than others but at any rate
6:06:44
what this actually looks like to make some some visuals here and i couldn't hope to draw this accurately
6:06:51
but if we have some wave packet at time t equals zero delta t two delta t and three delta t
6:06:57
it's going to propagate gradually you can see the disturbance these weight of this wave moving to the right
6:07:04
now i've drawn solid thick lines here behind it to designate the motion of the overall wave
6:07:10
packet the overall packet is moving at a speed more or less determined by the slope of
6:07:16
these thick black lines the thin gray lines identify features
6:07:21
for instance this peak becomes this peak becomes this peak becomes this peak
6:07:28
this peak is traveling at a more slow rate than the overall wave packet
6:07:33
and is essentially sort of falling off the back of the packet it's decreasing in amplitude as it goes
6:07:40
the slopes of these line are different lines are different meaning the features on the waves are propagating a different
6:07:46
speed than the overall wave packet this is actually a general feature of many waves it's not something we hear
6:07:52
about very often in everyday life because we never really think about whether there might be a difference or not plus most of the common waves that
6:07:59
we work with like sound waves for instance don't have this property but if you look closely for instance if you
6:08:05
drop a rock in a still pond the small scale ripples actually behave with this different
6:08:12
velocity in that case actually the features on the wave move faster than the overall
6:08:18
wave packet so in that case you could view this as sort of time reversed where the features
6:08:24
start at the back of the wave packet and propagate forwards but this is really the question of
6:08:32
what's called group velocity and the question of phase
6:08:39
velocity the phase velocity refers to the features in the wave whereas the group
6:08:45
velocity refers to the velocity of the wave packet this is not a wave mechanics course but
6:08:51
there are there's a lot of interesting math that can be done with this the group velocity and the phase velocity being different is one of the
6:08:57
one of the more interesting features of for instance propagation of electromagnetic waves and plasmas in
6:09:03
space so if you're interested in radio astronomy for instance you need to know about this in very high levels of detail
6:09:12
to give you a better feel for what this looks like here's an animation
6:09:17
what we're looking at now are the real and complex parts shown in red and blue respectively
6:09:24
of a hypothetical wave packet that might represent a solution to the schrodinger equation
6:09:29
it doesn't actually represent a solution to the schrodinger equation but this is the sort of behavior we're looking at if
6:09:34
i track a particular pulse say this one i'm moving my
6:09:39
hand to the right as i do so here
6:09:46
but i'm not moving my hand to the right nearly as fast as the overall wave packet is propagating
6:09:53
so the overall wave packet is propagating at effectively twice the speed of the
6:09:58
individual features on the wave so this is what uh wave propagation mate might actually look like for the
6:10:05
schrodinger equation you can construct wave packets like this if you add the time dependence then
6:10:11
you can determine how the wave prop wave packet will propagate how it will spread out how the individual wave features
6:10:18
will move and you'll know effectively everything you need to to check your understanding
6:10:24
here are a few true or false questions don't think that because they're true or
6:10:30
false they're easy think about these in detail
The Dirac delta function
6:13:44
we've already met the dirac delta function a couple of times in this course as examples
6:13:50
so it's good at this point since what we're going to be discussing next is the dirac delta function as a potential
6:13:56
to discuss the general properties of the dirac delta function how it works from the mathematical perspective
6:14:02
what i want you guys to think of when you think of the dirac delta function is the limit of a distribution
6:14:09
the gaussian distribution for example rho of x is given by 1 over the square root of 2
6:14:15
pi as a normalization sigma e to the minus say x squared over 2 sigma squared
6:14:22
the limit as sigma goes to 0 of this function
6:14:28
gives you something that is very much like the delta function this is not the only way to define the delta function
6:14:35
but if we start with for instance this purple curve here at
6:14:40
large sigma and this orange curve here at small sigma
6:14:48
as sigma gets smaller and smaller the distribution gets narrower and narrower and taller and taller as sigma gets
6:14:54
smaller for instance the dependence here in the exponent of e to the minus x squared gets faster and
6:15:01
faster since i'm effectively multiplying the x squared by a larger and larger number
6:15:06
and the normalization constant out front one over root two pi times sigma gets larger and larger as sigma gets smaller
6:15:12
and smaller so thinking about this as the limit in the limit we have a distribution that is
6:15:20
infinitely narrow and infinitely tall
6:15:26
it has absolutely no support for any values of x other than say x equals 0
6:15:31
here so this would be say delta of x as a
6:15:37
distribution you often see delta functions written as uh in terms of more conventional
6:15:43
function notation delta of x is equal to zero for x not equal to zero
6:15:49
and infinity for x equals to zero but this isn't a sufficiently accurate description
6:15:55
because it doesn't tell you this property that delta the delta function is the limit of a distribution it has
6:16:01
specified integral so you always have to add an extra condition here for something like for
6:16:06
instance something like the integral from minus infinity to infinity of delta of x dx is equal to one
6:16:12
that essentially sets the specific value of the infinity here such that the integral equals one
6:16:18
but thinking of it as the limit of a distribution is essentially the the actual definition of the delta function
6:16:26
knowing that the delta function acts like a distribution allows us to do things like calculate integrals with delta functions this is where delta
6:16:33
functions really shine if you have an integral of minus infinity to infinity of any function f
6:16:38
of x multiplied by the delta function if we think of this as a distribution
6:16:44
this is effectively the expectation of x of f x the expected value of f x
6:16:50
subject to this distribution given by the delta function now since the delta function has absolutely no support over any values of
6:16:58
x other than x equals zero essentially what this is telling you is
6:17:03
the expected value of f x where f where the only region that we care about is the area
6:17:08
very near zero so this just gives us f of zero
6:17:15
thinking about this in the context of a distribution if we had a distribution with some
6:17:21
very narrow width if this width gets extraordinarily
6:17:27
narrow then no matter what f of x does out here we don't care and
6:17:33
as the distribution becomes extraordinarily narrow we're just zeroing in on the behavior of x over
6:17:38
this region which makes f of x basically look like a constant and you know the expected value of a
6:17:44
constant like if i wrote this as the expected value of f of zero it wouldn't matter what the distribution
6:17:50
was it would just give you f of zero so this is the same sort of same sort of concept
6:17:55
the infinitely narrow distribution effectively just pulls out the value of f x at that
6:18:01
at that point so this is our first really useful formula with delta functions if we
6:18:07
integrate doesn't really matter what we integrate over minus infinity to infinity will work delta of x times any
6:18:13
function f of x integrating dx we just get f of zero
6:18:20
we don't have to do the integral delta functions effectively make integrals go away
6:18:27
we can do this not just for the delta function delta function of x we can do it for delta functions of x minus anything for
6:18:34
instance x minus a if we had if we plotted this distribution delta of
6:18:39
x minus a it's going to be 0 except for
6:18:44
the point where x equals a so at x equals a
6:18:51
the argument is delta function goes to zero so effectively we've just translated our delta function over by some distance a it's not the most clear
6:18:58
notation this is the x-axis and this is a so what this is going to do and you can
6:19:04
think about doing a change of variables some sort of u-substitution where u equals x minus a it's just going to give
6:19:10
you the value of f at the point where the delta function has support so this is
6:19:15
going to give us f of a so if we have some way of expressing the
6:19:21
delta function or if we're just using the delta function itself translated we can pull out the value of f at any point
6:19:30
we can do more with this though instead of just subtracting values in the delta function we can evaluate
6:19:36
the delta function of a function again what we're working with here is
6:19:41
integrals multiplied by some other function since that's how delta functions most often appear
6:19:46
in this context so if i have coordinates and what i'm
6:19:51
interested in now is g of x plotted as a function of x
6:20:01
suppose g of x looks something like this i have
6:20:09
some places where g of x crosses the uh the x axis where g of x equals zero
6:20:14
i know the delta function is going to be zero for any argument that's non-zero so essentially what this is going to do is
6:20:21
home in on these regions where g of x is equal to zero and i drew five of them here it doesn't really
6:20:27
matter how many there are as we consider a broader variety of
6:20:33
potentials when we solve the time independent schrodinger equation we get a broader variety of solutions
6:20:39
the potentials that we're considering next have a couple of unique conceptual features
6:20:44
that i like to talk about in a little more detail
Boundary conditions in the time independent Schrodinger equation
6:20:50
when you're trying to solve the time independent schrodinger equation for a complicated potential for instance a potential v of x that's
6:20:58
defined as a function of one region
6:21:04
and then another region having a separate function you may end up with a well-defined
6:21:09
solution in region one and a well-defined solution in region two for instance if we had say
6:21:16
a psi of x that was wave-like in region one
6:21:22
and behaved differently in region two for instance just smoothly curving down
6:21:28
to join with the axis it's useful to be able to combine these two solutions and the question then is
6:21:34
how do they match up at the boundary this is the question of boundary conditions which is the subject of this
6:21:40
lecture the boundary conditions that you need to match two solutions of the schrodinger
6:21:46
equation the time independent schrodinger equation now can be determined more or less from
6:21:52
consideration of the time independent schrodinger equation what is the allowed behavior of a solution
6:22:00
we've discussed the time independent schrodinger equation in detail you know now that this is the kinetic energy
6:22:05
operator and this is in some sense the potential energy operator but let's focus on the kinetic energy
6:22:11
operator since it has this second derivative of psi that's where we're going to get a good
6:22:17
notion for what's allowed of psi and what's not allowed of psi
6:22:22
suppose we had a step discontinuity
6:22:29
is that allowed what our psi would look like under those circumstances is something like this
6:22:36
maybe we have a psi that looks comes in on one side and goes out on the other
6:22:41
if this happens in an infinitely narrow region we say psi is step discontinuous here
6:22:48
if we wanted to look at for instance the kinetic energy associated with a step discontinuity like this we're going to
6:22:54
need to take a second derivative of psi so if i take the first derivative of psi
6:23:02
the first derivative of a step function
6:23:07
is a delta function if it's not
6:23:12
obvious why that's the case think about what you would get
6:23:18
if you integrated from one side of the delta function to the other side of the delta function
6:23:24
if you integrate from say a point here to a point here you'll get zero if you integrate from a point here to a
6:23:31
point here you'll get one or you'll get some multiple of one
6:23:36
depending on if you're say multiplying by a delta function like three times a delta function or five times the delta function you get three or you get five
6:23:44
so as a function of integrating from this point to this point
6:23:49
you would get zero zero zero zero zero some constant and then increasing your upper limit on your integration doesn't
6:23:54
change your final answer so integrating a delta function from some point on one side of the delta
6:24:01
to some variable point gets you a step and that's more or less if you go back to the fundamental theorem of calculus
6:24:08
what you expect if you say integrate the derivative as a function of the upper limit of the integral
6:24:14
so the first derivative of our wave function psi here gives us a delta function
6:24:21
if i take then a further derivative the second derivative with respect to x
6:24:27
of my wavefunction psi what i'm going to get is going to be the derivative of a delta
6:24:33
function it's going to be zero away from
The bound state solution to the delta function potential TISE
6:24:40
over the past few lectures we've developed the machinery necessary to solve the time independent schrodinger equation
6:24:45
with a potential given by a delta function we've talked about bound and scattering
6:24:51
states and the delta function potential will actually have both solutions there's both the types of solution
6:24:57
and we've talked about boundary conditions which will help us match solutions in the areas away from the
6:25:03
delta function where we can easily express the solutions match we'll be able to make those
6:25:08
solutions match at the delta function itself so what we're working with
6:25:15
is a delta function potential v of x and v of x under these circumstances
6:25:21
looks something like this it's zero everywhere except at an exact at a
6:25:26
specific point so we're looking at v of x
6:25:32
as a function of x and zero everywhere except at
6:25:37
the origin here x equals zero and there it goes to negative infinity
6:25:44
i'm defining v of x to be minus a times delta of x because we don't necessarily know exactly what the strength of this
6:25:50
delta function potential is you can have different strengths of delta function
6:25:56
if you treat a delta function as a normal as a distribution of course it has to be normalized but in this case we're treating it as a representation of
6:26:02
a potential so we need some constant here which determines the strength of the potential relative to sort of a unit
6:26:08
normalization unit normalized potential
6:26:13
what our solutions will look like under these circumstances depend on the energy of the solution for instance if we have
6:26:20
an energy up here e greater than zero
6:26:25
we know we have in these regions away from from x equals 0 we know we have sort of
6:26:31
traveling wave solutions we don't know exactly what happens at x
6:26:36
equals 0 here but we know these are going to look like solutions to our free particle potential
6:26:42
which we discussed a few lectures ago on the other hand if we have an energy
6:26:48
below zero then we know what the solutions have to look
6:26:54
like when our energy is below our potential
6:27:00
our solutions have to curve away from the axis and if we're going to have something
6:27:05
normalizable we need to have the solutions eventually as they curve away from the axis instead of curving up to
6:27:10
infinity or curving down to minus infinity they have to just sort of smoothly join in with the axis itself
6:27:18
and we have to have that on both sides of the boundary
6:27:23
but we still don't know what exactly happens at the boundary that's where our boundary condition
6:27:28
matching comes in but first of all let's consider what the solution looks like away from
6:27:35
the boundary and in this lecture i'm going to focus on the bound state the state where the
6:27:41
energy of the of the state is less than zero
6:27:46
for the bound states energy less than zero if what we're looking at is away
6:27:54
from x equals zero then we know v of x is equal to zero
6:28:00
so our time independent schrodinger equation becomes minus h bar squared over 2m times the second derivative of
6:28:06
psi with respect to x
6:28:12
is going to be equal to e times psi we know the energy now is negative so
6:28:19
we're going to have a negative quantity on the left and a negative quantity on the right
6:28:24
in order to consolidate some constants let's consider moving the 2m over h bar squared over to the right-hand side here
6:28:30
by multiplying through 2m over h bar squared we'll end up then with d squared
6:28:35
dx squared of psi is equal to k squared
6:28:41
psi where i'm defining k to be equal to something that looks a little strange
6:28:47
square root of minus 2 m e all over h bar
6:28:54
to make the signs clear here energy is negative so what we're
6:28:59
actually looking at here is the square root of a positive number we've got a negative energy positive mass and negative negative from
6:29:06
the minus sign negative from the energy so we're taking the square root of a negative quantity here so our k constant here is going to be
6:29:14
real looking at our equation here
6:29:20
you can look at this and think second derivative is giving me something squared times my
6:29:26
wavefunction back well i know what the solution to that
6:29:31
sort of differential equation is it's psi of x is equal to a
6:29:36
e to the minus k x plus b e to the kx
6:29:44
this is our general solution and as is typical in quantum mechanics
6:29:49
if what we're going to have is normalizable then we can set some conditions on this
6:29:56
our actual space looks like this we have as a function of x our potential is blowing up at x equals zero
6:30:04
so we know we have a solution away from x equals zero that's what we're trying to find here if we want a solution on the right here
6:30:11
for x greater than zero and we want our wave function to be normalizable we know we have to have b
6:30:17
equals to zero because if we have a non-zero b integrating say the squared modulus of
6:30:22
the wave function from zero to infinity will give us infinity because we have something growing exponentially here
6:30:28
so for x greater than zero we know b must be equal to zero similarly for x less than zero
6:30:35
we have to have a equal to zero because otherwise we have something growing exponentially as x goes to minus
6:30:41
infinity what our overall solution then will look
6:30:46
like is in one in in region one here let's say psi one of x
6:30:52
is going to be equal to a times e to the minus k or to the e to the k x
6:30:59
whereas in region 2 we're going to have our solution psi 2 is equal to b e to the k x with the minus sign
6:31:08
so e to the minus kx over here e to the kx over here what our solution then is going to look
6:31:13
like overall is something like this and something like this and we still
6:31:19
don't know exactly what happens at the boundary so let's figure out what actually happens at the boundary
6:31:26
our boundary conditions and we had two of them was first of all that psi was continuous
6:31:32
and second of all that the first derivative of psi was continuous unless the potential went to infinity
6:31:38
let's consider the first of those boundary conditions here psi continuous
6:31:46
in order to have psi continuous what this means is that in our regions here we have psi one on
6:31:52
the left and psi two on the right of x equals zero here
6:31:59
if we're going to match these two conditions continuously we have to have psi one of x equals zero
6:32:06
equal to psi two of x equals zero if i evaluate my solution on the left at
6:32:12
the boundary and my solution on the right at the boundary i have to get continuity i have to get equality
6:32:21
so if we go back to our general solution we had our psi 1
6:32:28
was flipping back a slide a moment to get my a's and b's straight our psi one was a
6:32:33
times an exponential growing with x and psi two was b times an exponential decaying with x
6:32:39
so going forwards a slide our solution in region one is a e to the k x if i'm
6:32:45
evaluating that at x equals 0 i have to get something that's equal to b times e to the minus k x evaluated at x equals
6:32:52
0. now when i evaluate the exponential parts here at x equals 0 i'm substituting in zero in the exponent
6:32:58
anything to the zero is zero or isaar is one so both of these terms become one and
6:33:05
i'm just left with a equals b that helps that helps a lot
6:33:12
but it doesn't tell us everything our second boundary condition was that the first derivative of the wave
6:33:18
function d psi dx was continuous
6:33:26
but it's actually not continuous in this case we had a condition on this boundary condition we could only apply this boundary
6:33:31
condition when the when the potential remains finite and in this case we have delta function potential at the origin so we're going
6:33:38
to actually break this boundary condition in this case we're not going to break it beyond all
6:33:44
hope of recovery though the question is what does dsidx do at the boundary
6:33:52
the way to solve this problem is to go back to the schrodinger equation the time independent schrodinger equation and keep in mind that our potential now
6:33:59
is delta of x it's a delta function we actually had a minus sign and an a in
6:34:05
front of that so if we go back
6:34:11
and think about what happens with delta functions delta functions are only really meaningful
6:34:17
when you treat them as distributions and integrate the trick here then
6:34:24
is to think about integrating the schrodinger equation
6:34:29
where does it make sense to integrate the schrodinger equation well i don't know anything about the solution
6:34:35
well i know everything about the solution away from the boundary but i don't know what happens at the boundary so let's just integrate over the
6:34:40
boundary let's integrate from say minus epsilon to epsilon just integrating over the boundary
6:34:48
to rewrite that what we've got is minus h bar squared over 2m times the integral
6:34:54
from minus epsilon to epsilon of second derivative of psi with respect to x squared
6:35:02
that's our first term then substituting in for our delta function we have minus a
6:35:08
integral from minus epsilon to epsilon of delta of x
6:35:14
psi of x and then on the right hand side we have an integral from minus epsilon to
6:35:19
epsilon of energy which is a constant and can come out psi of x
6:35:26
all of these integrals and i've left them off all over the place are taken with respect to x
6:35:32
so we have three separate integrals here and we can figure out what each of these terms look like
6:35:38
our left-hand term we have the integral with respect to x of a second derivative so that's easy we're just going to get
6:35:44
the first derivative minus h bar squared over 2m times d psi
6:35:50
dx evaluated at the endpoints epsilon and minus epsilon
6:35:57
so far so good the second term here we have minus a and now we just have a delta function
6:36:06
in an integral delta functions just pull out the value of whatever else is in the integral
6:36:11
wherever the delta function or wherever the argument of the delta function goes to zero in this case delta of x is going
6:36:17
to pull out the x equals zero value of psi so this is just going to give me psi of zero
6:36:24
in the right hand side here i'm going to get something but the key point
6:36:30
about this integral is that we're only integrating over the boundary we're going from minus epsilon to epsilon
6:36:36
you can probably see where i'm going with this i'm going to let epsilon be a very small number
6:36:42
as minus epsilon goes to epsilon or as both or as epsilon goes to zero i'm
6:36:47
essentially integrating this function psi from zero to zero so i'm not going to get anything meaningful here i'm just going to get
6:36:54
zero so this is actually all right what we've
6:37:00
gotten from consideration of integrating the time independent schrodinger equation over the boundary with the
6:37:05
delta function potential is a condition that tells us how much our first derivative changes
6:37:11
at the boundary if i rearrange the expression this
6:37:16
expression here i'm getting derivative of psi with respect to x evaluated at epsilon
6:37:23
minus what i get if i evaluate it at minus epsilon that's just equal to rearranging my
6:37:30
constants uh what is it going to be equal to minus 2 2 m a
6:37:36
over h bar squared times psi zero
6:37:41
so that's actually pretty nice to work with try and move this over a little bit
6:37:47
to give myself more space to work and
6:37:53
what we're left with then is substituting our general expression for our solutions for psi
6:38:00
now away from the boundary into this expression so four we had
6:38:07
d psi dx evaluating this at positive values of epsilon means i'm in
6:38:14
region two i'm on the right which means i'm working with psi two evaluating that at x equals 0 on the
6:38:21
boundary subtracting d psi 1 dx evaluated x equals 0.
6:38:28
so for now i'm letting epsilon go to 0 and i'm looking at just the values of
6:38:33
the first derivatives this is our left hand side over here
6:38:39
we can substitute in values for that because we know what these expressions are and furthermore we know that a is
6:38:45
equal to b in our expressions for the general solution so if you refer back to our definitions
6:38:50
earlier what you get here you're taking the derivative of an exponential which brings down the k
6:38:56
and we get minus b k e to the minus k x
6:39:02
and we're evaluating this e to the minus k x at x equals zero so this e to the minus k x is just going
6:39:08
to go to one so i'm not going to bother writing it i just get minus b k for the first derivative of psi in region 2
6:39:16
at the boundary for the first derivative of psi in region 1 at the boundary
6:39:23
now i'm subtracting it because i've this is the second endpoint endpoint
6:39:28
i get a very similar expression again b k e to the now plus k x
6:39:36
and again evaluating this at zero means my e to the k x is just going to be one
6:39:42
the right hand side now we had constants minus 2 m a over h bar
6:39:49
squared and then the eval the value of psi 0 psi at 0 is just going to be b e to the
6:39:56
plus or minus kx again substituting in x equals zero it doesn't matter if i'm considering the plus or the minus region
6:40:02
one or region two this is still going to be just one
6:40:08
so so far so good i can cancel out all of my b's and what i'm left with when i simplify a
6:40:14
little bit is minus 2 k being equal to minus 2 m a
6:40:21
over h bar squared this is the sort of condition we got
6:40:27
when we were looking at how the boundary conditions affected the solution to the particle in a box potential
6:40:34
the infinite square well potential when we actually looked at what the boundary conditions required and in the
6:40:40
case of the particle in a box it was that the wave function went to zero at the endpoints of the box
6:40:46
we got quantization we have quantization again here
6:40:51
except we have a strict equality there are really no more unknowns in this expression
6:40:58
if you manipulate this further k equals m a over h bar squared keeping
6:41:04
in mind that k is equal to the square root of minus 2 m e where e is a negative number over h bar
6:41:12
you can solve for the energy and what you get is that energy is equal to minus m a squared
6:41:19
over 2 h bar squared we have quantized energies
6:41:27
what our wave function then looks like so far what we know is that psi of x
6:41:32
is equal to on the left e and now substituting back in the
6:41:37
definition for k e to the m a x over h bar
6:41:43
squared if x is less than zero and e minus
6:41:49
m a x over h bar squared if x is greater than zero
6:41:55
now all of this had a b multiplying it out front which i canceled out here
6:42:01
so my first derivative boundary condition did not help me find b but there's one more fact that we know
6:42:07
about wave functions like this and that is that the wave function has to be normalized
6:42:12
so if you want to normalize this you calculate the normalization integral which you all should know by now
6:42:17
integral of psi star psi dx has to be equal to one you can substitute in this definition
6:42:22
for psi set it equal to one do the integral and find out what b is
6:42:27
this was one of our activities on day four so refer back to day four if you want to
6:42:32
see a little bit about how to normalize a wave function like this to summarize our results this is what
6:42:39
our normalized bound state solution actually looks like what you find for the normalization constant is the square
6:42:44
root of m a over h bar out front and now instead of writing it as a
6:42:50
piecewise function for positive and negative x i'm expressing this as an absolute value of x in the exponent
6:42:57
the energy associated with this was minus m a squared over 2h bar squared we are quantized but we only
6:43:03
have one bound state solution singular and this is what it looks like
6:43:09
for a delta function potential you get these two exponentials decaying as x moves away from the origin
6:43:16
to check your understanding consider the following two questions why is there only a single bound state
6:43:22
and can any initial condition be expressed as a superposition of bound state solutions in this case
Scattering delta function potential
6:43:30
we've developed the machinery to solve the time independent schrodinger equation for the delta function potential by connecting solutions
6:43:38
covering the regions away from the delta function and matching them together with boundary
6:43:44
conditions at the delta function itself the last lecture discussed the bound state solution
6:43:51
this lecture discusses the scattering state solutions to put this in context what we're
6:43:57
talking about is a potential v of x given in terms of a dirac delta function
6:44:03
a now is just a constant that defines how strong the delta function actually is
6:44:09
so our potential is everywhere zero except at some point
6:44:16
where it goes to negative infinity this is a plot now of v of x as a
6:44:22
function of x what we discussed in the last lecture was the bound state solution
6:44:28
what happens if we have an energy e of our state that's less than
6:44:34
the potential less than zero less than the potential away from the delta function and what we got
6:44:39
was a wave function psi of x that looks something like this
6:44:47
going down towards zero away from the actual position of the delts function i haven't done a very good job drawing
6:44:53
this but i think you get the idea the scattering state solutions by
6:44:59
contrast have energy greater than zero so we're talking about solutions with
6:45:05
energy e up here at regions away from the delta function we have basically the behavior of a free
6:45:12
particle we get traveling waves at regions away from the
6:45:18
delta function away from x equals 0. we don't really know what happens at the origin
6:45:24
but we know what our solutions should look like and we should be able to use our boundary condition matching to figure out what happens at the origin
6:45:32
so what do our scattering states look like well
6:45:38
away from x equals zero we have v of x is equal to zero
6:45:45
that means our schrodinger equation the time independent schrodinger equation looks like minus h bar squared over 2m
6:45:51
times the second derivative of psi with respect to x no potential now
6:45:57
is just equal to e times psi where energy now is strictly greater
6:46:02
than zero we can manipulate our constants much how
6:46:08
we did when we were talking about the bound state and express this as d squared psi dx squared equals minus k squared psi
6:46:17
now i'm defining a slightly different k than when i was talking about the bound state solution because we have a different sign for the energy instead of
6:46:24
having k be a negative or imaginary now i'm going to again have k be positive and real by
6:46:31
saying k is equal to the square root of 2 m e over h bar
6:46:37
if you recall when i was talking about the bound state i had e less than 0 and i had a minus sign inside this expression
6:46:45
looking at this ordinary differential equation we can write down the solution and the
6:46:50
solution is let's say psi is equal to a e to the i k x
6:46:56
plus b e to the minus i k x when we take the second derivative of
6:47:02
this exponential we'll bring down an i k quantity squared which will give us a minus k squared
6:47:09
since we're talking only about regions away from the delta function we really actually have two general solutions here
6:47:15
we have psi one for regions for say x less than zero and we have psi
6:47:20
two for x greater than zero psi two now to the right of the delta function is
6:47:26
going to look very similar and it's going to be f e to the i k x
6:47:31
plus g e to the minus i k x i should write this as a capital g sorry
6:47:38
instead of saying c and d i've jumped ahead to f and g to eliminate any possible ambiguity if we
6:47:44
have to design to assign future constants for example e
6:47:49
so we have our two general solutions covering regions for negative x and for
6:47:55
positive x what happens at the boundary how do we match these solutions up
6:48:02
our boundary condition matching in terms of these two general solutions is a two-stage process we have two
6:48:08
distinct boundary conditions and the first is that psi is continuous
6:48:17
what that means is that psi one of our solution for x's for negative x
6:48:24
evaluated at the boundary at x equals 0 must be equal to psi 2 of 0 our solution
6:48:30
for positive x's evaluated at the boundary if i substitute 0 in for these
6:48:36
exponentials for x in these exponentials what i end up with here is reasonably straightforward a plus b
6:48:43
equals f plus g that's the result of our continuity boundary condition
6:48:50
and it helps but it doesn't help all that much we only get a single equation out of this
6:48:55
so we need to do more the first derivative boundary condition
6:49:01
is that the first derivative of psi is continuous provided that the potential is finite however in this case our
6:49:08
potential is given by delta of x which does not remain finite at x equals
6:49:13
zero the trick that we used when we were discussing the bound state solution
6:49:19
was to effectively integrate the schrodinger equation dx
6:49:25
from one side of the boundary minus epsilon to the other side of the boundary plus
6:49:30
epsilon when we integrate this we should still have an equality integrating the terms on the left-hand side and
6:49:36
integrating the terms on the right-hand side and knowing the properties of the delta function we can simplify this integral
6:49:42
greatly i refer you back to the notes for the last lecture to see how would
6:49:47
see what this actually works out to be what it tells you is that d psi dx the first derivative of
6:49:54
psi which we get from integrating the second derivative of psi evaluated at epsilon
6:50:00
and then subtracting the value evaluated at minus epsilon essentially the change in the first
6:50:06
derivative as we go from one side of the boundary to the other is equal to minus
6:50:12
two times m times a the strength of our potential over h bar squared
6:50:18
times psi evaluated at 0. the right hand side here we actually got from the integral of our delta function
6:50:25
times psi so this is our boundary condition here appropriate for use with delta function
6:50:32
potentials this tells us about the behavior of the first derivative of psi as we cross the
6:50:37
boundary so we're going to need to know what our first derivatives actually are well psi 1 was equal to a e to the i k x
6:50:46
plus b e to the minus i k x so if i take the first derivative of this and evaluate it at
6:50:53
effectively zero some very small quantity what i'm going to get for d psi one
6:51:00
dx evaluated at zero essentially epsilon plus that or
6:51:05
minus epsilon i'm looking at psi one now so i'm talking about the negative half plane
6:51:11
negative x's what i get is
6:51:16
i k is going to come down from both of these and i'm going to get an a minus b
6:51:24
i can do the same sort of thing for psi 2 which was equal to
6:51:29
f e to the i kx plus g e to the minus i kx when i take
6:51:34
the first derivative of this d psi 2 dx and evaluate it at the boundary i'll
6:51:40
end up with i k times f minus g by similar reasoning
6:51:47
that means the left hand side here which i can calculate by looking at the derivative of psi for positive
6:51:54
values of x as x goes to zero this expression and subtracting
6:52:00
the first derivative of psi for negative values of x as x goes to zero this expression what i end up with is i
6:52:08
k times f minus g minus i k times a minus b
6:52:14
that's the left hand side now of our expression up here our right hand side is minus two m a
6:52:21
over h bar squared times the value of psi at x equals zero
6:52:26
now if you look at either one of these definitions you can see what happens when we substitute in x equals zero we get a plus b for this one or f plus g
6:52:33
for this one and i have a bit of a choice as to which one i want to use in this case i'm going to use a plus b
6:52:39
and you'll see why in a moment what we end up with now if you
6:52:44
manipulate this expression a little bit and define a useful constant in this case the constant is going to be beta
6:52:51
just to save some writing beta is defined to be m a over
6:52:56
h bar squared k what we end up with is f minus g
6:53:03
is equal to a 1 plus 2 i beta minus
6:53:09
b 1 minus 2 i beta and this is the result of our first
6:53:17
derivative boundary condition there's effectively no restriction on
6:53:23
these solutions so far we have something similar to what we had for the free particle
6:53:29
there were no boundaries that were terribly restrictive we did not end up with a quantization condition we did not
6:53:35
end up with enough of a restriction on our solutions that we ended up with something straight normalizable
6:53:40
but we have our two equations now involving a b f and g
6:53:46
that unfortunately is two equations to go with four unknowns we have our definitions of psi in terms
6:53:51
of a b f and g and these e to the i k x e to the i minus k x e to the minus ikx
6:53:58
and then we have our two on two equations relating a and b and f and g
6:54:05
it seems like we're not going to be able to come up with a very rigorous solution here but we can actually do a little better
6:54:11
if we start thinking about what the initial conditions might actually be first of all
6:54:17
note that these solutions are the spatial part and if we add a temporal part to come up
6:54:24
with an overall solution for an overall wave function we'll end up with the same sort of traveling wave states that we had for the free particle
6:54:30
those time the time dependence for those states was essentially e to the minus
6:54:36
i e t over h bar
6:54:41
if you look at each in each of these terms you can see this is a plus ikx going with the minus iet
6:54:48
as time increases space must increase here in order to maintain a constant
6:54:54
phase so as in our discussion of traveling waves the plus ikx here for
6:55:00
positive values of k is associated with the wave propagating to the right so if you think about our boundary here
6:55:05
at x equals zero in the space to the left of the boundary where we're considering psi 1
6:55:10
we have a wave coming in from the left whose amplitude is given by a
6:55:17
conversely the term with b in it here is associated with e to the minus i kx
6:55:23
that represents a wave traveling away from our boundary with amplitude b
6:55:31
[Music]
6:55:41
the bound states for the finite square well potential are discussed in another lecture the subject of this lecture is the scattering states for the finite
6:55:47
square well which can be derived in a very similar way the overall context
Finite square well scattering states
6:55:53
is our finite square well potential a potential v of x that's defined to be 0 for x less than minus a 0 for x greater
6:55:59
than a and a constant minus v naught for x is in between minus a and a
6:56:06
so this is an even potential and we exploited that fact when we were discussing the bound states
6:56:12
states where the energy is negative to figure out what the what those states look like and the lowest energy bounce
6:56:18
state that we found ended up looking something like this smoothly joining the axis as x became
6:56:25
becomes larger negative smoothly joining the axis for x becomes larger and positive and a smooth curve in between minus a
6:56:31
and a inside the well we found this by examining the general solution for regions
6:56:38
less than minus a between minus a and a and greater than a and smoothly matching those piecewise
6:56:44
defined solutions together with the boundary conditions for the schrodinger equation
6:56:50
we're going to take a very similar approach here except instead this time we seek scattering state solutions
6:56:56
where the energy e is everywhere above the potential and as a result our solution can extend all the
6:57:01
way from minus infinity to plus infinity the solutions that we get will end up
6:57:07
looking a little something like this but we'll see what they look like momentarily
6:57:16
given this potential we're looking at three distinct regions and we're trying to solve our schrodinger equation over
6:57:21
those regions our schrodinger equation as always is minus h bar squared over 2m second
6:57:27
derivative of psi with respect to x plus v of x
6:57:33
psi is equal to e times psi now we know away from our discontinuities to v of x is going to be
6:57:40
a constant so we expect the overall properties of this solution to be relatively straightforward
6:57:46
and indeed they are our three regions are divided by x equals minus a
6:57:52
and x equals a for x is less than minus a
6:57:59
our potential here is going to be defined to be zero and our schrodinger equation then
6:58:05
simplifies to something of the form second partial derivative of psi with respect to x is equal to minus k squared
6:58:12
psi where k is defined as for instance in the case of the free particle as 2 me
6:58:20
over h bar squared k squared excuse me k squared is 2 me over h bar squared
6:58:26
we know the solution to this case for the free particle gave us traveling waves and we're going to reuse that form
6:58:31
of our solution here we'll have psi being equal to a e to the i k x
6:58:38
plus b e to the minus i k x traveling waves moving to the right and
6:58:43
traveling waves moving to the left of course nothing is traveling about this now since we're just looking at
6:58:48
solutions to the time independent schrodinger equation but if you as before add the time
6:58:53
dependence to these solutions you find that they are indeed traveling waves
6:59:00
that was for the region where x is less than minus a the region where x is greater than minus
6:59:06
a is going to give us something very similar it's going to give us an exactly identical schrodinger equation and it's going to give us exactly identical
6:59:13
solutions except we'll be working with slightly different constants our wavefunction psi is going to be given by
6:59:20
in this case i'll call it f e to the i k x plus g e to the minus i k x
6:59:27
now i've used different constants for f and g but the same constant for k since overall we're trying to solve the
6:59:34
same schrodinger equation so we have effectively the same value for e and therefore the same value for k as
6:59:40
defined in terms of 2 m e over h bar squared for the region in between minus a and
6:59:48
plus a we're going to have a slightly different schrodinger equation it's going to give us essentially the
6:59:53
same sorts of solution though but i'm going to write them slightly differently our overall schrodinger equation will
6:59:59
become as before the second derivative of psi is equal to minus some constant
7:00:04
times psi but the constant is going to be different the constant instead of being 2m e over h bar is going to be 2m over h
7:00:12
bar squared times e minus v
7:00:18
e minus v naught but since sorry e minus v of x let me step this out a little bit e
7:00:24
minus v of x that's v of x and in the region between minus a and a
7:00:30
is minus v naught this is effectively e plus v naught
7:00:35
so we have our constants here and in the case of these solutions we
7:00:40
could easily write them in terms of traveling waves with l instead of k but it's actually slightly easier here
7:00:47
to write them instead in terms of sines and cosines this is just as general of a solution
7:00:52
but let's write psi in this regime as c times the sine
7:01:00
of l x plus d times the cosine
7:01:06
of lx apologies for being messy here these are then our three general
7:01:12
solutions we can call them psi one psi two and psi three if you like
7:01:17
but these are general solutions to the schrodinger equation the time independent schrodinger equation for these three regions
7:01:24
the next step is to mesh these solutions together with our boundary conditions
7:01:31
we had two boundary conditions and if you're unfamiliar with the boundary conditions that we'll be using under these circumstances i suggest you go
7:01:37
back and examine the lecture on boundary conditions the first of our boundary conditions was
7:01:43
that the wave function is continuous and the second was that the first derivative of the wave function is continuous and there are sound physical
7:01:50
reasons that those that that has to be the case for instance if the wave function itself is discontinuous the expectation value
7:01:57
for the kinetic energy of the wave function diverges to infinity and cannot be a physical state
7:02:03
but considering the boundary at x equals minus a ensuring that the boundary condition
7:02:09
holds means meshing the value of this wave function at minus a and the value of this wave
7:02:16
function at minus a excuse me
7:02:22
so let's go ahead and plug that in our boundary condition at minus a
7:02:28
here is going to give us a e to the minus i k x
7:02:33
plus b e to the i k x oh sorry not x we're plugging in for x
7:02:39
minus i k a and then b e to the i k a since i'm substituting in minus a for x
7:02:47
that's what i get for this region that has to be equal to what i get for this region
7:02:52
which in this case i will write as minus c sine la
7:02:58
plus d cosine l a
7:03:04
now if i substitute in minus a for x here i would actually get the sine of minus l a but since the sine is an
7:03:10
odd function i'm pulling the minus sign out front and writing this as minus c times the sine of l a just to keep the
7:03:15
arguments inside all the trig functions consistent so this is our boundary condition for
7:03:20
the continuity of psi we have another boundary condition for the first derivative of psi
7:03:25
and you can write that down more or less just as easily by noting that in either of these cases taking the first
7:03:31
derivative with respect to x is going to bring down an ik so we'll end up with i k times the
7:03:39
quantity a e to the minus i k a
7:03:44
plus b e to the ika
7:03:49
and i've screwed up the minus signs already since the sign here is going to bring down a
7:03:56
minus ik when i factor out the ik i'll still be left with the minus sign so that's our first derivative of the
7:04:02
wave function in this region and if we're going to ensure continuity of the first derivative we must also
7:04:08
equal the first derivative of this wave function evaluated at the boundary
7:04:13
taking the first derivative of sine and of cosine is going to pull out an l so i'm going to have something that looks similar i'm going to have l times the
7:04:20
quantity then the derivative of sine is cosine c cosine l a the derivative of cosine
7:04:26
is minus sine so i'm going to have minus d
7:04:33
sine and i'm evaluating it at minus la again which i'm going to use to cancel
7:04:39
out this minus sign sine of minus an argument is minus the sine of the argument so i have two minus
7:04:45
signs and i end up with a plus overall so these are our boundary conditions at
7:04:51
x equals minus a we get very similar expressions for our boundary conditions at plus a
7:05:00
but before i write them down i'm going to make an additional simplification since what we're considering here are
7:05:05
scattering states for instance in our consideration in our
7:05:10
consideration of the scattering of uh scattering states off of a delta function potential we had a wave
7:05:17
incident from the left a wave bouncing back to the left and we had a wave that was
7:05:24
transmitted through that was for a single potential if we have some potential well
7:05:32
we're still probably interested in the same sort of process a wave incident from the left a wave scattering back to the left and the wave transmitted
7:05:38
through to the right we're probably not so concerned with the wave coming in from the right so i'm
7:05:43
going to get rid of that one and that amounts to setting g equal to zero
7:05:49
on for our general solution in this regime so we're no longer working with a fully general solution
7:05:56
but we have one fewer unknown to work with since we've gotten rid of g which simplifies the algebra uh quite a lot
7:06:04
makes it solvable in fact so going through the same procedure we did at minus a instead evaluating the
7:06:10
wave function and its first derivative at a the expressions we get are
7:06:16
c sine l a my plus d cosine
7:06:22
l a is equal to f e to the i k a
7:06:28
that's from just continuity plugging in x equals a into this expression and setting it equal to plugging x equals a
7:06:34
into this expression our first derivative again by taking the first derivative and repeating the process gives you l times
7:06:41
c cosine c times the cosine of l a minus now
7:06:49
d times the sine of l a we have a minus sign here because
7:06:55
now we get the minus sign from taking the derivative of cosine and we're substituting in plus a
7:07:01
so what i did to get a plus sign here no longer works i can't factor a minus sign out
7:07:07
that's our left hand side and it's going to be equal to first derivative of this brings down an ik as
7:07:13
before i k f e to the i k a
7:07:19
so those are our general boundary conditions and we have essentially five equations and four
7:07:25
unknowns here we have a b c d f and k
7:07:31
all being unknown k is determined entirely by the energy and since what we're working with here
7:07:37
are scattering states linear algebra is very useful for quantum mechanics
Linear algebra introduction for quantum mechanics
7:07:42
we've already used a lot of the notation and terminology of linear algebra when we say for instance the two wave
7:07:48
functions are orthogonal to each other but quantum mechanics puts its own spin on
7:07:53
things in part for instance because we're not dealing with say three-dimensional cartesian coordinates we're dealing with
7:08:00
a complex vector space that describes the state of a physical system so dealing
7:08:05
with complex numbers and dealing with vector spaces in more a more general way is very useful especially as we move
7:08:12
away from simply solving the schrodinger equation but to manipulating solutions to the schrodinger equation to infer the
7:08:17
properties of physical systems so linear algebra will be useful in the
7:08:23
coming chapters to justify why this is useful i'm going to make a couple of analogies
7:08:30
there are some things that we can say on the basis of vectors we have vectors a
7:08:36
we can make dot products between two vectors we can express the vector a
7:08:42
and say cartesian coordinates a x x hat plus a y y hat plus a z z hat
7:08:50
we can also express some vector a in a different coordinate system i'll call it a sub alpha
7:08:57
not x hat excuse me alpha hat plus a beta beta hat plus a
7:09:04
gamma gamma hat where now the hatted vectors here are unit vectors and the numbers ax ay az a
7:09:11
alpha a beta and a gamma are simply coordinate or simply components they're simply numbers
7:09:18
if these x y and z alpha beta and gamma represent different coordinate systems
7:09:24
we can still say that this is the same geometrical object a the vector a is not
7:09:29
changed by expressing it in different coordinate systems it exists independent of any coordinate system
7:09:36
and of course we can also take dot products of unit vectors with themselves and get one
7:09:42
quantum mechanically speaking each of these expressions in terms of vectors has an analog the vector a
7:09:49
that's what we've been talking about so far as say psi of x that's the state of the physical system
7:09:55
the wave function taking the dot product for two vectors
7:10:00
that's our integral from minus infinity of say psi sub a star of x times psi sub
7:10:06
b star of x integral dx expressing a vector in terms of
7:10:14
one coordinate system versus in terms of another coordinate system is essentially the difference between looking at the state of the system as the wave function
7:10:21
psi of x versus the wave function in momentum space the wave function phi of
7:10:27
k which we got by taking fourier transforms back when we considered the free particle
Linear transformation
7:10:34
our whirlwind tour of linear algebra continues with linear transformations here we'll write linear transformations
7:10:41
with hats for instance t with a hat capital letters especially will be considered to be transformations
7:10:48
a linear transformation quite simply is a transformation that's linear what it means for something to be linear
7:10:54
is if i apply the transformation to a times the vector alpha plus b times the vector beta
7:11:00
i get a times the transformation t applied to the vector alpha plus b
7:11:06
times t applied to the vector beta if this sort of identity holds the transformation you're working with
7:11:12
is linear it's difficult to work with transformations in general so it's useful to consider what a transformation
7:11:19
looks like if we have a vector in a particular basis so suppose i have a set of basis vectors
7:11:26
x sub i not telling you how big this set of basis vectors is
7:11:31
but if we have our transformation applied to the basis
7:11:37
vector x sub i let's say x sub 1 in particular
7:11:44
that transformation applied to a basis vector will be given by another vector which is in general going to be expressed as a
7:11:50
sum of basis vectors so x1 will be transformed into some number
7:11:56
which i'll write as t11 x1 vector x1
7:12:01
not xi excuse me tx1 plus some other number t21 times the
7:12:08
vector x2 plus t31 times the vector x3
7:12:15
etc up to xn
7:12:20
if i have say x2 i get a similar expression except i'm
7:12:25
going to number things slightly differently i'll say this is the t12 number
7:12:31
is the x1 component of the transformation applied to x2
7:12:38
plus etcetera t 2 2 x 2 plus t
7:12:44
3 2 x 3 plus etcetera
7:12:51
so if i have some vector then alpha being expressed as a1
7:12:58
x1 plus a2 x2 plus a3
Mathematical formalism is Quantum mechanics
7:13:06
the mathematics of quantum mechanics is technically speaking linear algebra and an infinite dimensional vector space
7:13:12
now if that seems a little bit unfamiliar don't worry we will work through it step by step
7:13:18
it does turn out however to be an immensely powerful mathematical structure there's a lot more going on behind the scenes to quantum mechanics
7:13:24
than simply the wave function what we're really talking about in terms of the formalism of quantum
7:13:29
mechanics is attempting to represent the quantum mechanical state of the system
7:13:34
now what is the state of the system well quantum mechanically speaking it's everything that we can possibly know
7:13:40
about the physical system that we're working with there is no further level of information than knowledge of the state
7:13:46
and we've been working with states in a couple of different ways the first way we worked with state was this notion of a wave function let's say
7:13:52
psi of x and t and to some extent you can write down sort of closed form mathematical expressions for psi let's say psi is
7:14:00
equal to some sort of maybe it's a gaussian or a sinusoid or a complex exponential
7:14:06
we also thought about representing the state of the system as a superposition a
7:14:11
sum over n of some coefficient a sub n multiplied by some psi sub n
7:14:17
of x and t where these psi sub n's come from solutions to the time independent
7:14:22
schrodinger equation we're talking about say particle in a box or the quantum harmonic oscillator gives you sets of
7:14:29
wave functions that you can superpose together to represent an arbitrary state of a quantum mechanical system
7:14:35
we also talked about representing the wave function as some sort of an integral perhaps we're integrating from minus infinity to infinity instead of
7:14:42
summing we're computing an integral we're integrating perhaps decay if we're working with the free particle for
7:14:48
instance and we have some sort of a phi of k some sort of a coefficient that tells you how much each of the
7:14:54
stationary states for the free particle that we have to work with and those free particle states look something like e to
7:14:59
the i k x minus h bar k squared over two m t
7:15:05
uh and there was a normalization divided by the square root of two pi if i recall correctly now these expressions bear a certain
7:15:11
similarity instead of a sum we have an integral instead of a discrete list of coefficients we have a function phi of k
7:15:18
instead of a stationary state we have a stationary state
7:15:23
we also talked above and beyond these sorts of representations hinting at some sort of a deeper mathematical structure
7:15:28
we wrote down expressions like psi sub n is equal to a plus the operator acting
7:15:34
on psi sub n minus 1 all divided by the square root of n this sort of expression came from a consideration of an operator
7:15:40
algebra that actually had no knowledge whatsoever of the states so while you can think of representing
7:15:45
the states as sort of a closed form mathematical function some sort of a list of coefficients some sort of a
7:15:50
function there's actually more going on behind the scenes we also have this notion of operators relating different states to
7:15:58
each other and these expressions are going to be true regardless of the nature of psi one and or psi n and psi n
7:16:03
minus one that expression has to hold why well there is a deep mathematical
7:16:08
structure going on behind the scenes here so let's explore that mathematical structure that's what this chapter is all about
7:16:15
so what we're working with here like i said at the beginning technically speaking is linear algebra in hilbert space
7:16:22
now if you've studied linear algebra you know it deals a lot with vectors and you can gain a lot of intuition about the
7:16:27
behavior of physical systems in terms of vectors so say we have some sort of a vector a pointing in that direction
7:16:34
some sort of a vector b pointing in that direction you can do basic vector operations on these things we can for
7:16:39
instance take the dot product of a and b and i've drawn these things as approximately perpendicular to each other so you'd expect the dot product to
7:16:46
maybe be 0. we can also write perhaps the vector b as some sort of linear transformation
7:16:52
acting on a vector a and in the language of three-dimensional vectors it's easy to write down linear transformations as
7:16:57
matrices in this case three by three matrices so if you've studied linear algebra these sorts of concepts are familiar to
7:17:04
you in particular there are a lot of linear algebra concepts things like the inner
7:17:10
product or normalization or orthogonality and the notion of a basis that we can
7:17:15
express now the nuance in quantum mechanics is that we're working with a hilbert space and hilbert space technically speaking
7:17:21
is an infinite dimensional vector space
7:17:28
so the infinite dimensionality here i think i've actually wrote written infinite but you get the idea
7:17:34
instead of working in three dimensions we're working in infinite dimensions instead of lists of three numbers we need lists of infinitely many numbers
7:17:42
and that makes uh makes life a little bit more difficult the basic structure ends up being the same though so much of
7:17:47
your linear algebra experience is still going to hold here to give you some basic vocabulary it's
7:17:52
basic intuition we're dealing with vectors first of all and the notation that we'll use for a
7:17:58
vector in the notion of a vectors in the this hilbert space is going to be something like this so vertical bar name
7:18:04
of vector and then angle bracket we'll expand on this notation much more later on in the chapter but for now just think
7:18:10
of this vector as somehow representing the state of the system as a proxy think something like psi of x
7:18:16
if you need a more concrete uh representation of the state you don't want to just think in general now i can
7:18:22
tell that when we're talking about linear algebra and hilbert space as applied to quantum mechanics this representation is actually more useful
7:18:29
than the wavefunction and we'll see why that's the case later on oftentimes we don't need to know anything about the wavefunction to still make useful
7:18:36
conclusions on the basis of the vectors themselves so what else can we do in terms of
7:18:41
linear algebra well we can do inner products
7:18:46
the way we'll write that in this notation is b a or beta alpha here angle bracket beta
7:18:52
vertical bar alpha angle bracket in the language of states and wave functions
7:18:58
you can represent inner products like this as integrals minus infinity to infinity of in this case let's say psi
7:19:05
beta star as a function of x times psi alpha of x all integrated to dx this is
7:19:12
that same sort of normalization and orthogonality integral that we've been dealing with a lot in the context of wave functions but expressed in a more
7:19:18
compact notation in a more general mathematical form that of linear algebra with this notion for an inner
7:19:25
product we can also think about normalization
7:19:30
something like the vector alpha inner product with the vector alpha would translate into wave function language as
7:19:36
an integral from minus infinity to infinity of psi sub alpha of x psi sub alpha of x need to complex
7:19:43
conjugate this one sorry about that dx and in terms of normalization this had better equal one and this had better
7:19:49
equal one so the inner product of a vector in this hilbert space with
7:19:54
itself had better give you one if this is going to represent a valid quantum mechanical state same as the wave function has to integrate in the squared
7:20:01
modulus context to give you one we can also talk about orthogonality
7:20:10
orthogonality in the language of linear algebra refers to the vectors being perpendicular to each other if you're just thinking in three dimensions now in
7:20:17
infinite dimensions it's a little bit harder to express a little bit harder to think about but it's just as easy to write down i can say alpha and beta
7:20:25
equals zero that means these vectors are orthogonal to each other and the language of integrals here
7:20:32
integral from minus infinity to infinity of psi alpha of x complex conjugate psi
7:20:37
beta of x is going to give you zero if these come from for instance
7:20:43
uh solution to the time independent schrodinger equation perhaps we have a set of
7:20:48
set of wave functions to work with i'll write that as a set of states say psi sub n
7:20:56
i may be guaranteed that psi sub n inner product with psi sub m gives me a
7:21:02
chronic delta this would express orthonormality that this set is
7:21:07
or every element in this set is orthogonal with every other element and that each element of the set is properly normalized
7:21:15
uh we can also talk about the completeness
7:21:23
of a basis so working with this sort of set psi n suppose it comes from solving the
7:21:29
schrodinger equation and the language of the wave function i can express some arbitrary psi
7:21:35
arbitrary quantum mechanical state has a sum of let's say n equals 1 to
7:21:40
infinity potentially of a sub i size sorry a sub n psi sub n
7:21:48
if this sort of expression is possible these size of n's form a complete basis
7:21:55
and if you think about implied invoking the orthogonality and applying
7:22:00
fourier's trick to this sort of expression that works out just as well you can figure out that in this case a
7:22:06
sub n is going to be what you get if you take the inner product of psi sub n with
7:22:12
this arbitrary wave function that we're starting with
7:22:17
now these expressions have corresponding versions in the in terms of the wave function as well but since
7:22:22
i'm running out of space on the slide i'm not going to go into the details this one is going to be an inner product same sort of integral as we're working
7:22:28
with here likewise this is your infinite sum i think i have that expression on the last slide
7:22:34
now within the language of linear algebra and hilbert space we have these sort this sort of notation these sorts
7:22:40
of representations for what these states really are as they exist in the vector space
7:22:45
or in the hilbert space what can we do with these states well the fundamental question quantum mechanics generally has to do with the
7:22:51
observable properties of a system so what do we have in the language of observables well
7:22:57
observables we know are going to be real numbers
7:23:03
and they have some sort of statistical properties in quantum mechanics for instance we talked about the expectation value
7:23:12
say i have some sort of observable q i can write the expectation value as q inside a pair of angle brackets and
7:23:19
the angle brackets here are not exactly the same as the angle brackets in the earlier expressions that we've been working with but the connection is there
7:23:25
is a connection we'll come back to that later if you want to think about the expectation value for example in terms
7:23:32
of some sort of quantum mechanical system we're dealing with an operator so the observable isn't just going to be
7:23:38
the expectation value of some q some quantity q we've got some sort of an operator which i'll write as capital q
7:23:44
with a hat so what would our expectation value q
7:23:50
look like in this language of angle brackets well we know what it looks like in terms of
7:23:55
inner products or in terms of integrals of wave functions for example it's going to be an integral of
7:24:01
the wave function the state of the system then the operator then the wave function of the state of the system and
7:24:06
we have that same sort of notation in context of inner products in our vector space so we would have the state of our
7:24:12
system psi and we have our operator acting on psi
7:24:18
so the operator acting on psi gives you in some sense another state of the system it's not really another state of
7:24:24
the system though it's more a vector in this hilbert space
7:24:30
operators here if i think about this q operator acting on the state of the system is going to give you some new
7:24:36
vector in your hilbert space
7:24:44
now we know that this sort of expectation value quantity or concept
7:24:50
has to result in some sort of a real number so you can think about this as what happens if i take the complex
7:24:56
conjugate of this well if you're thinking about psi
7:25:02
q hat psi complex conjugated in the language of the integrals that we've been working
7:25:08
with this is going to be taking the complex conjugate of q hat psi so instead of being a psi star q hat psi
7:25:15
it's going to be a q hat psi star multiplied by psi inside the integral the same notation sort of holds here
7:25:21
whenever you take the complex conjugate of an inner product like this in our hilbert space you swap the order of
7:25:26
these things instead of the side being on the left the size on the right and the q hat psi on the right is on the
7:25:32
left so this notion of what appears on the left and what appears on the right is a useful way of keeping track of what's been complex conjugated so q hat
7:25:39
psi psi in our note in our revised notation here
7:25:48
now this sort of substitution here if this is going to be equal to the
7:25:56
original expectation value of q right complex conjugate of the expectation value has to be equal to the expectation
7:26:02
value itself if this is going to be a real number this expression has to be equal to
7:26:09
this expression and the equality of two operator expressions in the language of linear
7:26:15
algebra like this essentially the operator can act on the left or the operator can act on the right the
7:26:20
operator behaves the same when acting on a complex conjugate of the state as it does on the state itself complex
7:26:27
conjugate of state with operator state with operator gives you the same result that is only going to be true if the
7:26:34
operator here is hermitian
7:26:41
and there's lots more that can be said about the notion of hermitian operators and we'll come back to that in uh
7:26:46
further lectures but for now um know that there's a lot of mathematical formalism that goes along with linear
7:26:52
transformations such as vectors to new vectors in the space
7:26:57
especially associated with hermitian linear transformations
7:27:04
so as an example of the notion of a hermitian operator and how that manifests itself in this context uh think about the momentum
7:27:10
operator is the momentum operator herniation well if the momentum operator is hermitian
7:27:16
we know that if i have some sort of a wave function f the momentum operator acting on the wave
7:27:22
function g has to be equal to the momentum operator acting on f inner
7:27:28
product with the wave function g sorry i shouldn't say wave function i should say state state f momentum operator g
7:27:34
momentum operator f state g these things should be equal to each
7:27:40
other so let's do some manipulations of the one on the left and just since we have a large amount of machinery for working
7:27:47
with the notion of states in terms of wave functions let's express this in terms of wave functions
7:27:52
so our inner product the terms of the wave functions is going to be the integral from minus infinity to infinity
7:27:58
of some wave function f complex conjugated as a function of x multiplied by the momentum operator
7:28:05
applied to our wavefunction g and our momentum operator is minus i h bar partial derivative with respect to x
7:28:12
so this is acting on the function g of x and we're integrating dx
7:28:18
now this is an expression that looks a little bit difficult to work with we have partial derivatives inside an
7:28:24
integral but whenever you see a derivative inside an integral think integration by parts so let's say i do integration by parts i
7:28:31
can define my variable u to be some sort of f complex conjugate recognizing the
7:28:36
part that i would like to differentiate and the part that i would like to integrate would be well the part that's
7:28:41
already been differentiated so let's say dv is equal to partial g
7:28:47
partial x with dx tacked on so identifying this
7:28:53
part as my v and this part as my u you can pull the constants out front if you want
7:28:59
so this is going to give me du would be the partial derivative of f star partial x and my v when i integrate it
7:29:05
now it's the integral of the derivative so fundamental of theorem of calculus just gives me g
7:29:12
integration by parts then says this whole thing is going to be equal to f of x
7:29:17
sorry f star of x g of x evaluated at my boundary minus infinity
7:29:24
to infinity minus the integral from minus infinity to infinity of these two guys
7:29:32
v d u so i have my partial f partial x
7:29:37
and i have my g and i'm integrating dx so i forgot to technically i should put a dx there in my
7:29:43
integration by parts notation so uh as usual in quantum mechanics we
7:29:49
require these functions to be square integrable meaning normalizable meaning they have to go to zero at
7:29:55
infinity so zero at plus infinity zero at minus infinity this term all by itself drops out
7:30:01
oh and um i've got this coefficient overall that i should pull out front so minus i h bar
7:30:09
multiplies all of this so i've got a minus i h bar and a minus the minus i h bar and the minus are
7:30:14
going to cancel out if you want to simplify this and i'll have i h bar
7:30:19
let me put that inside the integral i h bar i have a partial derivative of f
7:30:25
star partial x and g and i'm integrating dx
7:30:30
so we're almost there this looks a lot like the momentum operator applied to the function f so we've almost sort of
7:30:37
closed the loop here we've almost shown that p is hermitian what's missing here well what's missing
7:30:42
is the notion of this minus sign on the ih bar this here itself doesn't look it is not
7:30:48
exactly the momentum operator applied to f but what we don't what we want actually isn't exactly the momentum
7:30:54
operator applied to f it's the momentum operator applied to f but then acting on the left in this inner product notation
7:31:00
which means we have to take the complex conjugate so if i really wanted to write this out i would have to say this is the
7:31:06
integral actually sorry i put some limits on here minus infinity to infinity
7:31:12
of minus i h bar partial f partial x all complex conjugated
7:31:19
multiplied by g integrated dx and now we've actually gotten back to this original expression
7:31:27
this here is the operator p acting on f complex conjugated inner product with the
7:31:33
function g so that's the end result we have sort of demonstrated by our that our definition
7:31:39
of minus i h bar partial g partial x here is indeed a hermitian operator and
7:31:44
perhaps this goes a little bit of the way towards explaining why exactly you had a minus i h bar or minus i in the
7:31:50
definition of the momentum operator that minus i is a little bit perplexing at first but it is it is required
7:31:56
essentially by the notion that the momentum operator be hermitian by the notion that the expectation value of the momentum is always going to be a real
7:32:02
number as a further example of how we can manipulate these sorts of things
7:32:10
in the language of formal linear algebra let's think about a state with no uncertainty
7:32:15
what sort of quantum mechanical state would have no uncertainty these things are also called determinate states
7:32:24
meaning you have some operator and all or some observable let's say the observable q as represented by the
7:32:30
operator q hat and it has absolutely no uncertainty associated with it this is there is a quantum mechanical state that has a
7:32:36
definite value of some mechanic or some some variable some now if you're thinking about something
7:32:42
like position or momentum you're you might be thinking along the lines of the uncertainty principle and well is that really possible and the answer is
7:32:49
probably not the states of determinate position and determinant momentum tend to be a little bit poorly behaved
7:32:55
mathematically speaking but in terms of energy perhaps you know states of determinate energy they are
7:33:00
the solutions to the time independent schrodinger equation so there's certainly nothing wrong with that
7:33:06
in particular i can write something like sigma q the variance sigma q squared the uncertainty
7:33:13
in a measurement of quantity q squared and i can write that this quantity was
7:33:19
back when we were talking about variance and probability distributions defined as the expectation value
7:33:24
of the operator q minus the expectation of q
7:33:30
so the deviation of some observable from its mean squared so
7:33:35
the expected mean squared deviation the mean squared deviation from the expected value
7:33:41
now in our language of interactive linear algebra here we can write this out as psi on the left
7:33:46
and then q hat minus the expected value of q acting on psi
7:33:52
on the right oh and this is squared of course
7:33:58
so i can expand out the square let's say psi on the left and then q hat minus
7:34:03
expected value of q q hat minus expected value of q twice
7:34:10
acting on psi and
7:34:15
this operator if this is going to represent an observable has to be hermitian so if q hat the operator is
7:34:22
hermitian q multiplication by a number here the expected value of q this is just going to be a number it's also of
7:34:27
course going to be hermitian multiplication by a number is going to be a hermitian operator it doesn't matter if you do it on the wave function
7:34:34
on the right or the wave function on the left i can take this whole thing
7:34:39
and apply it on the left since this is a hermitian operator
7:34:48
so if you make that sort of manipulation you end up with now on the left i have q
7:34:54
hat minus expectation of q acting on psi and on the right
7:34:59
q hat minus expectation of q acting on psi
7:35:05
and if this whole thing is going to have zero uncertainty what exactly does that mean
7:35:12
well if this whole inner product is going to turn out to be 0 then either psi equals
7:35:18
0 meaning my wavefunction is in some sense trivial that's not terribly useful if psi is not
7:35:24
0 then each individual piece here this has to in some sense be equal to zero or
7:35:30
this piece on the left has to be equal to zero what that means well either left and right these are
7:35:36
very similar expressions it means q hat minus the expected value of q
7:35:42
in terms of that as an operator acting on my state that has to equal zero and that's easily
7:35:49
rearranged into q hat acting on the state equals the expected value of q
7:35:55
multiplied by the state this is just a number it's not an operator this
7:36:00
here this is an eigenvalue problem
7:36:08
and there is a yet another massive set of linear algebra machinery dedicated to
7:36:13
solving eigenvalue problems we've already done some of them for example the hamiltonian operator acting on the
7:36:20
state of the system is the energy times the state of the system this is our time independent schrodinger equation
7:36:27
this gave us the states of definite energies and that's the same sort of framework as you got in here
7:36:34
so that's a taste of the sorts of things that we can represent and think about in the language of linear algebra as
7:36:40
applied to quantum mechanics we can express generalized states with no uncertainty and derive that they are
7:36:46
going to be the states that are eigenstates of the linear operators that represent the observables now we haven't
7:36:52
really written down any linear operators in detail in the notation of linear algebra or really in quantum mechanics
7:36:58
we've only really got a few operators that we can work with like hamiltonian position and momentum and whatnot
7:37:03
but this is hopefully hopefully i have at least convinced you that there's more to quantum mechanics than just dealing with the wave function
7:37:09
that we can do some interesting things with the linear algebra structure so to check your understanding here
7:37:16
let's consider a set of states that you get stationary states from the quantum harmonic oscillator that means the
7:37:22
solutions to the time independent schrodinger equation which if you wanted to write it out in terms of operators and linear algebra is h bar psi
7:37:29
sub i let's say let's say psi n actually is equal to e n psi n
7:37:35
so that would give us these this set of solutions here so in terms of the language of linear algebra some basic notational questions
7:37:41
and in terms of whether or not observable operators are hermitian or not think about why the operator or why the
7:37:48
operator x hat the position operator would be hermitian
Hermitian operator eigen-stuff
7:37:54
let's continue our discussion of the mathematical formalism of quantum mechanics by considering hermitian operators and the eigenvalue and
7:38:01
eigenvector problems that result from their consideration what we're talking about here is a
7:38:07
hermitian operator in general so for hermitian operator i'll just write q with some hat on it
7:38:14
and you can consider just this general operator to be hermitian if the following condition holds the inner product of
7:38:21
some arbitrary function arbitrary state in the hilbert space f inner product with the operator acting
7:38:28
on some arbitrary state in the hilbert space g is going to be equal to
7:38:34
the operator acting on the state f inner product with the state g so if
7:38:39
this inner product and this inner product are equal to each other for all f and g
7:38:48
then the operator is hermesian these sorts of operators show up a lot
7:38:54
in quantum mechanics because hermitian operators are what we are considering if we're talking about observable quantities in quantum mechanics
7:39:02
now in terms of eigenvalue problems the general statement of an eigenvalue problem looks like the operator applied
7:39:09
to some general state is equal to some eigenvalue which i'll write as lowercase q in the case of the operator uppercase
7:39:16
q multiplied by that state so applying the operator to the state doesn't really do anything it only changes the overall
7:39:22
scaling factor by some some amount q so these sorts of eigenvalue
7:39:29
problems show up in quantum mechanics all over the time they're all over the place for example the time independent
7:39:35
schrodinger equation is such an equation we have the hamiltonian operator acting on a state giving you the energy
7:39:42
multiplying the state h psi equals e psi now solving the eigenvalue problem
7:39:50
gives you one of two general kinds of solution first of all what
7:39:56
we're going to get are going to be eigenstates
7:40:01
those are going to be our size that solve this sort of equation generally we're going to get a lot of them
7:40:12
and we'll get some sort of eigenvalues those are going to be the values of q
7:40:19
that result from application of this operator to a particular solution to the eigenvalue problem and we're going to
7:40:25
get many cues as well each solution to this problem and there will be many generally has its own distinct value
7:40:32
of q in this sort of expression and the sets of size and the sets of
7:40:37
cues that solve these problems generally come in two discrete classes we have
7:40:43
discrete and we have continuous
7:40:49
the discrete case means that we have some explicit set of let's say psi sub n
7:40:56
there are a potentially infinite number of these psi sub n's but we can write them down in a list
7:41:03
psi 1 psi 2 psi 3 etc we're also going to get some set of q
7:41:08
sub n's where q sub n goes with psi sub n it's an example of where this has
7:41:15
occurred already that you've seen talking about the particle in a box solving the time independent schrodinger
7:41:22
equation gave us a set of stationary states and the associated energies
7:41:27
for the continuous case things are a little bit more complicated for an example of this that you've seen before consider something like the momentum
7:41:34
operator applied to the wave function giving you the momentum the value multiplied by the wave function this
7:41:41
sort of eigenvalue expression came up in our consideration of the free particle and under those circumstances we didn't
7:41:46
get a real nice set of solutions we got wave functions that look something like well there was some free parameter k
7:41:53
our wave function as a function of x looked something like a complex exponential we had e to the i k x minus
7:42:00
h bar k squared over 2m t for the time dependence
7:42:05
um probably we were dividing this by root 2 pi if i remember correctly to effectively normalize it within the
7:42:10
language of the fourier transform at least so there's no way of writing down psi 1
7:42:16
psi 2 psi 3 psi 4 there is only psi k and k can take on essentially any value
7:42:22
the eigenvalue that we got was well in the case of the momentum operator h bar k
7:42:27
so given the definition of k that we came up with in this consideration of the free particle we have an infinite set of
7:42:34
continuously variable solutions this k value can be anything as opposed to indexed by just an integer one two or
7:42:40
three sort of uh setup now the mathematics that results from a discrete spectrum a discrete set of
7:42:47
eigenvalues versus a continuous spectrum a continuous set of eigenvalues are
7:42:52
going to be a little bit different but it's a little easier to understand the discrete case it's a lot easier to write down mathematical expressions so let's
7:42:58
consider that case first most of the results will still hold and we'll come back to the continuous case later on in
7:43:04
lecture so the first thing that you probably want to know about the eigenvalues that
7:43:09
result from these eigenvalue problems is whether or not they can possibly represent observables and in this case
7:43:15
the eigenvalues of hermitian operators are real you can see that by fairly straightforward application of the
7:43:20
eigenvalue equation itself looking at q hat the operator applied to some arbitrary wavefunction psi giving you
7:43:27
the eigenvalue q multiplied by the wavefunction psi you can take the complex conjugate of
7:43:33
that expression and complex conjugating the left-hand side merely converts this into well the
7:43:39
result of complex conjugating the operator acting on the wave or acting on the state which we're writing in our vector
7:43:44
notation as angle bracket on the left instead of angle bracket on the right complex conjugating the right hand side
7:43:51
of this expression gives you well the complex conjugate of the eigenvalue q star
7:43:56
uh multiplied by the result of complex conjugating this wavefunction or this state psi so again angle bracket on the left
7:44:04
the other ingredients to understanding why the eigenvalues of hermitian operators are real is the definition of
7:44:10
a hermitian operator which says that q acting on some state f
7:44:16
inner product with sums with the same state f perhaps is going to give you the same result as
7:44:21
if you take the inner product of the state f itself with the operator acting on the state f on the right operator on
7:44:27
the left operator on the right gives you the same result now if i apply this sort of expression
7:44:34
over here and this sort of expression over here
7:44:40
you can see what's going to happen applying the operator on the left turns this into
7:44:46
q complex conjugate f inner product with f and applying this expression on the
7:44:52
right turns this part into q the number multiplying f
7:44:58
now a number inside an inner product like this is just going to factor out so we're left
7:45:04
with q the number times f inner product with f
7:45:09
and the inner product of a state with itself is always going to be non-zero
7:45:15
so i can effectively divide both sides of the equation by this and thereby show that q star is equal to q therefore our
7:45:23
eigenvalues of the eigenvalue problem for a hermitian operator is going to be a real number
7:45:31
real numbers means that these are potentially feasible representations of observable quantities
7:45:38
so that's a step in the right direction now we talked about a lot of other facets of solutions for the time
7:45:44
independent schrodinger equation for example what about orthogonality and normalization and what not
7:45:50
we can talk about those within the language of eigenvectors and eigenvalues eigenstates of a hermitian operator
7:45:56
it turns out the eigenstates of a hermitian operator are orthogonal to each other
7:46:02
now that's not a completely rigorous mathematical statement i'll point out some of the difficulties with it later on
7:46:07
but in the context of orthogonality we're talking about an inner product of two different states so suppose i have
7:46:15
q hat and i'll say the state f gives me some eigenvalue qf
7:46:22
multiplying the state and then i have a distinct state q let's call it g
7:46:28
gives me the eigenvalue g sorry q g
7:46:33
multiplying the state g these two eigenvalue problems are solved
7:46:39
for the state f and for the state g so in principle i know f i know g or q f i know g i know q g
7:46:48
now if you consider the definition of a hermitian operator in the context of the states f and g
7:46:53
i have f acting on q
7:46:59
times g and that has to be equal to q acting on
7:47:05
q acting on f inner product with g this is our definition of a
7:47:11
hermitian operator and we know considering eigenvalues and our
7:47:17
eigenvalue problems here qg i can write down that that's just going to give me q sub g times the state
7:47:23
g so this is going to give me qg times the inner product of f and g
7:47:30
and qf on the left we've talked about how to do that sort of thing on the last
7:47:35
slide this is just the complex conjugate of this sort of thing so this is going to give me qf
7:47:42
complex conjugated times the inner product of f and g
7:47:48
now this looks a lot like the sort of expression we were talking about before but in the case of showing that the eigenvalues were purely real we were
7:47:54
working with the state f and itself not the state f and some other state g
7:48:01
so we have some potential problems with this expression if
7:48:07
qg and qf are not equal to each other
7:48:12
and f and g the inner product here is non-zero then we have
7:48:17
the same expression on both sides can be divided out qg is equal to qf but that's
7:48:23
going to cause some problems the problem that we run into is that we have
7:48:28
a failure of our our inequality here and the the inequality that fails if i say divide these things out qg if qg is
7:48:36
different than qf then i have a contradiction the contradiction is that f and g are not
7:48:42
don't have non-zero inner product if f and g has zero inner product i can't just divide it out because i'm dividing both sides of my equation by 0.
7:48:49
so what we can conclude from this expression is that either
7:48:54
f g is equal to 0 or
7:49:00
qg is equal to qf and i'll just say qg is equal to qf since we've just shown that the
7:49:06
eigenvalues are real qf-star is equal to qf so we've shown that if the eigenvalues
7:49:13
are different from each other then the inner product can be there must be zero if the eigenvalues
7:49:19
are the same we are not guaranteed that the uh eigen states f and g will be
7:49:26
orthogonal to each other in the case that qf equals qg we describe the state the
7:49:34
eigenvalue as degenerate and we have to go through some extra
7:49:40
procedures in order to ensure that we have a well-behaved set of eigenstates
7:49:45
in particular what we want to do is something called gram-schmidt orthogonalization
7:49:58
aside from having a lot of letters in it uh orthogonalization is simply the process of taking these
7:50:04
two states f and g and converting them into two new states
7:50:10
f prime and g prime that are constructed as superpositions of f and g
7:50:16
such that they are actually orthogonal i won't go into the details here but it
7:50:22
has to do essentially with finding the component of the vector f that is not
7:50:28
orthogonal to the vector g and subtracting it off of the original vector f so that i only have the part of
7:50:34
f that is orthogonal to g left over when i've computed f prime
7:50:41
so that's a little bit about the eigen functions in terms of their orthogonality the other thing that we
7:50:47
needed to be able to compute meaningfully in quantum mechanics is completeness we
7:50:52
needed to represent states arbitrary states as superpositions of for instance
7:50:58
stationary states solutions to the time independent schrodinger equation for
7:51:03
say the quantum harmonic oscillator in the language of linear algebra the mathematical formalism of quantum
7:51:08
mechanics and that's an eigenvalue problem with the hamiltonian operator and it turns out that we have the same
7:51:15
sort of mathematical formalism there the eigenstates of hermitian operators are indeed complete
7:51:21
and i can't really say much more here than just give you a definition in terms of the completeness we're talking about our eigenvalue problem as before
7:51:30
giving us a spectrum of stag and states let's say size of n
7:51:36
and the resulting set of eigenvalues and it turns out that this is indeed a
7:51:43
complete basis within the language of linear algebra the set of vectors here spans the
7:51:49
complete space that you're working with and what that means is that any arbitrary state let me call it f
7:51:58
can be written as a superposition let's say n equals one to infinity here of some coefficient
7:52:05
n multiplying psi n so i can express any vector in my vector
7:52:12
space as a superposition of this set of vectors it forms a complete basis
7:52:18
that spans any desired function that you would be interested in
7:52:24
you can given the orthogonality of these states as shown in the last last slide
7:52:30
apply fourier's trick to this sort of expression and to determine that this a sub n coefficient is fairly
7:52:36
straightforward to calculate you just multiply from the left by size of n um
7:52:42
take the inner product with the state that you want to represent now it's important to note that
7:52:48
this sort of statement is not on as solid and mathematical footing as the earlier states regarding orthogonality
7:52:55
the completeness is often not easily proven it is typically going to be something that we assume
7:53:01
and while in the case of consideration of the wave function we can write down the time independent schrodinger
7:53:06
equation as a partial differential equation and apply the language of stormley oval theory and apply the results of storm
7:53:13
renewable theory in particular to show the results are completed the set of solutions to the time independent
7:53:18
schrodinger equation forms a complete set of basis functions the same sorts of results are typically
7:53:23
going to apply here so while we can't always prove it we are generally going to assume it certainly
7:53:29
at the level of mathematical sophistication of a course like this so that's about it for the results
7:53:36
one thing i did want to say before we close here is that all of what i've been stating so far are for discrete spectra
7:53:42
so what about continuous spectra what if instead of getting a discrete set of eigenstates and eigenvalues i get a
7:53:49
continuous set of eigenstates and eigenvalues the example i gave earlier was
7:53:54
consideration of the momentum operator as an eigenvalue problem if i have some arbitrary function apply
7:54:01
the momentum operator and get that same function back the solutions that we got looked
7:54:07
something like e to the i k x minus h bar k squared over 2 m t
7:54:15
with eigenvalues that look like h bar k now first problem this is not
7:54:22
normalizable so within the language of linear algebra writing down something like if i call this
7:54:29
psi sub k in the language of linear algebra writing down something like psi k psi k
7:54:35
what exactly sense does that make can i really say this is normalized well
7:54:40
if i have two different or two different values of k
7:54:47
let me say express this in terms of momentum instead so i'll write this as a
7:54:53
psi sub p if you consider say psi p1 inner product with psi p2 what does the
7:55:01
orthogonality actually look like orthogonality or normalization
7:55:06
well if you write this out in the language that we know that we've been working with so far that of wave functions this is going to be the
7:55:12
integral from minus infinity to infinity of well this sort of expression first of all i've got
7:55:18
psi sub p1 complex conjugated so that's going to be e to the minus i
7:55:23
k 1 x minus h bar k 1 squared over 2 m t
7:55:29
multiplied by e to the plus i k2 x minus
7:55:36
h bar k2 squared over 2m t this is all going to be integrated dx
7:55:42
from minus infinity to infinity so in the case that p1 equals p2 meaning
7:55:48
this is really the same state then the exponential argument here is the same but has opposite sign so i've
7:55:54
got e to the plus something times e to the minus something which is just going to give me 1. i'm going to
7:56:00
get the integral of minus infinity from minus infinity to infinity of 1 dx
7:56:05
what now that's going to be infinity surely right it's not a very meaningful expression but it's going to give me something very
7:56:11
large now what if let me move this over a little bit to the left
7:56:16
what if i consider p1 not equal to p2 then well in that case my integral here is
7:56:22
going to have some function of x k1 k2 are going to be different
7:56:27
in the subtraction here that i get if i combine these two things i'm going to get some function of x it's going to be like the integral of minus infinity to
7:56:34
infinity of e to the i something x
7:56:39
there's going to be other stuff up here as well but i've got this sort of oscillatory behavior you can think of
7:56:44
this as cosine plus i sine in other words now as far as formally defining this
7:56:50
mathematically what this makes what this limit says you've got an oscillatory function you're integrating it all the way to infinity it's not going to go to
7:56:57
infinity it's going to oscillate right and it's going to oscillate about zero it's going to average out to zero so in
7:57:03
some sense we can say this sort of goes to zero and i should really put this in quotes so that i don't make my inner mathematician too angry
7:57:12
we do know however from working with these in the past that these do form a complete basis
7:57:17
these sorts of things can be used to express any arbitrary initial conditions we talked about that in the context of
7:57:23
the free particle when we wrote expressions like that the wave function psi of x say can be written as an
7:57:29
integral from minus infinity to infinity of decay
7:57:34
some coefficient phi of k multiplied by say e to the ikx
7:57:41
these sorts of expressions this is like the inverse fourier transform of psi sub k so given some suitable definition of
7:57:47
psi sub k these e to the i k x these sorts of functions these sorts of functions can actually
7:57:53
represent pretty much anything that you might want now if we substitute in
7:57:59
our definition for fee of k back from when we were talking about these sorts of things
7:58:05
it looks like this it's the integral from minus infinity to infinity dk from before and our phi of k was itself an
7:58:12
integral from minus infinity to infinity this time it was an integral dx
7:58:18
and it was sorry let's
7:58:23
not leave it as an integral dx because i've got x in this expression as well let's use a dummy variable my usual squiggle c
7:58:30
integral dc of psi of c e to the minus i k z
7:58:39
so this sort of expression that was our definition of phi sub k if i multiply this by e to the i k x
7:58:46
continuing my expression over here you end up with something that makes a certain amount of sense
7:58:52
in particular i can manipulate this let's consider exchanging the order of integration here and manipulating these
7:58:59
such that my exponentials multiply together you can think of this as the integral from minus infinity to infinity
7:59:06
dc first and then the integral from minus infinity to infinity dk
7:59:13
e to the i combining these two things together i'm going to get something like
7:59:19
k x minus k c
7:59:25
and all of this is going to be multiplied by psi of c
7:59:33
now if this whole thing is going to be equal to psi of x this expression right here
7:59:39
should look familiar what function gives me psi of x when multiplied by psi of a dummy
7:59:44
variable and integrated over the dummy variable this function here this guy
7:59:51
we have a name for it it's delta of x minus c
7:59:57
or c minus x so this sort of delta function
8:00:03
this is what i'm really going to get out of these sorts of normalization
8:00:08
conditions the infinity that it goes to when p1 equals p2 is like the infinity that the delta function goes to when x
8:00:14
equals zero or x minus c or x equals c the zero that it goes to is like the
8:00:20
zero of the delta function when its argument is non-zero so subject to this
8:00:27
version of orthonormalization that if p1 is not equal to p2 you get 0
8:00:34
and if p1 is equal to p2 you get well infinity but infinity in a useful way such that in
8:00:40
the context of integration i can get functions out that i would exp as i would expect you can prove the same sorts of results
8:00:46
for an eigenvalue problem with a continuous sort of spectrum
8:00:53
that's all about the that's about all that i want to say about these sorts of topics to check your understanding let's
8:00:59
consider the position operator x hat is it hermitian uh what is the spectrum
8:01:04
like is it continuous or discrete what are the eigenfunctions of x the operator and do those eigenfunctions
8:01:11
form a complete basis so think along those lines and um hopefully that will help solidify
8:01:18
this notion of the mathematical formalism that we've been working with in the language of her in the context excuse me of hermitian
Statistics in formalized quantum mechanics
8:01:26
the formal mathematical structure of quantum mechanics can also of course be applied to determine the statistics
8:01:32
perhaps of measurements made of quantum mechanical systems these notions of statistics appear a lot in the context
8:01:37
of uncertainty for example variance and the overall average outcome the expectation value so let's consider how
8:01:43
the formal mathematical structure of states in a hilbert space can be used to determine statistical properties of
8:01:49
quantum mechanical what we're talking about here is some observation so consider just some
8:01:55
generalized observation meaning i'm talking about some observable q as represented quantum mechanically as an
8:02:02
operator q hat we've talked about over the last
8:02:07
couple of lectures eigenvalue problems q hat applied to some state gives me q
8:02:12
the eigenvalue multiplied by that state and we've talked about the results of
8:02:18
these eigenvalue problems either we have a discrete spectrum we get some sort of set of size of n's
8:02:25
associated with some q sub n eigenvalues from which we can construct for instance
8:02:32
any arbitrary state f for example has a superposition of a bunch of stationary
8:02:38
states a bunch of states here a bunch of psi sub n's multiplied by some sort of a coefficient
8:02:44
and we can determine that coefficient with fourier's trick left multiplying this overall expression
8:02:51
by a particular psi sub i so a sub i is going to be given by psi sub i
8:02:57
f coming from the left hand side the sum on the right hand side collapses etc the usual fourier's trick reasoning applies
8:03:03
involving the ortho orthogonality of the size of ends we have this nice set of mathematical
8:03:09
tools that we can use we have a set of vectors that forms a complete basis for arbitrary functions these are
8:03:15
orthonormal basis vectors basis states and they can be used to construct
8:03:22
anything we also talked a little bit about what happens if you get a continuous set of
8:03:28
solutions not a discrete set so let me just write this as some arbitrary psi of q
8:03:34
i'll write this as a state it looks sort of like a function and a state think of this as a state that depends continue on
8:03:39
some continuous parameter q so each value of q plugged into some general structure gives me a distinct
8:03:46
state and i can think about the eigenvalue as q uh under those circumstances the
8:03:52
completeness of the basis states can be expressed as an integral so i'm
8:03:57
constructing the same sort of general quantum mechanical state as an integral over q
8:04:04
of some sort of coefficient let me write it as f of q multiplying this state psi of q so i
8:04:12
have some general function multiplied by some general coefficient and i'm integrating up if i have some sort of continuous spectrum of eigenstates and
8:04:20
eigenvalues this f of q is determined by fourier's trick using
8:04:26
the dirac orthonormalization of these sorts of states in much the same way it's
8:04:32
again going to be an inner product of psi of q with the state that we're trying to find
8:04:39
or with the state that we're trying to represent excuse me now given this sort of mathematical
8:04:45
structure can we discuss the notion of measurement or some sort of an observation what happens
8:04:51
when we measure q we've got some sort of device we've put
8:04:56
our quantum mechanical system into it and it spits out a number what numbers is it likely to spit out well in the
8:05:02
discrete case here it's actually quite straightforward you are going to get one of these eigenvalues this is the
8:05:10
generalized statistical interpretation of quantum mechanics you're going to receive one of the q sub n's in that set
8:05:17
of q sub n's i should probably use a different index here one particular value from that set
8:05:24
and you're going to get it with probability
8:05:30
given by well if i'm looking if i get value q sub n i'm going to get it with probability a
8:05:36
sub n squared so the coefficients that appear in this expansion this representation of the
8:05:42
state in terms of these basis vectors uh is really the
8:05:47
well square root in some sense of the probability of receiving each particular
8:05:52
eigenvalue so this is actually quite an interesting statement when we measure q in a system
8:05:58
with a discrete quantum mechanical spectrum we always get one of the eigenvalues of the operator
8:06:04
corresponding to the observable that we measure and we get that value with probability
8:06:10
given by this very simple sort of formula you can take the squared magnitude of essentially what you're
8:06:15
looking at here is the part of f that is in the psi sub i direction if you want
8:06:20
to think about it
8:06:25
there is of course a continuous counterpart to this but measurements of a continuous spectrum are a little bit
8:06:30
more subtle you have to think about what it means to observe something and you're never going to if you're trying to
8:06:36
compute the probability of getting say exactly 6 out of a continuous distribution exactly 6 will never happen
8:06:43
you will only get over numbers very very close to six but you can think about what's the probability that i get some value
8:06:50
q in between q zero and q zero plus some dq so i've got some
8:06:56
sort of interval here between q zero and q plus q 0 plus dq if the value that i get
8:07:02
falls in that range then we can represent the probability here and you'll get it with probability
8:07:11
given by the magnitude of f of q squared multiplied by dq
8:07:19
so this f of q this coefficient that we determine is the result of an inner product with our sort of basis and the
8:07:26
function that we're state that we're trying to represent can be used as a probability this is the
8:07:32
sort of thing that we're talking about when we talk about the say the squared modulus of the wave function as a probability density the wave function
8:07:39
psi of x is really the result of some sort of inner product
8:07:45
between eigenfunctions of position operator which are direct delta
8:07:51
functions as applied to the uh the state that we're trying to represent
8:07:58
so that's that's where the probability density comes from now this is not so much a mathematical result that can be
8:08:04
proven these sorts of you'll always get an eigenvalue and you get some sort of a probability
8:08:09
or you'll always get some sort of a continuous value some sort of a value with this sort of probability those aren't mathematical results as much as
8:08:15
they are sort of axioms of quantum mechanics this is a generalized statistical interpretation that takes us beyond the notion of the wave function
8:08:22
as something that gives you the probability density of position measurements meaning the probability density of where you're likely to find
8:08:28
the particle if you observe the particle meaning observe its position
8:08:34
so these sorts of probabilities are of course going to be useful in the context of computing probabilities but in order
8:08:40
for them to be useful in computation of probabilities we first of all have to have some sort of normalization
8:08:46
now you can think about normalization of a wave function or of a state in the
8:08:51
context of these vectors in the hilbert space as the inner product of the state itself must equal 1.
8:08:57
now if you think about that in the case of a discrete spectrum this state f can be written as the sum of some a sub n
8:09:04
times some psi sub n meaning if i'm working with some set of psi sub n functions some sort of a basis
8:09:12
i can figure out the overall i can figure out these coefficients and
8:09:17
determine the overall state that i'm trying to represent
8:09:22
if you look at this inner product in this context
8:09:31
you're going to have well it's an infinite sum and an infinite sum so i've got some sort of sum over n of a sub n
8:09:39
star size of n on the left and some sort of an infinite sum over n
8:09:44
or sorry i should use m different index of a sub m size of m
8:09:51
and if i distribute these two infinite sums together i'm going to get psi n psi m terms
8:09:57
and psi n and psi m those inner products obey an orthogonality relationship i'm
8:10:02
assuming these psi sub n's come from the eigen states of a hermitian operator
8:10:09
so the orthogonality is going to collapse the two sums together and i'm just going to have one sum left i'll get
8:10:14
say a sum over n of a sub n star a sub n
8:10:20
and the normalization means psi n psi n inner product is one so my wavefunctions
8:10:25
are gone so this normalization condition here
8:10:30
implies that the sum of the squares of those coefficients in the representation of my
8:10:37
state is going to be one in the language of
8:10:44
continuous spectra what we're talking about here again is an inner product inner products you can
8:10:50
think of as a integrals so we've got some sort of an integral of some sort of f of q
8:10:56
squared modulus dq this is again sort of an addition of all probabilities well we've got an addition of probabilities
8:11:02
here a summation of a bunch of probabilities that better add up to one this is an integral of a bunch of probabilities that adds up to one and
8:11:08
this integral comes from the same sort of orthogonality argument as uh the infinite sums collapsing here instead of
8:11:14
two infinite sums multiplied together we would have two integrals which we could manipulate to
8:11:19
get a dirac delta function in terms of the dirac orthonormalization of these sorts of basis states
8:11:26
what i wrote as a psi of q on the last slide so these normalization conditions make a
8:11:33
fair bit of sense probabilities have to sum to one we can we can make some use of that
8:11:41
another situation where these probabilities are useful is in the computation of a computation of
8:11:46
an expectation value so say i want to compute the expectation value of some arbitrary operator q
8:11:52
that in the language of these linear operators is f inner product with q
8:11:59
times f q operator acting on f so here's my arbitrary state f again
8:12:05
and q being applied to f so again i can make these sorts of infinite sum expansions sum over n of a
8:12:12
sub n star not f excuse me psi sub n
8:12:18
multiplied by an infinite sum over m of a sub m
8:12:25
times q acting on f sorry not f
8:12:31
once again psi sub m excuse me
8:12:37
coming from this same sort of expansion of f and the expansion of q f so the expansion of q f is going to be q acting
8:12:44
on the infinite sum and i've distributed q into that infinite sum acting on each individual term
8:12:52
now q acting on psi sub m that was my original eigenvalue equation
8:12:58
q acting on psi sub m is simply going to give me q multiplying sides of m
8:13:05
so in the case of calculating the expectation value of some general operator when you have your
8:13:11
general state represented in terms of eigenstates of that operator is actually
8:13:17
quite simple again we're going to get psi n and psi m when i distribute these two sums together
8:13:23
you're going to have a sum over n and a sum over m i'm going to have an a
8:13:28
n excuse me that looks a little bit like a w a n star
8:13:34
and an a m and a q this is technically going to be q sub m excuse me
8:13:42
associating psi sub m with q sub m was part of the definition of these size of m's
8:13:48
and i have a size of n and a size of m which again
8:13:53
i can say this is some delta n m which collapses my sum down and what i'm going to get in the end
8:14:00
is a sum just over a single variable let's say n times the squared modulus of
8:14:07
a sub n times q sub n so this if you look at it from the
8:14:12
perspective of statistics this is a weighted average these are the probabilities associated
8:14:18
with each observation and this is these are the values that are associated with each of those probabilities
8:14:26
you can do the same sort of thing within the context of a continuous spectrum
8:14:31
under those circumstances you're going to have um i'll write it out in this under these circumstances the expectation value of q for a continuous
8:14:39
case is the integral from minus infinity to infinity let's say i've got
8:14:45
dq uh right so i'm constructing an integral representation of f
8:14:52
so let's say that's going to be an integral over q1 f of q1 i have to complex conjugate this
8:15:01
so this is my coefficient from the integral from the representation of f complex conjugated
8:15:07
and then i've got my psi of q1 actual function
8:15:12
and definitely running out of space here shift this to the left a little bit
8:15:17
and that whole thing is going to be multiplied by a similar looking integral except this time i'm going to be
8:15:24
representing q applied to f so this is going to be an integral d q 2
8:15:29
to use a different variable i'm going to have a coefficient f of q 2 again appropriate for representation of my
8:15:35
state i'm going to have my operator q multiplying my
8:15:40
psi of q2 and close my state and close my parentheses off screen hopefully that's
8:15:46
reasonably clear in terms of at least my handwriting this is a representation of
8:15:52
this and this is a representation of q applies to that
8:15:58
you can make the same sort of arguments here q applied to my state is going to be q 2
8:16:03
in this case times psi of q2 that's my eigenvalue operation and then i have the same sort of double
8:16:10
integral becoming a delta function sort of thing as i had a double sum becoming a kronecker delta over here
8:16:17
so this is going to give me rearranging the order of these integrations a little bit integral minus infinity to infinity
8:16:22
dq 1 integral minus infinity to infinity dq
8:16:27
2 and then i've got an f star of q1
8:16:33
and an f of q2 and q2
8:16:40
and an inner product of psi of q1 and psi of q2
8:16:47
and subject to these dirac orthonormalization constraints that we have to have in order to make continuous
8:16:52
spectra really make any sense this is going to be a dirac delta function of q1 minus q2
8:17:01
applying that dirac delta function in this integration means i can do one of these integrals and what i'm going to
8:17:07
get is the value of the integrand such that or that occurs where the
8:17:12
argument of the delta function is 0. so if i'm doing the integral dq2 i'm going to get the value where q2 has
8:17:19
become q1 so all you're going to be left with is a single integral minus infinity to
8:17:25
infinity dq1 and i've got an f star of q1 as before
8:17:31
and an f of q not two anymore excuse me f of q one
8:17:37
this q two becomes q one is basically the whole point of applying the delta function this is the result of
8:17:42
doing a delta function integral i've also got that q1 laying around from before and that's it that's all there is
8:17:49
to it so this getting a little cramped in the right is the integral from minus infinity to
8:17:56
infinity of that squared modulus of f of q dq
8:18:03
multiplied by sorry not dq let's say multiplied by q integral dq
8:18:08
so this is the same sort of expression here as you have here it's a squared modulus times the value squared modulus
8:18:15
times the value properly normalized given the dirac orthonormalization and the kronecker delta sort of orthonormalization of
8:18:21
these two sorts of sets either we have a discrete spectrum in which case things are infinite sums or we have a continuous spectrum in which case things
8:18:28
are integrals so that's what your expectation values are going to look like they're going to
8:18:33
be sort of weighted averages with sums or weighted averages with probabilities
8:18:38
yeah with continuous functions as computed in integrals you've seen expressions like this before and for example the
8:18:45
computation of the expected value of the position operator this is going to be an integral over position multiplied an
8:18:51
integral over position multiplied by the position multiplied by sorry this should be squared the
8:18:57
squared magnitude of the probability density by of the wavefunction f of x
8:19:04
now of course all of this is expressed in terms of some general operator q so let's do an example let's think about
8:19:11
measuring the momentum for the quantum harmonic oscillator ground state now measurements of momentum means we're
8:19:17
talking about the momentum operator we know we're always going to get one of the eigenvalues of the momentum operator
8:19:23
so we have to in principle solve the eigenvalue problem momentum operator applied to some
8:19:29
arbitrary state gives me the momentum the number multiplied by the state
8:19:34
and solving that eigenvalue problem is something we've done you end up with something like e to the i p x over h bar
8:19:41
divided by square root of 2 pi h bar i think goes in the denominator as well
8:19:46
associated with eigenvalue p so these are my eigen states expressed as wave functions and these are my
8:19:53
eigenvalues of those wave functions now we've talked about these things before this was e to the ikx over over root 2
8:19:59
pi and this was k h bar how can we determine for example
8:20:06
what the probability distribution of momentum measurements is going to be
8:20:12
for a particle prepared in the ground state of the quantum harmonic oscillator
8:20:17
well we're going to get some value p and we're going to get it with probability
8:20:24
given by the magnitude of some function f p squared
8:20:29
all right we're not going to get p we're actually going to get something between p naught and
8:20:34
p naught plus delta p running out of space here
8:20:42
but the the language sort of makes sense i have some sort of a
8:20:48
probability density multiplied by the size of the interval over which i am accepting values of p from p naught p
8:20:54
naught plus dp and that's my sort of probability density now within the language of the linear
8:21:00
algebra that we're working with this function f of p is going to be that
8:21:06
psi of p function think about that as the complex conjugate of this multiplied by psi zero
8:21:12
oh sorry not psi 0 being the ground state of my quantum harmonic oscillator
8:21:18
and you can write out this inner product in terms of wavefunctions if you know what these things are minus infinity to
8:21:24
infinity i'm integrating dx and i have my psi sub p
8:21:30
on the left meaning complex conjugated so this is going to be e to the minus i
8:21:36
p x over h bar divided by root 2 pi h bar
8:21:44
and then i have my quantum harmonic oscillator ground state and we found that in a variety of ways it looks something like m omega over pi h bar
8:21:51
raised to the 1 4 power times e to the minus m omega
8:21:57
over 2 h bar x squared so i have an integral dx
8:22:04
of e to the minus x squared and e to the ipx we've done this problem before this is
8:22:09
computing the fourier transform essentially of your your ground state this fourier transform is essentially a
8:22:16
special case of the sort of transforms that we are making when we compute the sort of coefficients that appear in the
8:22:23
expansions or representations of some arbitrary state in some arbitrary basis in this case we're working with the
8:22:29
eigenstates of the momentum operator we could also be working with eigenstates of the kinetic energy
8:22:35
operator or eigenstates of any other hermitian operator they're all going to form a complete orthonormal basis for
8:22:41
which these sorts of probability calculations work
8:22:46
um so this integral is doable not all that difficult you end up with another gaussian just as a function of
8:22:52
momentum it's a sort of closed-form mathematical expression so to check your understanding of these
8:22:58
sorts of probabilistic interpretations or these probabilistic contexts the results of here as they result from
8:23:06
the linear algebra in quantum mechanics suppose you're considering a particle in a box so we're solving the time
8:23:11
independent schrodinger equation for the hamiltonian this which is an eigenvalue problem for the hamiltonian operator we
8:23:17
get a set of stationary states and a set of eigenvalues now suppose i'm telling you that some
8:23:22
arbitrary state psi is prepared in this superposition of psi 1 and psi 2.
8:23:28
answer these questions if you measure the energy what's the probability of observing one of a couple of different
8:23:33
energies double check that this oops this shouldn't be f sorry i don't know why i
8:23:38
always manage to make typos in these check your understanding questions this should be psi
8:23:45
is the inner product of psi with itself what you expect it to be does it make sense and suppose i had some general
8:23:51
observable with eigenvalues and eigenvectors such that i have some eigen state g7 which gives me eigenvalue q7
8:24:00
if i observe q uh write down an expression for what i would expect in terms of the probability of getting q7 as a result of that
8:24:07
measurement so that's a bit on the statistical interpretation of
8:24:13
formal of the formal mathematical structure of quantum mechanics this basis allows us to construct
8:24:18
probabilistic interpretations of way more than just position and momentum and we'll continue on along those lines uh
8:24:25
far more later on in the rest of the course given our discussion of the formal mathematical structure of quantum
Generalized uncertainty principle
8:24:31
mechanics let's think about the uncertainty principle usually we're talking about something
8:24:37
like delta x delta p is greater than equal to h bar over two under those circumstances but can we do better can
8:24:43
we expand this beyond simple position momentum uncertainty the linear algebra structure of quantum
8:24:49
mechanics gives us a way to do that what we're talking about here basically
8:24:54
is the uncertainty in some observable quantity i'll leave it general and say q here meaning we have some sort of a
8:25:00
hermitian operator q hat that we can use when we're talking about making measurements
8:25:06
the uncertainty in that physical quantity usually expressed as the variance sigma sub q squared is
8:25:12
expressed as an expectation value so this outer pair of angle brackets is our usual representation or usual notation
8:25:17
for expectation value what we're computing the expectation of is a quantity that's squared so this is the
8:25:23
mean squared deviation from the mean q hat minus the expectation of q now this
8:25:30
looks a little bit odd we have one pair of angle brackets giving us the expectation of q that's just some sort of a number we can
8:25:36
determine that before we even start computing and then we have the outer pair of angle brackets that's going to give us the
8:25:42
expectation of this overall expression q minus the expectation of q
8:25:47
let me simplify the notation a little bit here and write this number as just mu sub q so this is the mean of q so this is the deviation from the mean
8:25:54
squared this is the average mean squared deviation that's our normal definition of the variance
8:25:59
now you can expand this out using our notation for things like expectation values in the linear algebra structure of quantum mechanics we have some sort
8:26:06
of a wave function q hat minus mu q squared acting on the wave function so
8:26:12
this as an operator we've got the operator q we've got the operator mu q mu q treated as an operator just
8:26:18
multiplies by mu it's like saying 6 as an operator is just going to multiply the wave function by 6.
8:26:24
you can expand this out psi on the left q hat minus mu q
8:26:32
q hat minus mu q uh acting on psi
8:26:38
and at this point you can look at this and say well q as represented by q hat in quantum
8:26:43
mechanics this q hat is going to be a hermitian operator since we're talking about an observable queue
8:26:50
and hermitian operators can act either to the left or to the right so let me take this q hat minus mu q
8:26:56
also of course going to be hermitian because this is going to be a real number this is going to be a hermitian operator the difference is just going to
8:27:02
behave itself as a hermitian operator let's have this one act on the left
8:27:07
leaving this one to act on the right what i get then is going to be the result of having q hat minus mu q
8:27:14
act on the left inner product with the result of having q hat minus mu q
8:27:21
act on the right so this is uh just a sort of
8:27:27
straightforward manipulation of the expression for the uncertainty in some observable quantity q
8:27:34
now you've got the same sort of thing on the left as on the right let's look at this and let's say this is
8:27:39
some vector f and this is well then it's going to be the same vector f this overall here is
8:27:45
going to act as just an inner product f inner product with itself i've got
8:27:51
these two variables or this vector which happens to appear twice so whatever this vector is i
8:27:56
hesitate to call it the state of the system but it is a vector in the hilbert space as a result of applying a hermitian operator to a state
8:28:03
and you can you can write that down just this is a definition of f
8:28:14
now in the context of uncertainty principles we can always have determinant states
8:28:19
any of the eigenvalues of q or eigenstates of this hermitian operator q are going to have certain value of q so
8:28:26
it's certainly possible for sigma sub q to be equal to zero
8:28:31
but if we have a second observable that's where we start talking about uncertainty principles so suppose i have
8:28:37
a second operator or a second observable quantity r as represented by some hermitian
8:28:43
operator r hat i can use that to construct sigma sub r squared
8:28:48
in exactly the same way as this substituting r for q everywhere in this expression
8:28:54
and when you get down to it instead of calling that f let me call that g
8:29:04
so if we have two separate operators there's nothing to prevent me from making this manipulation for both of them
8:29:10
which means what we're talking about in the language of the uncertainty principle as motivated by that delta x delta p
8:29:16
structure we're talking about something like sigma q squared sigma r squared
8:29:21
that's going to be equal to well it's this f inner product with itself g inner product with itself just
8:29:27
multiplied together this is sigma q squared this is sigma r squared this is sigma q squared this is sigma r squared
8:29:33
uh that should be fine so what can we do with this we've got f and we've got g this is where things get
8:29:40
a little bit subtle but the overall derivation here is not terribly mathematically complicated you
8:29:47
just have to pay attention as things go past so we've got this sort of expression what can we do with it
8:29:53
there are two simplifications that are going to turn this equality into an inequality and convert it into a form
8:29:59
that is useful from the perspective of the uncertainty principle the first of those simplifications working with this ffgg
8:30:07
expression for two general vectors in our hilbert space f and g
8:30:12
is the schwarz inequality
8:30:17
now the schwarz inequality is just a relationship between any sort of vectors like this it says that if
8:30:23
i've got the inner product of a vector with itself multiplied by the inner product of another vector with itself
8:30:29
that inner product is always going to be greater than or equal to the absolute magnitude of the inner
8:30:35
product of the vectors with each other squared you can think about this inequality very
8:30:41
simply from the perspective of three-dimensional vectors in three-dimensional space the inner product then is the dot product and what
8:30:47
this tells you is that the dot product of two vectors squared a dot b quantity squared is always going to be less than
8:30:54
or equal to the magnitude of a squared times the magnitude of b squared and if you're used to thinking about vectors
8:31:01
like a dot b in the normal sort of notation you've probably seen the formula magnitude of a magnitude of b
8:31:06
times the cosine of the angle between them now since we're working in an infinite dimensional vector space things
8:31:11
like the angle between them is somewhat difficult to define but this is the same sort of expression if i dropped the
8:31:16
cosine and made this into an inequality meaning the right hand side without the
8:31:22
cosine is always going to be greater than or equal to the left-hand side and then i were to say square both sides
8:31:28
here you would end up with the same sort of overall expression magnitude of a squared magnitude of b
8:31:33
squared magnitude of dot product squared so that's just an analogy the schwartz inequality holds in general so it's
8:31:39
somewhat difficult to prove the textbook doesn't even bother proving it so this is the first sort of
8:31:45
simplification we're going to pretend that instead of working with magnitude of f and magnitude of g we're going to
8:31:50
work with the magnitude of the inner product the second simplification
8:31:57
is that if we have some sort of complex number z its squared magnitude
8:32:04
is always going to be greater than or equal to the squared magnitude of the imaginary part
8:32:10
of z this is a very silly sort of construction to make if you think about it but we can rewrite
8:32:16
this in the context of that complex number z so the complex number z then is always
8:32:22
going to be at least greater than or equal to the imaginary part of z now the
8:32:29
imaginary part of z where the z is this complex number f inner product with g we can write that
8:32:35
as f inner product with g minus g inner product with f
8:32:41
so this is that number z minus its complex conjugate now minus the com the complex conjugate just flips the sign on
8:32:47
the imaginary part leaving the real part unchanged so this subtraction is going to cancel out the real part and double
8:32:54
the imaginary part now if i want if i think about this this is actually twice the complex part of this
8:33:02
number f inner product with g so i would have to divide it by 2. and
8:33:08
the imaginary part is of course going to be a purely imaginary number so if i divide it by 2i i'll get a purely real
8:33:14
number and i can stop worrying about the absolute magnitude this is going to be a
8:33:21
result this result is essentially the same as this so i have 1 over 2i dividing the
8:33:26
difference of a number and its complex conjugate to pull out the imaginary part cancel out the i and then i'm squaring the result same as
8:33:33
i would be squaring the result here so this sort of simplification
8:33:40
putting the overall expression up
8:33:46
tells you what we started with which was sigma q squared sigma r squared is going
8:33:55
to be greater than or equal to that final result 1 over 2i times
8:34:00
the complex number f inner product or sorry complex vector f inner product with complex vector g
8:34:07
minus inner product of complex vector g with complex vector f
8:34:14
so somewhat complicated expression and unfortunately it's going to get worse before it gets better
8:34:19
let's take a closer look at what these rifters represent keep in mind that our vector f here was
8:34:26
defined to be q hat minus mu q acting on our state psi
8:34:32
and complex vector g was defined to be operator r minus mu r
8:34:38
acting on our state psi those were our definitions so writing this out
8:34:45
let's take this first term first we've got
8:34:50
f inner product with g that's going to be written out in terms of these definitions so this is
8:34:57
q operator minus mu q acting on state psi on the left inner
8:35:04
product with g which is vector operator r minus mu r
8:35:10
acting on state sign now these are hermitian operators
8:35:16
which means i can take the one that's acting on the left and push it back over to the right now
8:35:22
that seems a little bit strange didn't we just do that step uh in reverse earlier on yes yes we did but it's a
8:35:28
hermitian operator it's a perfectly valid mathematical expression so that leaves me with
8:35:35
just psi on its own on the left and then we have this product of two operators
8:35:40
q hat minus mu q r hat minus mu r
8:35:46
acting on psi all acting on the right this is now two binomials it can be
8:35:52
expanded out so psi on the left all by itself and then here we've got something that
8:35:58
needs to be foiled and keep in mind operators don't commute in principle while the operators q and r are not
8:36:04
going to commute mu q r mu r and q etc those are just mu q and mu are just
8:36:09
multiplication by numbers that commutes with pretty much everything so what we're left with we've gonna
8:36:14
we're going to have a q hat r hat term here
8:36:20
we're going to have a minus mu q r hat term
8:36:26
here we're going to have a minus mu r
8:36:32
q hat term from here and we're going to have a plus
8:36:39
mu q mu r term here uh so there's our smiley face we've
8:36:45
counted for all of our terms got all of the signs correct all of that is acting on psi on the right
8:36:52
now this is just an operator expression with four terms in it separated by addition these are linear operators
8:36:59
meaning i can separate this out into four separate expressions what you're going to have then is going
8:37:05
to be psi q hat r hat acting on psi
8:37:10
minus mu q can be factored out of this sort of resulting expression mu q
8:37:17
times psi acting on r hat psi from the r hat acting on the side mu hat
8:37:23
being pull factored out likewise mu r
8:37:28
psi q hat psi plus mu q
8:37:34
mu r psi psi so we can simplify some of these terms
8:37:41
right away this guy is just one this is the normalization integral if our state is properly normalized this inner product
8:37:48
is going to be 1. and the rest of these things these are expectation values
8:37:54
this is the expectation value of q hat r hat this
8:38:01
is the expectation value of r hat yeah this is the expectation value of q
8:38:06
hat so if i was to pull along the constants um have them all come for the ride this
8:38:13
is q hat r hat minus uq expectation of r hat minus mu r expectation of q hat
8:38:20
plus mu q mu r but r hat that's just mu r and q hat
8:38:27
that's just mu q so i've got the expectation value of q hat r hat whatever it is minus mu q mu r
8:38:34
minus mu r mu q plus mu q mu r these are just scalar multiplications they commute
8:38:39
so one of these is going to cancel out let's say that one and what i'm left with is the
8:38:46
expectation value of q hat r hat minus mu q
8:38:52
u r so that's what i got for f g now f g
8:39:00
i've also got to work with g f g f is going to end up very similarly
8:39:06
if you think about g and f it's going to look essentially identical to this except q and r are going to be
8:39:12
interchanged so g and f here is going to give me the expectation value of r hat q
8:39:18
hat minus again mu q mu r same sort of product of
8:39:24
uncertainties or product of means so that believe it or not is all we need
8:39:30
to get our main result we have sigma q and sigma r in terms of
8:39:35
these sorts of complex numbers which are expressed in terms of expectation values of those fundamental operators
8:39:42
so if you substitute all of that back in we had
8:39:47
f g minus g f that's going to be
8:39:53
bracket q hat r hat so expectation of q r minus the expectation of r hat
8:40:00
q hat and that's it the mu q mu r terms are going to cancel
8:40:05
out they were added on regardless whether we're talking qr or rq so when we subtract they're just going to cancel
8:40:10
out you can think about this as being the expectation of q hat r hat minus r hat q
8:40:18
hat which this qr minus rq you should recognize
8:40:23
this is a commutator so we can write this down instead as the commutator of q hat and r hat
8:40:32
so our final expression then putting all the constants back into it is that sigma
8:40:39
q squared sigma r squared is always going to be greater than or equal to
8:40:44
1 over 2 i times the expectation value of the commutator of the operator q with the
8:40:51
operator r all of that squared that is our result
8:40:58
that is the generalized uncertainty principle what this tells you is that any two
8:41:04
operators q and r are going to have an uncertainty relation if they have non-zero commutator so if
8:41:11
the two operators commute there's nothing wrong with knowing both of them precisely they can both have zero
8:41:17
uncertainty but if they have non-zero commutator meaning the expression qr minus rq
8:41:24
does not have zero expectation value then any two observables
8:41:29
will then those two observables will have non-zero uncertainty principle there will be some
8:41:35
minimum uncertainty the obvious example to do here is position and momentum
8:41:41
we talked about the commutator of the operator x hat and the operator p hat before
8:41:46
it's just x hat p hat minus p hat x hat and if you substitute in the definition
8:41:52
of p hat as minus i h bar partial partial x and the definition of the
8:41:57
operator x hat as just x you know multiplied by and you insert some dummy wave functions on either side that was an activity that
8:42:04
we did earlier on in the course you find that the commutator here is equal to just a constant
8:42:10
i h bar it's complex constant which seems a little strange but there's nothing wrong with complex numbers when you're mixing
8:42:16
operators like this it's only when you would make an observation of a single operator single physical quantity that you have to get real numbers
8:42:23
what that tells us is that sigma x squared sigma p squared in the generalized uncertainty relation is
8:42:29
going to be 1 over 2i times the expectation value of the commutator which is just i h-bar
8:42:36
squared so the expectation value of a constant is just going to be the constant so this is just going to be
8:42:42
i h bar over 2 i quantity squared eyes cancel out
8:42:48
and we've just got h bar squared over 4
8:42:53
h bar over 2 squared now the way the uncertainty principle is usually stated is sigma x sigma p is greater than or
8:43:00
equal to h bar over 2 and that of course is clearly the same expression that we're working with here so good we've
8:43:06
got the same sort of uncertainty relation that we introduced earlier on in the course
8:43:12
to check your understanding of this sort of process here are some questions for you what would happen in the derivation
8:43:18
if instead of throwing out the real part meaning instead of saying that the absolute magnitude squared of some
8:43:24
complex number is always greater than one over two i z minus z star
8:43:31
all squared what would happen if i instead threw out the real part by adding the number to its complex con or
8:43:38
complex conjugate instead would you still get a commutator and what extra terms would it introduce
8:43:43
and finally just in terms of some of the steps in that derivation why exactly did this step happen what are the principles
8:43:50
that are applied in that equality what definitions do you need to know now that's about all that there is to
8:43:56
the generalized uncertainty principle it's an amazingly powerful mathematical tool but
8:44:01
well let's let's play with it a little more how strict is this limit and can we beat it
8:44:07
now the limit that we're talking about here is this relationship i had something some sort of
8:44:13
uh sigma q squared sigma r squared was always greater than or equal to 1 over
8:44:19
2i times the expectation of the commutator of operator q and operator r
8:44:27
all squared that's our generalized uncertainty principle this
8:44:32
inequality where did that inequality come from well it came from two places it came from the schwartz inequality
8:44:42
um which told you that the inner product of that vector we define f with itself
8:44:48
multiplied by the inner product of the vector g with itself was always going to be greater than the squared modulus of
8:44:55
the inner product of f and g that was one source of the inequality
8:45:01
so if we're trying to make this into an equality we have to not
8:45:07
uh not grant any space in between the result of these inner products and the inner product of the vectors with itself
8:45:14
um how can we make the schwartz inequality into an equality in other words and that's rather straightforward
8:45:20
if you think about it the vector g is just going to be some constant say c times the vector f
8:45:27
if this is true then this is going to be c squared f squared and this is going to be c squared f squared we're going to
8:45:33
have an equality here overall [Music]
8:45:40
the second inequality we had was when we threw out the real part
8:45:48
we said the magnitude of that complex number fg
8:45:53
in terms of its squared modulus was always going to be greater than or equal to this 1 over 2i
8:46:00
times f g minus g f
8:46:07
all of that squared this statement can we make this into an
8:46:13
equality as well well what we're looking at here is going to be an equality if we're throwing out
8:46:20
the real part we're taking the the squared magnitude of it the squared magnitude is only ever not going to change when we throw out the real part
8:46:27
if the real part is 0 to begin with so we've got equality here if the real part of f g that inner product is equal
8:46:35
to zero and that's reasonably straightforward we're looking at fg but
8:46:41
we know g can be expressed in terms of c so we're talking about the real part of
8:46:46
f times g expressed as cf gives me a c and another f
8:46:52
so the real part of c times this inner product of a function or of a vector with itself this inner product of a
8:46:58
vector with itself is going to be a real number no matter what you do you're taking a
8:47:03
complex conjugate multiplying it by itself essentially you're going to get a real number so this is only ever going to equal 0
8:47:12
if c is complex c is sorry purely imaginary
8:47:21
c being purely imaginary let's write it as the imaginary unit i times some real number
8:47:27
a so given sum c equals i times a if we define our states or our
8:47:34
yeah if we define our operators in our states such that g is given by some complex unit times a
8:47:40
times the state f for you know some real a
8:47:47
then we've turned our both of our inequalities into equalities so what does that mean
8:47:54
what sort of implications does this have let's consider that in the context of position momentum uncertainty just to make this a
8:48:00
little more concrete we have this notion that our vector g is
8:48:05
imaginary unit times some real number times our vector f
8:48:10
now in the version or in the language of position momentum uncertainty then this vector g
8:48:17
is going to be p hat minus expectation of p
8:48:22
times our state and we know what the position or the what the momentum operator is this is
8:48:28
going to be minus i h bar partial derivative with respect to x
8:48:34
minus expectation value of p i'll just leave it as expectation value of p here this is just going to be a number so there's no magic there
8:48:40
and this is going to be multiplied by psi of x if i'm writing out my momentum operator in terms of partial derivatives
8:48:45
i better write my wave function in terms of x instead of just as some arbitrary state vector
8:48:51
likewise we've got our vector f and this has to be expressed in terms of our position so this is going to be x
8:48:58
hat minus expectation of x acting on our state
8:49:04
and likewise in terms of wave functions this is going to be x multiplication
8:49:09
minus expectation value of x the constant multiplying our wave function psi of x
8:49:17
so our expression for g in terms of i a times f with these particular definitions of g and
8:49:23
f uh we can plug these together substitute these expressions here into this
8:49:28
equation here and you end up with separating things out minus i h bar
8:49:35
partial psi partial x minus expectation value of momentum multiplying psi
8:49:42
and that has to be equal to i times a times our expression for f
8:49:50
which you know i'll just uh expand that out we've got i times a times x times
8:49:55
psi of x minus i times a times expectation of x
8:50:00
times psi of x this right here
8:50:06
is a differential equation for psi and it turns out it's actually a pretty easy differential equation to solve
8:50:13
if you arrange rearrange things a little bit you can find out this is going to give you a derivative of psi with
8:50:18
respect to x as in terms of let's see what have i got i've got a after i've divided through by
8:50:26
minus i h bar i'm going to have a minus a over h bar um
8:50:31
let's say x psi pulling the complicated term first
8:50:39
and then i'm going to have a plus a over h bar expectation of x
8:50:44
psi and a plus i expectation of p over h bar
8:50:50
psi provided i've got all of my signs correct there and i haven't lost any
8:50:56
terms i've got the over h bars yeah i think that looks right this is a fairly straightforward
8:51:02
ordinary differential equation to solve now i'll leave it as an exercise to you guys to actually go through and solve
8:51:07
this but the procedure for solving it i think is most easy to think about let me just guess
8:51:14
that my wavefunction psi is equal to e to the some sort of function f of x
8:51:20
if you do that you find a simplified differential equation just for f this sort of initial guess where psi is going
8:51:26
to be some sort of an exponential and you're trying to find the behavior of the exponent is a common technique for solving differential equations where
8:51:31
your derivatives essentially give you the function back multiplied by various terms
8:51:37
under these circumstances you can figure out what your psi of x actually looks like and your psi of x under these
8:51:43
circumstances has to be e to the minus a
8:51:50
over two h bar uh let's see x minus the expected value
8:51:55
of x uh quantity squared e to the i expectation value of p over h
8:52:01
bar times x and then there's another constant floating around here something like e to
8:52:07
the a expectation value of x squared all over 2 h bar
8:52:16
um this solution comes out of just a straightforward solve here uh the only simplification i've made on the result
8:52:22
is to complete the square in the exponent whenever you have a x squared
8:52:27
sort of behavior it's good to pull that off by itself now the reason i've separated these three terms out instead of writing them
8:52:33
all is sums together in the exponent as it makes the structure a little bit more straightforward this is some sort of a constant
8:52:40
this is something that looks like just a something with a certain momentum i kx
8:52:45
and this this is a gaussian e to the minus something x squared
8:52:52
this gaussian form is definitely a realizable wave function we've actually met gaussian wave
8:52:58
functions before for example in the quantum harmonic oscillator ground state under those circumstances you have met
8:53:05
the uncertainty limit you can meet the uncertainty principle limit so the two messages there is first of all
8:53:12
the authority limit is attainable but it's difficult you have to be in a very specific sort of mathematical state this
8:53:19
is not going to be true for anything that's non-gaussian uh the second take-home message from
8:53:24
this is that the uncertainty principle is actually a fairly strict limit that despite the fact that we made those
8:53:30
seemingly a little bit fudgy simplifications when we were working through the derivation of the generalized uncertainty principle
8:53:35
applying the uh the schwarz inequality and uh just assuming that the real part of the number could be neglected and the
8:53:42
imaginary part was the only thing that mattered um we haven't actually seeded too much ground there the uncertainty principle
8:53:48
is a fairly strict limit that is actually attainable it's not like we've made some ridiculous lower limit or yeah
8:53:53
ridiculous lower limit on the uncertainty regardless that's a mathematical
8:54:00
discussion of the formal structure of the uncertainty principle in quantum mechanics and subject to
8:54:06
the generalized uncertainty principle any two operators with a non-zero commutator are going to have some sort of
8:54:12
uncertainty principle and you could go through the same sort of derivation of what the minimum uncertainty behavior would look like for any two r2 operators
8:54:19
it's relatively straightforward for the position momentum structure and you get a gaussian but you could do it for other cases as well
8:54:26
i think that about sums it up though generalized uncertainty in quantum mechanics is like i said a very powerful mathematical tool so keep that one in
8:54:32
your bag of tricks given the generalized uncertainty
Energy time uncertainty
8:54:39
principle for any two quantum mechanical operators something like sigma q squared
8:54:44
sigma r squared is greater than or equal to one over two i times the commutator
8:54:50
of the operator q and the operator r all squared
8:54:56
you might think that uncertainty principles have been pretty well settled but that's actually not the case while
8:55:01
this does give a good and satisfying explanation of something like the classic sort of delta p delta x is
8:55:07
greater than or equal to h bar over two sort of uncertainty relation it doesn't cover the case delta e delta t is
8:55:15
greater than or equal to h bar over two if you've seen this sort of uncertainty principle it's also very useful in physics but it
8:55:22
is of a fundamentally different nature than position momentum uncertainty and the fundamental reason for that is that
8:55:28
there's something special about time time in quantum mechanics is a parameter that shows up in the arguments to your
8:55:34
equations it's not so much like momentum where there's a well-defined momentum operator
8:55:39
so how can we handle energy time uncertainty well the notion of time in a quantum
8:55:45
mechanical system is a little bit squishy if you're talking about the time evolution of something like
8:55:51
e to the i e t over h bar that solution to the time dependent schrodinger equation or at least the
8:55:57
time part thereof when you apply separation of variables this thing just rotates around complex number space it
8:56:03
doesn't actually change the fundamental nature of the solution unless you have some sort of a superposition of two
8:56:09
states where they have different time dependences two states of different energies and the overall time dependence
8:56:14
only ever depends on the energy difference now that suggests that if we're talking
8:56:20
about some sort of a change in a process some sort of a change in expectation value of position for instance that as
8:56:26
it results from a superposition of two states with two stationary states with different energies we have to consider the notion of change
8:56:33
time is only ever going to be relevant when we're considering things that change because if nothing is changing then what does time really mean
8:56:40
well um if we're talking about change we're talking about some sort of an operator because we're talking about something
8:56:47
that changes we need to have an observable so we need to have some operator and as usual i'll call that q hat meaning the hermitian operator that
8:56:54
corresponds to some sort of quantity q
8:56:59
so let's consider time derivative of the expectation value of q this gives
8:57:05
us some sort of classical almost notion of how things change with time
8:57:11
now the expectation value in our generalized linear algebra formulation is an inner product of our state psi
8:57:19
our operator q hat acting on state psi this inner product has three components
8:57:24
to it we've got a wave function on the left an operator which potentially has time dependence in it itself and another
8:57:29
wave function on the right or another state on the right and if you think about the inner product as written out in terms of an integral
8:57:36
of wave functions this is going to be a complicated integral but it's got three things in it that are all going to potentially vary with time
8:57:42
so let me sweep some of the mathematical details under the rug here and rewrite this more or less applying the product rule
8:57:49
so we've got a partial derivative of psi with respect to time whatever that state
8:57:54
may be multiplying our inner product with q acting on psi
8:58:00
we have psi on the left acting on a partial derivative of q hat
8:58:05
with respect to time whatever that may be that operator acting on psi
8:58:10
and we have psi acting on our inner product with q hat acting on
8:58:15
partial psi partial t now this is a very suggestive notation it feels like it's only ever going to be
8:58:22
relevant if we're talking about psi as functions of time what on earth does this notation mean to begin with
8:58:28
um not much to be quite frank with you there's a lot of somewhat dicey mathematical things that have happened behind the scenes in applying the quote
8:58:35
product rule unquote to this sort of expression if we're really going to write these
8:58:40
things out as integrals then these are well-defined mathematical operations and you can apply the product rule and all these
8:58:46
sorts of things make sense but if we're trying to do this in general i've kind of swept a little bit too much under the rug
8:58:55
that said i'm going to leave things in this general form the reason for that is it's a much more concise notation so if
8:59:01
you want a sort of behind the scenes idea of what's going on in each of these terms try and translate it into an
8:59:06
integral and figure out what exactly has happened in each of these steps
8:59:12
if you're willing to take me at my word that this is at least somewhat meaningful notation we can write down
8:59:17
for instance some of these terms with partial derivatives of psi in them can be simplified with the time dependent
8:59:23
schrodinger equation the time dependent schrodinger equation tells us that i h bar partial psi partial t
8:59:31
is given by the hamiltonian operator acting on psi
8:59:37
so really i ought to say this is a state and this is a state in my vector notation
8:59:43
but in this sort of context you can simplify this sort of term and this sort of term
8:59:50
so let's uh let's do that let's substitute in for this and in for this
8:59:56
when you do that these three sort of expectation value like terms can be simplified a little bit first of all
9:00:04
this partial side partial t on the left i've got a
9:00:10
one over i h bar when i simplify to just get partial side partial t by itself so this is
9:00:17
one over i h bar hamiltonian applied to
9:00:25
psi as our replacement for this overall state here on the left
9:00:31
and then i've got q hat psi on the right
9:00:37
this middle term here is just going to be the expectation value of partial q partial t now what on earth is that can
9:00:44
i take the partial time derivative of an operator um yes if the operator has
9:00:49
explicit time dependence if the operator doesn't have explicit time dependence then it's not going to have any uh any
9:00:56
partial time derivative this term is going to be zero and we're about to say this term is equal to zero in a few minutes anyway to give you an example of
9:01:02
a situation where this term would be non-zero think about something like the potential energy in the harmonic
9:01:07
oscillator where the spring constant of the harmonic oscillator is gradually being tuned the frequency of the
9:01:13
oscillators being is is changing with time perhaps the spring is getting gradually weaker or the temperature is
9:01:19
changing affecting the spring constant under those circumstances this term would be non-zero the operator for say
9:01:25
the potential energy in that quantum harmonic oscillator would be a time dependent operator and taking the
9:01:31
partial time derivative would give you something that's non-zero this third term
9:01:37
we can also apply a simplification we've got psi on the left we're not going to touch that
9:01:44
and on the right hand side we've got let's see 1 over i h bar we've got a q
9:01:50
hat and an h and 1 over i h bar there acting on psi
9:01:58
now the next step in the derivation here in
9:02:05
considering how we can possibly simplify this is we've got a term with q h bar or q h q hat h hat excuse me on the right
9:02:13
and a term here h hat and q hat so let's see if we can simplify this by applying the notion of a hermitian operator to
9:02:19
each of these terms if i use the fact that h hat is a hermitian operator
9:02:27
i can simplify or not simplify i can move the h i can instead of having h act on the left i
9:02:32
can have h hacked on the right so this will become an h hat q hat acting on psi similar to my q hat h hat
9:02:39
over here now the other thing that i have to do in order to simplify these terms is to figure out what to do with these
9:02:45
constants multiplication by a constant on the right does nothing i h bar in the denominator i'm just
9:02:52
going to move that outside so that will become a 1 over i h bar outside this expression
9:02:58
now the one over i h bar here cannot simply be moved outside and the reason for that is it's inside this left hand
9:03:04
side of the equation so if i move it outside i have to think about taking complex conjugate so if i'm going to move this guy outside
9:03:13
i have to stick a minus sign on it because i've got an i in it i have to flip the sign
9:03:19
on now if i do those two simplifications first i have a minus 1 oops
9:03:27
1 over i h bar and this term i have psi
9:03:33
h hat q hat psi this term which i'm going to write next
9:03:39
is plus 1 over i h bar psi
9:03:44
q hat h hat psi and my remaining term over here is
9:03:51
partial q hat partial t expectation of that whatever it may be
9:03:57
now this overall expression here can be simplified even further here i have a h
9:04:03
hat q hat and a q hat h hat if you're seeing a commutator on the horizon you're thinking the right thought let's
9:04:09
combine these two terms together these two expectations together essentially factoring out the psi on the left and
9:04:14
the psi on the right what we're going to be left with is something like minus 1 over i h bar
9:04:21
psi and then the operator here is going to be h hat q hat minus q hat
9:04:29
h hat factored out a second minus from the q hat h hat term here and i've got psi on the right
9:04:36
uh and as before i've got my expectation of partial q hat partial t
9:04:43
coming along for the right so this term now i can write that as i
9:04:49
over h bar if i multiply and divide both of these things by i basically move the item numerator flips the sign
9:04:55
i have here the expectation of the commutator of h
9:05:01
and q plus the expectation of the partial
9:05:07
derivative of the operator q hat with respect to time so this is a somewhat general result
9:05:14
any time derivative of an expectation value is going to be given by a commutator of
9:05:20
that operator that gives you the expectation and the hamiltonian
9:05:25
plus some sort of explicit time dependence if there isn't any explicit time dependence in this what this tells you
9:05:31
is that if the operator and the hamiltonian commute with each other if the commutator is zero in other words if
9:05:37
hq is equal to qh then there is potentially going to be no
9:05:43
time dependence for your expectation essentially time evolution ignores time
9:05:50
evolution of system as given by the time dependent schrodinger equation essentially ignores the expectation
9:05:56
value of the operator that you're considering it's some sort of a conserved quantity that's a very useful sort of thing to be
9:06:01
able to figure out so if you've got commutator is zero you're going to have a conserved quantity keep that in the back of your mind
9:06:08
now for the special case where the partial derivative of the q operator itself is exactly zero then what we're
9:06:17
left with from the previous slide is that the time derivative of our expectation value
9:06:23
of q is equal to i over h bar times the
9:06:29
expectation of our commutator h hat q hat
9:06:35
that was our general result i just dropped the partial expect the expectation value of this
9:06:40
sort of term back to the notion of uncertainty if i
9:06:47
have the hamiltonian and my operator q as the two things that i'm considering meaning i'm looking at an uncertainty in
9:06:54
the hamiltonian squared and the uncertainty in my operator q squared
9:06:59
this is going to be our energy uncertainty what is it sigma q going to be
9:07:05
well given this expect you're given this expectation of a commutator that's the sort of thing that appears on
9:07:11
the right hand side of our generalized uncertainty principle we had a 1 over 2 i expectation of a commutator applied to
9:07:17
this particular operator pair is going to be h hat q hat inside the commutator
9:07:22
all squared so expectation of a commutator i can rewrite that in terms of
9:07:28
the time derivative of the expectation so my right hand side here i can rewrite
9:07:34
in terms of this as i've got my 1 over 2i as before i got to solve for the commutator by
9:07:40
multiplying through by h dividing by i so i've got h bar over i on the left hand side and d dt
9:07:48
of the expectation value of q oh that's going to be squared
9:07:54
so simplifying this i've got an i and an i which is going to give you a minus 1 in the denominator so i'm going to have a minus sign but i'm squaring everything
9:08:00
overall so that's not going to change much and what i've got for my right hand side is h bar
9:08:06
squared over on c let me write it as h bar over 2 quantity squared and then i've got my d
9:08:13
dt of the expectation value of q squared
9:08:19
so what this tells you is that sigma h
9:08:24
sigma q taking the square root of both sides of this equation is going to be greater than or equal to h bar over 2
9:08:31
times this weird thing the time derivative of the expectation value of q i'll put that in absolute magnitude sign
9:08:38
to cover my bases in terms of square roots of squares what this tells you is that
9:08:44
the uncertainty in the value of an operator the uncertainty in the operator itself
9:08:50
is going to be related to the time derivative of the expectation value of that operator
9:08:55
essentially what that's telling you is that your uncertainty in the outcome of a measurement is going to depend on how quickly the quantity that you're
9:09:01
trying to measure is changing and that seems honestly rather logical there is another factor here in terms of
9:09:08
the uncertainty in the energy that helps bring things uh bring things into focus further though so let's uh let's
9:09:15
make a note of this result it's nice and sort of qualitatively appealing the notion that the uncertainty in an
9:09:21
observable is related to how fast it changes and the more quickly it's changing the higher the time derivative of its expectation value the larger the
9:09:28
resulting uncertainty must be but let's see if we can cast that in terms of that classic delta e delta t
9:09:33
uncertainty if we're talking about delta e
9:09:38
that's essentially our sigma sub h it's our uncertainty that results from a measurement of the energy which is given
9:09:46
by proxy in the notion of quantum mechanic or the language of quantum mechanics in terms of the hamiltonian operator
9:09:53
and really we need some notion of delta t as well what is delta t in this case well let's define delta t to be
9:10:00
something like the uncertainty in our observable q divided by the magnitude of the time
9:10:05
derivative of the expectation value of q this is sort of
9:10:10
some characteristic size of change in q multiplied by the rate of change in q
9:10:17
so if this is some sort of delta q over dq dt this would give me some sort of a notion of delta t more by dimensional
9:10:23
analysis than anything else really what this means is sigma q
9:10:30
can be thought of in terms of the time derivative of the expectation value of q
9:10:36
and delta t if i just say multiply this out onto the left hand side which says that this characteristic time that i'm
9:10:42
interested in is the amount of time it takes the system to change by one sort of standard
9:10:48
deviation of the observable in question so this is going to depend on the observables that you're working with in
9:10:53
some sense but it is a notion of the characteristic time scale of change in the system
9:10:59
now under these circumstances our sigma h sigma q expression
9:11:06
is going to look like h bar over 2 and then we have the time derivative
9:11:12
of the expectation value of q that is going to be converted into
9:11:18
delta e replacing sigma h delta t replacing sigma q
9:11:26
with this sort of expression and then you can cancel out essentially this time derivative of q is going to appear
9:11:32
both on the left hand side and the right hand side thinking about it along those lines and what we'll be left with is just that
9:11:38
this is greater than equal to h bar over 2. so
9:11:44
there you have it we have a derivation of the conventional energy
9:11:49
time uncertainty relation what you should keep in mind here is that all of this was derived assuming a particular
9:11:55
observable so the potential results that you're going to get are going to depend on the quantity that you're interested in if some quantity that you're
9:12:00
interested in is changing very rapidly then you're going to end up with a relevant delta t this delta t is not
9:12:07
just some time measurement uncertainty it's a time scale of change of the quantity
9:12:13
that you're interested in so there has to be some sort of quantity in the back of your mind you're not just saying delta t for the system you're
9:12:18
saying delta t for momentum or delta t for position or delta t for kinetic energy or something like that
9:12:26
regardless the conclusions are the same as the system is evolving rapidly
9:12:34
meaning with respect to the variable that i'm concerned about the the time derivative
9:12:39
of the expectation value is large
9:12:47
then what that means is that delta t will be small right
9:12:52
a large number in the denominator gives you a small number and what that means is that the uncertainty in the energy will be large
9:13:01
essentially what that means is if you have a system that is changing rapidly it has to consist of a superposition of
9:13:08
a wide range of different energies you can only ever get a system to evolve rapidly with time if it contains a wide
9:13:15
range of energies and that gets back to the same sort of discussion we were we had earlier on in this lecture where
9:13:20
where i said that the only ever the only way you ever got an expectation value to evolve was if you had a superposition of states with multiple energies the wider
9:13:27
the separation between those energies the more rapidly the evolution would occur that's reflected again in this
9:13:33
energy time sort of uncertainty relation the flip side of this if the system is relatively stable
9:13:40
what that means is that your system is evolving slowly with respect to the observable that you're interested in
9:13:46
so the time derivative of the expectation value of that observable is small then that means it will take a long time
9:13:51
for the observable to change by one sort of standard deviation in the observable which means our delta t is
9:13:57
large and consequently our delta e can be small
9:14:02
we can have a small uncertainty in energy if we have a slowly varying system
9:14:08
if you have a system that's stable with time nothing is changing very rapidly then
9:14:15
the energy uncertainty can be small it can have a very precise energy keep in mind these are all just inequalities so
9:14:21
you can have a very large energy uncertainty and a very rapidly evolving and a very
9:14:26
slowly evolving system but at any rate uh the last thing that i wanted to mention to mention here is
9:14:33
that all of this is really valid for any sort of q so this q is representing any observable
9:14:44
what that means is that if anything is changing rapidly then the
9:14:50
energy uncertainty will be small we can flip that statement around and say that if
9:14:55
the energy uncertainty will be large we can flip that statement around and say if the energy uncertainty is very small
9:15:02
meaning we're dealing with sort of a determinant state something with almost no energy uncertainty then
9:15:09
all time derivatives of expectations of any observable are going to be small
9:15:16
and we said that before in the context of stationary states stationary states are the states that are eigenstates of
9:15:21
the hamiltonian operator they evolve with time in a very simple way and for a system that is in a single
9:15:28
stationary state the energy uncertainty is zero therefore the delta t has to be a very very large
9:15:35
number effectively infinity in order for this inequality to hold which means
9:15:41
all changes in the system take place on a very very very long time scale everything is evolving very very slowly
9:15:46
and in the sense of a true mathematical stationary state that is exactly stationary nothing is allowed to change
9:15:53
with time stationary states are truly stationary
9:15:58
so that wraps up our discussion of energy time uncertainty this is fundamentally different than the notion of
9:16:05
position momentum uncertainty where both position and momentum are operators but it does have some nice general
9:16:10
interpretations in terms of the rate of change of expectation values of operators so keep all of this in the back of your
9:16:17
mind it will help you interpret the behavior of quantum mechanical systems in general
9:16:22
as they evolve with time
Schrodinger equation in 3d
9:16:34
we started off this course by building a framework talking about quantum mechanics in one dimension where it is
9:16:41
most simple and easiest to understand then we built up some formalism
9:16:48
talking about the mathematical structure of quantum mechanics now we're going to come back to where we
9:16:53
started except instead of talking about quantum mechanics in one dimension we're going to talk about it in three dimensions
9:16:58
we live in three dimensions so this is where the real world examples start to enter quantum mechanics
9:17:05
first of all how do we go from one dimension to three dimensions if we're going to start off
9:17:11
in one dimension we ought to have counterparts for the concepts that we encountered in one
9:17:16
dimension in three dimensions in one dimension we had a wave function which was a function of position and
9:17:22
time in three dimensions our wave function is going to be a function of position in
9:17:28
three dimensions and time thankfully it has not become a vector function it is still only a scalar
9:17:34
function but it is now a function of four variables instead of only one instead of only two here
9:17:41
we will see shortly that when we were talking about the time independent schrodinger equation as
9:17:47
derived from this full time dependent wave function we ended up with the solution to the time independent
9:17:52
schrodinger equation where simply a function of position times e to the minus i energy time over h bar
9:18:01
we're going to find out something very similar happens in three dimensional quantum mechanics we'll get a function of position in three dimensions
9:18:08
multiplied by the same exponential factor e to the minus i energy time over h bar
9:18:17
the operators that will appear in the schrodinger equation for instance in one dimension we had for instance the
9:18:22
position operator x hat and the momentum operator p-hat x-hat and p-hat and three dimensions are
9:18:28
going to be vector operators so instead of just having x-hat i'll have x-hat y-hat and z-hat in a vector
9:18:36
or p x hat p y hat and p z hat
9:18:41
in a vector and the definitions here are more or less what you would expect for instance um
9:18:48
let's just say p x hat or sorry p x hat
9:18:54
is going to be minus i h bar derivative with respect to x
9:19:00
i have to start being more careful about the difference between total derivatives and partial derivatives now since we're talking about functions of multiple
9:19:07
variables but hopefully the notation will become reasonably clear shortly
9:19:13
the full momentum vector operator here is going to be written then in terms of
9:19:19
partial derivatives of x y and z and we have some notation for that
9:19:24
minus i h bar times this upside down triangle with a vector
9:19:30
hat on top of it this is the gradient operator from vector calculus
9:19:36
and this is going to be read as del or grad or the gradient of depending on whatever it's acting on
9:19:44
and this gradient operator here as before let me
9:19:49
move this out of the way a little bit so my notation is less confusing
9:19:55
this full vector one of the key experiments that really got quantum mechanics started was
Hydrogen spectrum
9:20:01
spectroscopy brightline spectra of the elements they couldn't really be explained in the context of what physics was known at the
9:20:08
time and we've finally gotten to the point now where we can use the quantum mechanics we've learned so far to
9:20:13
explain these bright line spectra at least some of them perhaps this is the spectrum of hydrogen this is the
9:20:20
spectrum of mercury this is the spectrum of neon and this is a xenon so four
9:20:25
gases and we'll be able to explain successfully the most simple gas possible hydrogen
9:20:32
our discussion of the time independent schrodinger equation in 3d separated in spherical coordinates
9:20:38
as appropriate for the spherically symmetric potential of a charged particle orbiting a nucleus
9:20:45
gave us psi with three quantum numbers n l and m
9:20:50
i'm not going to reproduce the long complicated expression for what these are but you know the radial part is
9:20:56
given by the associated lager polynomials and the angular part is given by the spherical harmonics
9:21:04
as we went through the solution of the time independent schrodinger equation we introduced a variety of constants
9:21:09
and then requirements in particular for periodicity in the fee solution
9:21:16
the convergence and well-behavedness of the angular solutions and convergence and
9:21:21
well-behaveness of the radial solutions gave us quantization conditions that we use to construct these n l and m
9:21:28
the constants that we got for instance we defined a k squared that was given by a 2me over h bar squared
9:21:34
that should look familiar we found out that that constant had to be given by one over some
9:21:40
a squared some radius squared times an n squared quantum number
9:21:46
this a value the bohr radius is about half an angstrom and the energies that we got after re
9:21:52
you know unwinding all of those definitions that we made look something like this
9:21:57
you have the energy of the nth energy level the nth stationary state the stationary state with n
9:22:03
as the quantum number is given by this constant times 1 over n squared
9:22:09
and that constant should look familiar it's minus 13.6 or it's 13.6 electron volts with a minus sign out front
9:22:15
signifying that these are bound states their energy is less than the energy of a free particle so minus 13.6 electron volts
9:22:22
over and squared those are the energy levels of our stationary states
9:22:29
our stationary states are not going to be stationary in reality because atoms bump into each other and atoms interact
9:22:34
in random ways that we haven't described the physics of yet but
9:22:40
suffice it to say perhaps that these energies are not going to remain forever
9:22:45
fixed if i prepare an atom in say the n equals three a quantum state with n
9:22:51
equals three it's not going to stay there forever after a while it will lose that energy and when it does
9:22:57
it will emit a photon the changes in energy that take place
9:23:02
are energy carried off by the photon so we would say for instance that if we had say n equals three goes to n equals
9:23:10
two there's a change in energy here and we would say the atom has emitted
9:23:17
a photon correspondingly if you have an atom in
9:23:22
state n equals two and it's excited up to state n equals three by uh an electromagnetic field surrounding the
9:23:28
atom we would say this atom has absorbed a photon
9:23:37
this absorption and emission of photons photon here is our shorthand term for a particle of light
9:23:44
or quanta of light perhaps i should say quantum
9:23:49
of light is really the the crux of the matter here all of our experiments that
9:23:56
motivated quantum mechanics had somehow to do with the interaction of light and matter with our treatment of the hydrogen atom
9:24:02
we now have descriptions of how we can calculate changes in energy on the matter side we haven't really said
9:24:08
anything about the photon side and unfortunately for that we'll need relativistic quantum mechanics which is a topic for another course
9:24:15
but at any rate you know that light is going to be emitted and absorbed in quanta and the energies of those quanta
9:24:22
are going to be given by the changes in energy of the thing that we can calculate the thing that happens on the
9:24:27
atomic side so these stationary states are not going to be all that stationary and by
9:24:32
plugging in numbers for initial and final energy levels you can calculate out what the energy of the photon would
9:24:38
be what the change in energy of the atom would be these transitions have names and this is
9:24:44
a very standard visualization of what those energies might look like the y-axis here is an energy scale and it
9:24:50
has zero at the top anything with energies higher than zero is not a bound state
9:24:55
the thick horizontal lines here represent the energies of the nth energy level here's n equals one the lowest
9:25:01
energy level n equals two three four five six seven et cetera up to infinity where the bound state isn't really bound
9:25:08
anymore has essentially zero energy the transitions that are possible for
9:25:14
instance if we're looking at the emission of light by a hydrogen atom the atom is going to start in a higher
9:25:19
energy level and drop down to a lower energy level when it does so from an energy level
9:25:25
two three four five six etc up to infinity all the way down to the ground state n equals one we call that a lyman
9:25:33
line the emission in the spectroscopic context has a particular pattern of
9:25:39
energies that were first examined by while lyman and the lines are named after him
9:25:45
transitions that start with three four five six etcetera go up to infinity and drop down to the second energy level are
9:25:51
called balmer lines likewise end state lines with n equals three are passion lines
9:25:57
there are there are also bracket lines you don't hear very much about them even less common are the fund lines and the
9:26:03
humphrey lines which you can imagine have a final state of energy 5 and energy 6.
9:26:09
so these transitions are the sorts of things that you would expect from the energy structure that we
9:26:14
calculated as a result of the time independent schrodinger equation with a 1 over r potential
9:26:22
the transition wavelengths can be calculated pretty simply
9:26:28
what we have here is an energy that we can calculate and we
9:26:34
know the energy of the photon is going to be given by planck's constant times the speed of light sorry let's say
9:26:39
planck's constant times the frequency or alternatively plots constant times the speed of light divided by the wavelength
9:26:47
note that this is planck's constant not h bar the version of the reduced
9:26:52
plonks constant that we've been using so far so when you actually go out to calculate these things you can calculate
9:26:57
wavelengths easily by using the expression we had for the energy change by the atom it's using that as the
9:27:03
energy of the photon symbol for photon is gamma typically and solving for the wavelength
9:27:09
doing so you end up with this sort of thing and this is a logarithmic scale now 100 nanometer wavelength 1000
9:27:14
nanometer wavelength ten thousand nanometer wavelengths and these things fall in very specific patterns
9:27:20
the lyman series which ended with n equals one as the final state so this is
9:27:26
a two to one transition the longest wavelength lyman line this would be a three to one four to one five to
9:27:32
one etcetera all the way up to infinity to one likewise for the balmer lines um
9:27:39
uh three to two four to two five to two six to two seven two et cetera up to infinity 2.
9:27:45
same for the passion series in the bracket series in the fund series and the i forgot his name already the
9:27:50
humphrey series they all have these nice patterns and they all overlap
9:27:56
and if what you're looking at is the visible spectrum of hydrogen you're looking at the balmer lines
9:28:01
there are probably other lines that are visible if you look at a quote hydrogen gas unquote source being excited by a
9:28:09
gas discharge high voltage for instance those are likely due to impurities and if you
9:28:14
think about the hydrogen atom well that's going to behave differently than the hydrogen molecule it's going to behave differently than
9:28:20
the singly ionized hydrogen molecule and spectra like this even with just a
9:28:26
single atom and this is just as predicted for the hydrogen atom with just a single electron
9:28:32
you already have very complicated behavior so if i flip back to my motivating slide here
9:28:37
this is just looking at the visible portion of the hydrogen spectrum and you can now identify this as the
9:28:44
n equals three to two transition this
9:28:50
has the four to two transition five to two six to two and if you continue into the
9:28:56
uv seven two eight two nine two ten two et cetera these are the balmer lines of hydrogen
9:29:02
when you work with more complicated atoms with more electrons you have far more complicated behavior
9:29:08
and this is unfortunately something that quantum mechanics still really cannot predict well
9:29:17
to check your understanding of all of this i have some simple calculations for you to do first of all figure out how the formulas
9:29:23
that we gave for hydrogen would change for helium you still have just a sorry singly
9:29:28
ionized helium so a single electron instead of orbiting a single proton orbiting an electron or orbiting an
9:29:34
alpha particle something with two protons so the charge on the nucleus is going to double and that will change the energies
9:29:42
then make it some calculations of energies figure out whether they would be visible or not
9:29:48
and as finally calculate the longest wavelength identify the transition for the longest wavelength in
9:29:53
the line in series these are conceptual sorts of questions that you need to understand the structure of the energy
9:30:00
levels of hydrogen in order to answer and there are also some simple calculations to do but the fact that you are capable of
9:30:07
making these calculations is really a triumph of quantum mechanics we started with something that is essentially just
9:30:13
an equation hypothesized almost entirely without justification and it actually seems to work
9:30:20
you can do separation of variables you can go through a lot of complicated mathematics which from the physics perspective is
9:30:27
more or less just turning the crank trying to solve this equation and the structure that you get subject
9:30:32
to all of this interpretation we did as far as the the probabilistic interpretation of quantum mechanics requiring normalization of the wave
9:30:38
function and the the overall structure of all of this
9:30:43
leads to calculations of real measurable physical quantities and for instance the answer that you'll calculate for this is
9:30:50
something that you can look up if you look up helium spectrum in google you will get lots and lots of matches and
9:30:55
some of them will include data tables with hundreds if not thousands of observed and identified helium lines
9:31:01
and the energies that you calculate the energy that you calculate will be in that list and that's really quite astonishing if
9:31:08
you think about it it goes to it speaks to the overall power of quantum mechanics
Angular momentum operator algebra
9:31:14
we started this chapter by considering quantum mechanics in three dimensions the first tool we used to solve problems
9:31:22
to solve the time independent schrodinger equation in three dimensions in particular was separation of variables we used separation of
9:31:28
variables back in one dimension as well to separate the time evolution of an equation from the spatial evolution that
9:31:34
was how we got the time independent schrodinger equation from the time dependent schrodinger equation
9:31:40
in the case of three-dimensional space we also use separation of variables to separate the dimensions of space from
9:31:46
each other x from y from z or in the case of spherical coordinates which are most
9:31:51
convenient for spherically symmetric potentials like we have for the case of the hydrogen atom
9:31:56
are from theta from phi another major difference between three-dimensional space and one-dimensional space is that in
9:32:02
three-dimensional space we have angular momentum angular momentum is not something that's going to fit into a single dimension of
9:32:08
course so let's think about how angular momentum might behave in quantum mechanics
9:32:13
the approach we're going to take in this lecture uses operator algebra the same sort of cleverness that we used back
9:32:19
when we were talking about the quantum harmonic oscillator in one dimension with raising and lowering operators
9:32:25
we're going to take a very similar approach here back to basics though first let's
9:32:30
consider angular momentum angular momentum is what you have when you have an object
9:32:36
and is rotating about some axis in classical physics you're used to
9:32:41
thinking about this as something like r times m times v the momentum and the
9:32:47
radius mvr the best way of expressing this in classical physics is as l which is a
9:32:52
vector is r vector cross with momentum vector where r is the vector that goes from the
9:32:59
axis to the object that's rotating and p is the momentum linear momentum of the object that's rotating
9:33:06
we can make an analogous expression in quantum mechanics simply by replacing the arrows with hats i know that's not
9:33:11
terribly instructive and we'll talk about that in more detail but let's define a momentum operator l hat
9:33:17
that's equal to r hat cross p hat where p hat is a vector momentum operator and
9:33:23
r hat is a vector position operator essentially
9:33:28
x hat y hat z hat as a vector crossed with p x hat p y hat p z hat if i was
9:33:38
writing things out in cartesian coordinates now at this point i'm going to save myself a lot of writing
9:33:44
and drop the hats i'll try and make it clear as i write
9:33:50
these things down what's an operator and what's not an operator but for the most part in this lecture what i'm going to be working with are operators this is an
9:33:56
operator algebra lecture after all so if you actually do the cross product between these x y and z operators and
9:34:04
these p x p y and p z operators what you end up with is well you can do
9:34:09
cross products presumably you end up with y hat p z sorry i was dropping the hats
9:34:15
wasn't i y p z minus
9:34:20
z p y that's our x component z p x minus x p z that's our y component
9:34:28
and x p y minus y p x that's our z component
9:34:33
now these are all operators and they're the same sort of thing that you're familiar with y and i'll put the hat on
9:34:38
in this case is going to be y the coordinate multiplied by something whatever the operator is
9:34:44
acting on y hat acting on that is just going to be y the coordinate times whatever it's acting on the function in
9:34:50
this case likewise for instance p y hat is
9:34:55
minus i h bar partial derivative with respect to y of whatever the operator is acting on
9:35:02
so these are the usual operators we're just combining them in a new way in three dimensions
9:35:10
now as far as answering the question of how angular momentum behaves one of the interesting questions is is
9:35:15
it quantized for instance how should we describe it the approach that we're going to take
9:35:21
here is motivated by for instance when we were talking about the position
9:35:26
operator we considered the eigenstates of the position operator those were the dirac delta functions those were useful
9:35:34
if you consider eigenfunctions of the momentum operator in one dimension you get plane wave states states with
9:35:39
definite momentum and of course if we're considering eigenstates of the hamiltonian those are the stationary states
9:35:45
whatever the operator if we consider these states the eigenstates of that operator
9:35:51
we get states with a definite value of the observable associated with that operator this is especially interesting to do in
9:35:57
the case of angular momentum so i said this was an operator algebra
9:36:03
question how can we analyze the algebraic structure of the angular momentum
9:36:08
operators well i set angular momentum operators there and there are going to be three of
9:36:14
them i'm going to break it down into lx l y and lz in cartesian coordinates because those are the coordinates that
9:36:19
are most easy to work with the way to think about these things
9:36:24
in the operator algebra context is to think about commutators and you'll see a very example very good example later on
9:36:31
of why commutators are useful but in this case for instance consider calculating the commutator of lx
9:36:37
and ly now i know what the definitions of lx and ly are in terms of their cartesian
9:36:42
coordinates so i can expand that out y p z minus z p y
9:36:49
z p x minus x p z that's what i get for l x l y and from
9:36:55
that i'm going to subtract z p x minus x p z
9:37:02
and y p z minus z p y
9:37:07
so this is l x l y minus l y l x
9:37:12
just by the definition of the community if i expand out each of these terms for
9:37:17
instance you'll get if i expand the term from the product of these two terms in the expansion i've got a y i've got a z
9:37:26
i've got a pz and i've got a px all of these coordinates are in some sense different except for pz and z
9:37:34
back when we were talking about quantum mechanics in three dimensions the very beginning of this chapter we talked about the commutators of for instance pz
9:37:42
and z being the same sort of commutator as you calculated in one dimension between say x and px
9:37:48
y and pz however commute as do y and px
9:37:55
z and py etc if the momentum and the position operators that you're
9:38:00
considering are not the same coordinate for instance if i'm not talking about x and p x y and p y z and p z the
9:38:07
operators commute so when i calculate the product here y pz times z px i have to keep the
9:38:13
relative order of pz and z constant but i can move the px and the y around
9:38:19
wherever i want what you end up getting something what you end up getting for that then is
9:38:25
something like this i'll start at the left this is going to be a kind of long an annoying expression apologies in advance
9:38:31
we're going to get a y p x p z z so y and i have to keep the pz and the z
9:38:39
in order and i'll put the px on the right for instance
9:38:44
actually you know what i'll save a simplification step here i'm going to move the px to the left because i can do that px commutes with pz and z
9:38:52
and just write pz z and i'll put parentheses around them to signify that i have to keep them
9:38:57
together in that order the next term i get multiplying across here
9:39:04
i have a y i have a pz i have an x and i have a pz so i have a pz and a pz and pz
9:39:09
of course commutes with itself it doesn't even matter the order that i write pz and itself
9:39:15
so for this term i'm going to get something like minus y x and i'll write p z
9:39:21
p z just writing it down twice if i keep expanding out these terms
9:39:27
minus z hat sorry minus z z
9:39:33
p y p y sorry p y p x
9:39:38
it's hard to read my notes here since my handwriting and my notes is even messier than my handwriting on the screen
9:39:44
x p y z p z in parentheses again from the
9:39:50
contribution of this term comes in with the plus sign because we have two minuses the z and the x commute as
9:39:56
needed as does the py and the pz but i have to keep the z and the pz in order so i've got z p z
9:40:02
x and p y being pulled out front that's for the top two terms here
9:40:07
for the bottom two terms everything is going to have a relative minus sign so i'm going to get a minus and y p x
9:40:17
z t z plus z z p y p x
9:40:24
plus x y p z p z
9:40:29
minus x p y and then pz z
9:40:35
so these are all my operators that i get as a result of expanding this out provided i've copied everything down correctly from my notes
9:40:42
now if i've done things right here you notice i have a z z p y p x here and a
9:40:47
minus z z p y p x here so these two terms cancel out
9:40:53
i have a x y p z p z here and a y x p z p z here but x and y compute so these two terms
9:40:59
are actually the same as well and they also cancel out another thing to notice here is here i
9:41:05
have ypx on the left these two terms both have ypx on the left and on the right i have things that
9:41:12
don't commute pz z and zpz so this term here
9:41:19
all right in black i can combine these together i'm going to have a y
9:41:25
p x and then a p z z minus a z and you know what that is that's the commutator
9:41:31
of pz and z the operators i can make the same sort of
9:41:37
simplification over here i have an xpy on the left and i have a commutator of pz and z over here on the right plus
9:41:45
x p y z p z commutator
9:41:50
coming from these two terms now you know what the commutator of pz and z is
9:41:56
the commutator of z and pz is i h bar
9:42:02
this is the reason we like commutators commutator-like expressions often appear in expressions like this and allow us to
9:42:08
simplify things in this case just down to a constant so this guy is going to be i h bar
9:42:14
and this which is the same commutator only with the order reversed is going to be minus i h bar you can easily verify for yourself that
9:42:20
swapping the order of the arguments in a permutator gives you minus the original commutator
9:42:26
so what i'm going to get now at the end of all this is
9:42:32
y where'd it go i have a minus on h bar and i have an ih
9:42:38
bar here so i'm going to factor that out and i'm going to have a ypx and xpy
9:42:44
which should start looking familiar ypx and xpy appears in lz
9:42:51
so this overall expression is just going to be i h bar lz
9:42:57
so we started out calculating the commutator of lx and ly and we got i h
9:43:02
bar lz you can write down expressions for
9:43:08
all of the commutators in this way the commutator of lx and ly
9:43:13
is i h bar lz the commutator of l y and lz
9:43:20
is i h bar l x and the commentator of l y sorry l z
9:43:26
and l x is i h bar l y
9:43:31
likewise if you swap the orders you get minus signs these are the commutators that are going to be useful to us
9:43:38
in considering the algebra of angular momentum if you feel the need to memorize
9:43:44
formulas like this note that the order these expressions always come in is always sort of cyclic always sort of
9:43:50
alphabetical x to y to z and back to x here i have x
9:43:55
y z here i have y z x here i have z x y always going around in this sort of
9:44:01
clockwise order you see a lot of sort of cyclic or
9:44:07
anti-cyclic sort of permutation type arguments associated with commutators like this and this is the first time
9:44:13
that this sort of thing has shown up so one thing you notice right away is that lx l and l y don't commute we
9:44:21
didn't get zero for the right hand side here what that means is that if you want to determine
9:44:27
simultaneously lx and ly you have to consider the uncertainty relation between lx and ly
9:44:33
if i want to simultaneously determine lx and ly the generalized
9:44:39
uncertainty principle from the last chapter tells me that the product of the uncertainties in lx and
9:44:44
ly is going to be given by the commutator of lx and ly and if you go back to the previous page
9:44:51
and figure out what that expression actually looks like you get h bar squared over 4 times the
9:44:56
expected value of lz squared so if i have some angular momentum in the z direction i cannot
9:45:03
simultaneously determine lx and ly what that means is that if i'm considering angular momentum i shouldn't
9:45:09
be thinking about the angular momentum in the x direction or the angular momentum in the y direction there are not very convenient observables to work
9:45:16
with what is actually a convenient observable to work with
9:45:21
is l squared which is defined to be the sum lx squared plus ly squared plus lz
9:45:30
squared essentially the squared magnitude of the angular momentum if you wanted to think about this in the classical context this is sort of like
9:45:37
saying r squared is the total length of a vector
9:45:42
so the question then is how does this l squared work
9:45:47
one thing you can do with this l squared since we're calculating commutators is ask what's the commutator of l
9:45:54
squared with for example lz can i simultaneously determine one of my
9:45:59
angular momentum coefficient direction coefficients with this total angular momentum squared sort of operator what
9:46:06
is this commutator equal to well this l
9:46:12
is going to be lx squared plus ly squared plus lz squared and we can separate out those commutators
9:46:17
lx squared commutator with lz plus commutator ly squared commutator with lz
9:46:26
and the third term is commutator of lz squared with lz
9:46:32
now the commutator of lz squared with lz is just going to be zero this term drops out this is going to be
9:46:38
lz lz lz minus lz lz lz these two commutators we have to treat
9:46:44
in a little more detail so let's expand them out this is going to be
9:46:51
l x l x l z minus l z
9:46:56
l x l x and this is going to be
9:47:02
l y l y l z minus l z l y l y
9:47:10
you can simplify this expression by adding and subtracting the sort of missing terms if you think
9:47:17
about this here i have two x's on the end and lz what about lz in the middle
9:47:23
so let's add and subtract lz in the middle here i'll write this as minus
9:47:29
l sorry minus lx lz lx plus
9:47:35
lx lz lx so i haven't actually changed this expression any i've just added and
9:47:41
subtracted the same quantity in the operator case the addition subtraction gets a little bit more
9:47:47
difficult to understand but this is essentially an identity and i can do the same sort of thing here
9:47:54
all right minus l y l z l y plus l y l z l y
9:48:03
now this we can actually work with if you notice here i have an lx on the
9:48:09
left and then an lx lz minus lzlx so if i was treating these two terms just by
9:48:15
themselves i could factor out an lz on the left and i would be left with a commutator of lx and lz
9:48:22
that would end up looking like this so this is still an equality lx on the left
9:48:28
and then lx commutator with lz accounts for this term
9:48:34
this term is accounted for in much the same way except i have to factor an lx out to the right
9:48:40
so this is going to give me an lx lz commutator with an lx on the right
9:48:48
i can make the same sort of simplifications over here for exactly the same reasons and i end up with
9:48:54
pulling the l y out to the left l y commutator with l z
9:48:59
and pulling the l y off to the right l y commutator with l z
9:49:05
l y on the right so still equal to my original expression
9:49:11
i haven't really made very much progress but i know what the commutators of lx and lz
9:49:16
are are ly and lz those were the commutators i calculated on the last page so this does actually simplify things
9:49:23
out the commutator of lx and lz is minus i h-bar l-y
9:49:30
so this whole thing is going to be lx i'll stop writing it in square brackets because it's not a commutator anymore
9:49:37
minus i h-bar l-y what i get for this
9:49:43
this commutator is the same it's going to be minus i h bar l y l x
9:49:50
plus over here i've got a y on the left and these commutators are in alphabetical
9:49:56
order so i'm just getting positive i h bar plus i h bar l y
9:50:02
now oops i forgot where to go
9:50:08
i forgot my operator the commutator of l y and l z is not just i h parts i h bar l x
9:50:14
plus i h bar l x l y now
9:50:20
if you notice here here i have an lx followed by an l y i
9:50:25
have to keep these in the right order because they don't commute but i have a minus i h bar l x l y i can bring the
9:50:30
minus i h bar out front here i have an i h bar l x l y so minus
9:50:35
i h bar l x l y plus i h bar l x l y these two terms cancel out
9:50:42
these two terms here i have an l y l x here i have an l y l x here i have a minus i h bar here i
9:50:48
have a plus i h bar these two terms commute or cancel out as well
9:50:53
so essentially what we're left with here since everything is cancelled is 0 which means that l squared does commute
9:50:59
with lz l squared commuter with commutator with lz
9:51:06
is equal to zero this is the result that we hope for it means that we don't have a generalized
9:51:12
uncertainty relation between lz and l squared which means i can simultaneously
9:51:18
determine both lz squared and sorry l squared and lz
9:51:24
that means i can hope to find eigenstates of that are so i hope to find states that
9:51:30
are both eigenstates of l squared and lz and that's really what we want when we're done with this we want something
9:51:36
that's easy to work with and eigenstates are especially easy to work with so we've worked out the general
9:51:43
algebraic properties of angular momentum operators and we've
9:51:48
settled on working with this combination l squared and lz those are operators that we can hope to work with
9:51:54
and what we're hoping to find are eigenstates things that we can you know most easily work with
9:52:02
so how are we going to proceed the way we're going to proceed is
9:52:08
ladder operators this is the same approach that we took back when we were doing the
9:52:13
one-dimensional quantum harmonic oscillator it was difficult to explain then and it's difficult to explain now
9:52:20
fundamentally if we're working with l squared and lz as our operators of
9:52:26
interest consider this just a definition l plus or minus is equal to l sub x plus or minus i l
9:52:34
sub y these should look a little bit familiar and we're in the end going to make the same sort of cleverness arguments that
9:52:40
we made back when we were doing the quantum harmonic oscillator but for now let's just consider the properties of these l plus or minuses
9:52:47
we're doing algebra with operators and we're calculating commutators so let me ask you the question what is lz
9:52:54
commutator with l plus or minus well you can substitute in the definitions of lz l plus and l minus
9:53:03
and since the commutator is linear i can just split this up into two separate commutators lz commutator with lx plus
9:53:10
or minus i times lz commutator with l y
9:53:15
you know what both of these commutators are we've already calculated them out you get i h bar
9:53:21
l y plus or minus i times
9:53:26
z and y here now are in the wrong order so i'm actually going to get a minus i h bar
9:53:32
l x in this case so this is our commutator and if you
9:53:37
simplify that down you'll find that this is actually equal to plus or minus h bar
9:53:43
l plus or minus so calculating the commutator of lz with l plus or minus gave me something
9:53:49
relatively simple it just gave me l plus or minus back if i ask you the question what is the
9:53:56
commutator of l squared with l plus or minus again you can expand out the definition
9:54:03
of l plus or minus l squared lx plus or minus i times the commutator of
9:54:09
l squared and l y but you know l squared commutes with lx
9:54:15
and l squared commutes with l y these are essentially the same as it commuted with lz
9:54:20
so without even calculating anything here we know the answer is zero
9:54:25
so this is the algebraic structure of these ladder operators the key fact that i mentioned earlier is
9:54:31
that what we're looking for are eigenstates of both of these operators
9:54:36
simultaneously simultaneous eigenstates like that essentially the question that we need to ask them that we can use these letter
9:54:43
operators to answer is if we have some state and i'm just calling it f here
9:54:48
if l squared f is going to be given is eigenvalue if f is an eigenstate of l squared it would have an eigenvalue
9:54:54
lambda for instance and f is a simultaneous eigenstate of lz it would have an eigenvalue for instance
9:55:01
mu what about l plus or minus acting on f
9:55:07
now the terminology here should be suggestive i call these things ladder operators let's see what that actually gets us
9:55:14
first of all consider l squared acting on this l plus or minus f acting on f
9:55:23
well you know that l plus or minus commutes with l squared so i can write this as l plus or minus times l squared
9:55:30
acting on f without changing anything but l squared acting on f i know what that is it's just an eigenvalue
9:55:37
multiplied by f so this is l plus or minus times acting on lambda f lambda
9:55:42
just being a constant can be pulled out front so i've got lambda and then l plus or minus f
9:55:48
what this tells you is that l plus or minus f
9:55:53
if f is an eigenvalue sorry if f is an eigen state of l squared l plus or minus
9:55:59
f is also an eigenstate of l squared with the same eigenvalue
9:56:06
i can ask the same question of lz what does lz do to this mysterious
9:56:12
quantity l plus or minus acting on f this is a little bit more complicated and i can simplify it by rewriting it
9:56:19
slightly let's say this is l z l plus or minus now i'll write this as
9:56:25
minus l plus or minus lz plus l plus or minus lz
9:56:32
i've just added and subtracted the quantity and you can see what i'm trying to do now i'm trying to arrange things
9:56:37
such that i get commutators as well as things that i know because this is all acting on f and i know what
9:56:43
lz does to f it just gives me an eigenvalue so this is now going to be the
9:56:49
commutator of lz and l plus or minus acting on f
9:56:55
plus l plus or minus lz acting on f and i know what lz does under these
9:57:01
circumstances since f is in hypothetically an eigenstate of the lz operator it's
9:57:07
just going to give me mu f back this commutator
9:57:13
i also know how this behaves this is lz in the last lecture we were able to
Angular momentum eigen function
9:57:20
purely by examination of the structure of the angular momentum operators derive the quantization properties of
9:57:27
angular momentum in quantum mechanics we were able to examine the commutators manipulate the operators and essentially
9:57:33
derive the eigenvalues associated with the operators l squared and l sub z
9:57:39
that's nice and it's very useful the eigenvectors of sorry eigenstates associated with hermitian operators in
9:57:46
the hilbert space have nice properties but we don't actually know what those eigen states look like in order to get
9:57:53
something easier to visualize let's consider what the eigenfunctions are trying to express the
9:57:59
angular momentum operators as partial differential equations that we can solve with the techniques that we've been applying earlier in this chapter
9:58:07
the angular momentum operators that we were working with in the last lecture are expressed in cartesian coordinates
9:58:14
this was very nice because the cartesian form has this nice symmetry to it and we could calculate commutators easily
9:58:21
just by manipulating these we were able to derive expressions like the eigenfunctions of l squared
9:58:26
had this sort of form h bar squared l l plus 1 was our eigenvalue
9:58:32
likewise for l sub z we ended up with eigenvalues of the form m times some constant h bar
9:58:40
the l's that we got had to be half integers e were they either zero or one half or one or three halves
9:58:48
etc and the constants m that we got here had to be between minus l
9:58:53
and l going up in steps of one so our eigenvalue structure here as i
9:58:59
mentioned doesn't tell us anything about the actual form of f when we were working with the
9:59:04
one-dimensional quantum harmonic oscillator we were able to derive for instance the ground state by knowing that the lowering operator acting on the
9:59:11
ground state gave us zero that was a differential equation that we could work with since we knew differential forms
9:59:16
for the lowering operator we can do the same thing with the angular momentum operators but in
9:59:23
this case it's more worthwhile to think more generally so suppose we just have some general psi
9:59:30
of r theta and phi this is our wave function expressed in general polar coordinates and it would
9:59:36
be nice to know how our angular momentum operators act on this general wave function if we can express our angular momentum
9:59:43
operators in spherical coordinates we can write down this sort of eigenvalue equation it will then be a partial
9:59:48
differential equation that we can solve in general for any value of l or m
9:59:54
unfortunately in this lecture we run into some thorny notational issues
9:59:59
i like to use hats to designate operators griffiths your textbook author likes to
10:00:04
leave the hats off when it's not ambiguous this is one of those cases where it is
10:00:10
ambiguous and i would like to use the hats but unfortunately hats are also significant in other ways in particular
10:00:16
hats in this section of the textbook mean unit vectors so i'm going to try and follow
10:00:22
griffith's notation and i'm going to try and point out where things are operators and where things are unit vectors but in
10:00:27
this case in this lecture if i write something like lx i mean the operator
10:00:32
and if i write something like r hat i mean the unit vector
10:00:39
like i said i'll try and be clear about what i mean in each case at any rate our goal here is to come up
10:00:46
with spherical coordinates expressions for the operators that we were working with when we were considering angular
10:00:51
momentum operator algebra l squared and l sub z so first of all let's consider just l
10:00:58
in spherical coordinates there's going to be a lot of math in this lecture and i'm going to go through
10:01:03
it only conceptually the level of grunge in this sort of coordinate transformation is
10:01:09
above and beyond what i would expect you to be able to do for an exam so most important i need you to
10:01:15
understand the overall structure the sorts of manipulations that are being done
10:01:20
change of variables in the context of partial differential equations is tricky
10:01:26
so let's try and just understand overall how it works first of all what we're working with is
10:01:32
angular momentum l which is given by r cross p now i've left both vector hats and
10:01:38
operator hats off of these but this is the angular momentum operator this is the position operator in spherical
10:01:44
coordinates and this is the momentum operator in spherical coordinates the momentum operator in spherical
10:01:50
coordinates is rather straightforward to write down we can write it as minus i h bar times this laplace times this
10:01:55
gradient operator del which you know as i'll write it in cartesian
10:02:00
coordinates x hat times the partial derivative of x plus y hat times the partial derivative with respect to y
10:02:06
plus z hat times the partial derivative with respect to z you can apply this to an arbitrary
10:02:12
function of x y and z a scalar function and it will give you a vector so this is a vector as is the momentum so this is a
10:02:18
sort of momentum vector operator this gradient can be expressed in
10:02:25
spherical coordinates as well and expressed in spherical coordinates it has this partial derivative with respect to r partial derivative with respect to
10:02:31
theta and with respect to phi the partial derivatives with respect to theta and phi have to be rescaled since
10:02:37
for instance if you consider it in cartesian coordinates this is essentially a spatial rate of change it's a vector that points in the
10:02:43
direction that the function changes most quickly with respect to physical space
10:02:49
and a change with respect to theta is not changed with respect to physical space r
10:02:54
d theta is a motion in space whereas r sine theta d phi is a motion in space so
10:03:00
these are our motions in space and the rescaling necessary is taken care of by this one over r and this one over sine
10:03:05
theta this gradient gives us the momentum which we can cross
10:03:12
with the radius operator the position operator in spherical coordinates which is quite simply r r
10:03:19
hat so this hat now designates a unit vector
10:03:25
and this designates a coordinate and as usual our position operator is multiplication by the
10:03:30
coordinate in question of well the multiplication of this with whatever the operator is acting on some
10:03:37
function in this case so our angular momentum then is going to be a cross product of something like i
10:03:44
don't know why i erased it r r hat so i'm going to be taking the cross product of r hat that's the vector part of my
10:03:50
position operator with this part of my momentum operator i can pull my minus i h bar out and this
10:03:57
is what you end up with simply taking cross products r cross r r cross theta and r cross v
10:04:03
where here i had a one over r in my gradient but it's been cancelled out by the r-coordinate multiplication in my
10:04:10
position operator likewise for fee there was a 1 over r here as well
10:04:17
this can be simplified slightly you know that r cross r is going to be zero the cross product of any vector
10:04:23
with itself is going to be zero since the cross product depends on the angle between the vectors they have to be pointing in different directions
10:04:31
r cross theta is going to give me phi hat the unit vector pointing in the ph hat direction
10:04:36
and r cross phi is going to give me minus theta hat a unit vector pointing in the minus theta direction
10:04:43
you can therefore and only you're only going to end up with two terms
10:04:48
and that will be our angular momentum operator since however what we were actually doing when we were working with l
10:04:55
squared and lz we needed expressions for instance for things like l plus or minus
10:05:02
this l plus or minus was expressed in terms of lx and ly so what we actually need to do is take the overall angular
10:05:09
momentum operator in spherical coordinates and use it to find angular momentum operators in cartesian
10:05:15
coordinates expressed in spherical coordinates now this is a very strange way of saying things but essentially
10:05:21
what i want is the angular momentum about the x-axis the x-component of the angular momentum
10:05:27
but expressed still in spherical coordinates the way to do that and the way griffiths
10:05:33
uses at least is to take this expression for the angular momentum operator which has ph hat and
10:05:40
theta hat in it and express the phi hat and the theta hat in cartesian coordinates
10:05:46
those cartesian coordinates values of theta hat and phi hat will depend on
10:05:51
theta and phi so we end up with this weird hybrid cartesian chord cartesian spherical coordinate system
10:05:57
but doing so allows you to identify the x component of the angular momentum y
10:06:03
component and z component if you actually do that substitute in
10:06:08
phi hat in cartesian coordinates for instance
10:06:13
phi hat in cartesian coordinates this weird cartesian spherical coordinate system is minus
10:06:19
sine phi i hat plus
10:06:25
cosine of theta j hat where i hat and j hat now are
10:06:31
cartesian coordinate unit vectors this would normally be written as x hat in a normal physics class but of course we
10:06:36
know x hat has the x component position operator and we can't reuse that notation
10:06:41
you can see why i'm sort of glossing over the details of this actually doing it all out would require a fair number
10:06:47
of slides and a good deal of your time at any rate substituting in this
10:06:52
expression for instance for phi hat and a similar expression for theta hat you can identify the i hat component of
10:06:59
l the x component of the angular momentum and when you do that this is what you're left with so the x component of the angular
10:07:05
momentum has derivatives with respect to both theta and phi likewise for l sub y the y component of
10:07:11
the angular momentum l sub z however only has derivatives with respect to phi and this should make a fair amount of sense since z is
10:07:18
special in spherical coordinates phi is the angle that rotates around the z axis
10:07:24
so that's all well and good we're starting to work our way towards
10:07:29
expressions of the operators that we're actually interested in l squared and l sub z we have one for
10:07:36
l sub z but what about l squared l squared it turns out is easy to express if you think about it
10:07:43
in terms of the l plus or minus operators this was the trick that we used back when we were doing operator
10:07:48
algebra l plus or minus of course is expressed in terms of lx and l y
10:07:53
but we have lx and l y now so we're ready to go l plus or minus
10:07:59
being expressed in terms of l x and l y going back to your notes from the
10:08:04
lecture on the algebraic structure of the angular momentum operators we can express l squared rather simply
10:08:10
in terms of l plus and l minus l plus an l minus being expressed in terms of lx and l y we can make
10:08:17
combinations of lx and ly multiplying those out is simply a exercise in
10:08:24
calculus multivariable calculus taking partial derivatives applying chain rules etc
10:08:29
when you do all of that evaluating this expression that we got from the algebraic structure of l squared in
10:08:35
terms of l plus l minus and lz squared and lz you can go and look that up in your
10:08:40
notes you end up with an expression for l squared
10:08:46
this should start looking reasonably familiar what i really want to do here
10:08:51
is write this into an eigenvalue problem by adding some
10:08:58
arbitrary function f this whole operator acting on some function f is going to be
10:09:04
equal to we know what the answer is from our consideration of operator algebra it's going to be h bar squared l l plus
10:09:11
1 times f it's going to give us our original function back
10:09:16
so this right here this is our partial differential equation that we can solve
10:09:22
for f where f now is a function of r theta and phi and is going to
10:09:28
essentially give us our wave function we only have angular components here
10:09:33
so they're really there isn't going to be any radial part that should make a good amount of sense radial motion doesn't contribute any angular momentum
10:09:42
we can do something very similar for l sub z l sub z acting on some arbitrary
10:09:47
function and l sub z we already had an expression for is minus i h bar partial derivative with respect to phi
10:09:53
of f we know what that's going to give us already as well because we know the eigenvalue structure of l sub z as well
10:10:00
it's going to give you m times h bar f
10:10:05
both of these are going to be then partial differential equations that we can solve this tells us something about the
10:10:12
eigenstates of l sub z this tells you something about the eigen states of l squared
10:10:17
and if you look at these equations they should be familiar these are the angular equations
10:10:26
that we had earlier these essentially gave us
10:10:31
the ylm of theta and phi as their solution so what we've shown here
10:10:38
is that the eigen functions associated with the l squared and l sub
10:10:44
z operators are exactly the spherical harmonics the spherical harmonics were what we got from
10:10:50
a century a spherically symmetric potential expressing the time independent schrodinger equation in spherical coordinates and this should
10:10:57
make a certain amount of sense since what we're talking about now is angular momentum and l squared for instance angular momentum
10:11:03
squared has to do with the rotational kinetic energy so it ought to play some role in the
10:11:09
time-independent schrodinger equation which tells us the energy of the stationary states so if we have an eigenvalue of l squared
10:11:17
simultaneous eigenstates of l squared and lz are exactly the spherical harmonics
10:11:25
there is a slight difference here and it comes down to the value of l
10:11:32
essentially we have two classes of solutions here we have half integer l and integer l
10:11:38
our consideration of spherical harmonics gave us only integer l whereas sorry our consideration of wave
10:11:45
functions these the solutions to these partial differential equations give us spherical
10:11:50
harmonics which are only meaningful for integer l or is your yes integer l
10:11:56
half integer l doesn't really make any sense in the context of spherical harmonics which means what we're ty if what we're talking about is angular
10:12:02
momentum of something like a physical particle orbital angular momentum rotational kinetic energy essentially
10:12:09
we can't have half integer l but we do have these half integer l solutions
10:12:15
if i'm talking about wave functions i have to have
10:12:22
ylms for my solution that means i have to have l being 0 1 2
10:12:27
etc and m being you know minus l up to l
10:12:33
if what i'm just talking about though is the algebra of things then i don't really know what the
10:12:39
solutions look like but i can have l is zero or half or one
10:12:44
or three halves this is interesting my m values are going to behave the same
10:12:50
way minus l going up to l but these half integer
10:12:55
values of l they're uh they're rather strange
10:13:00
they're going to behave in ways that are utterly unfamiliar if what you're used to thinking about are things that actually live in ordinary
10:13:07
three-dimensional space but these do actually happen to have physical reality
10:13:12
and it has to do not so much with orbital angular momentum the motion of a particle around in orbit for instance as
10:13:18
they do with spin angular momentum or at least that's the name quantum mechanics quantum mechanists i think i don't think
10:13:25
i should say quantum mechanic quantum mechanists say is associated with these half integer
10:13:30
values they have physical meaning in the context of spin angular momentum
10:13:38
as an example of how these angular momentum structures can be useful consider the rigid rotator
10:13:44
what i mean by that is suppose i have two masses both equal to mass m
10:13:49
separated by some distance a and i put them on a rod of length a
10:13:54
and i spin them around this is a you know
10:14:00
system that can in principle be treated with quantum mechanics the only energy associated with this
10:14:06
system is going to come from rotational kinetic energy since the thing is not allowed to translate i'm fixing it to
10:14:11
rotate about the center here so my hamiltonian operator
10:14:17
is going to essentially be the rotational kinetic energy which is going to be l squared
10:14:22
over 2 times the moment of inertia this is the rotational analog of p
10:14:28
squared over 2m i have angular momentum squared divided by twice the moment of inertia the rotational equivalent of the mass
10:14:36
now i suppose i should either erase the hat from my hamiltonian operator or add a hat to my angular momentum operator i
10:14:41
said in this lecture i wasn't going to use hats to designate operators so i'll erase it from the hamiltonian
10:14:48
at any rate you know how l squared behaves the moment of inertia here i is going to be
10:14:54
two since i have two masses times m r squared essentially so the mass times
10:15:00
the radius squared which is going to be a over 2 squared
10:15:05
so this is going to be m a squared over 2 for my moment of inertia
10:15:11
the time independent schrodinger equation then becomes h times my wavefunction is e times my wavefunction
10:15:17
that's my original when i substitute in the specific definition of the hamiltonian here i have l squared my
10:15:23
l squared my squared angular momentum operator divided by twice my moment of inertia which is just m a squared i have
10:15:30
an over 2 here and i have a 2 here and they cancel each other out if this is going to be equal to e
10:15:36
sorry l squared acting on psi is going to be equal to e times psi m a squared here is a constant
10:15:43
i can rearrange this and write l squared psi is equal to m a squared e times psi
10:15:51
this now this m a squared e this is my eigenvalue of an eigenvalue problem with l squared
10:15:57
in it i know what those eigenvalues are this is h bar squared l l plus 1
10:16:02
that's my eigenvalue the form of my eigenvalues of the l squared operator
10:16:08
so what that tells me is that m a squared e is equal to h bar squared l l
10:16:14
plus 1. and i can solve this for e easily it tells me that e is equal to
10:16:20
an equal side somewhere h bar squared l l plus 1 divided by m a squared
10:16:29
these are the allowed energies the energies of the stationary states for the rigid rotator
10:16:34
you can just as easily go through the same sorts of arguments and write down normalized wave functions for the rigid
10:16:41
rotator but essentially this is a very common structure that you're going to encounter
10:16:47
in quantum mechanics angular momentum is of course a conserved quantity in classical physics and it's a conserved quantity in quantum
10:16:53
mechanics as well which means it's interesting in a lot of respects and the quantum mechanical structures
10:16:58
you get either if you're looking at something like a rigid rotator now since we could actually write a real world
10:17:06
wave function for this we're stuck with just spherical harmonics for the wave functions integer values for l
10:17:13
and you're going to encounter this sort of expression a lot in quantum mechanics especially if you go on to the upper
10:17:18
levels think about for a moment what we've accomplished solely by messing with operators and
10:17:26
solving partial differential equations as motivated by this original hypothesis of the time or the time dependent
10:17:33
schrodinger equation we were able to determine conserved angular momentum structures
10:17:40
we're even able to predict that there's going to be something strange happening for half integer values of l in these eigenvalue equations and that's going to
10:17:47
be the topic of the next section in the textbook spin the half integers have a lot of strange
10:17:53
properties associated with them so that's where we are and that's where we're going the machinery of quantum mechanics is
10:17:59
obviously very productive and we're going to keep working our way through the results of it for the next couple of
10:18:05
lectures we've spent the last couple of lectures talking about angular momentum
Spin in quantum mechanics
10:18:12
from the quantum mechanics perspective we ended up talking about a total angular momentum operator l squared and
10:18:17
a z component of angular momentum operator l sub z these two operators gave us a certain
10:18:24
algebraic structure and we ended up with quantum numbers l and m
10:18:29
the allowed values of l were either integers or half integers l could be zero
10:18:35
a half 1 3 halves etc going up to infinity in steps of a half
10:18:42
whereas m could only be in between minus l and l in steps of 1.
10:18:48
these quantum numbers were interesting from for a couple of perspectives if we considered the motion of a particle for instance the electron
10:18:55
orbiting the nucleus in the hydrogen atom we only got integer values of l zero one two three etc
10:19:01
whereas the algebraic structure of these operators allows for l equals a half or three halves etc going up in steps of a
10:19:08
half these half integer values are essentially valid solutions and that brings us to the topic of spin
10:19:14
in quantum mechanics essentially these half integer values of l are perfectly valid physical solutions
10:19:21
and they have meaning they're actually what we use to describe an intrinsic property of fundamental
10:19:26
particles like electrons called their spin spin is essentially a property of the
10:19:32
universe that's just the way things are i don't have a good answer for why does
10:19:38
an electron have spin but i can describe the spin of the electron and i can describe it using the
10:19:43
same language as we used when we were discussing angular momentum so angular momentum we were working with
10:19:49
equations like l squared f and the eigenvalues we got for that were h bar squared l l plus 1.
10:19:58
likewise l sub z applied to f gave us eigenvalues of the form m h bar
10:20:05
examining the algebraic structure of this gave us allowed values for the l quantum number of zero or a half or one or three
10:20:13
halves etc these integer and half integer values
10:20:19
have different interpretations if i look at just the integer values
10:20:24
those describe orbital angular momentum the angular
10:20:30
momentum of particle as it moves in a circle around around a focus for instance around the center
10:20:36
so now we're talking about particle motion and we can write a wave function
10:20:43
psi of say x y and z or perhaps more accurately r theta and phi
10:20:49
that has this property of orbital angular momentum you know what the answers for this are already we've discussed in in
10:20:56
previous lectures the wave functions with specific values of l squared and l sub z the eigenfunctions
10:21:03
of the l squared and l sub z operators are the spherical harmonics
10:21:10
we're also allowed to have spin angular momentum with integer values
10:21:16
but spin is really more interesting when we're talking about the half integers one half three halves
10:21:22
five halves etc i keep writing three thirds i wonder why
10:21:28
here these half energy cases don't have any nice wave function that we can express
10:21:34
so we're really only talking about spin under these circumstances
10:21:40
so what exactly is this spin thing
10:21:46
i can't give you a good argument or a good answer for this other than saying this is essentially just a
10:21:51
property of the universe the name spin at least i can explain and the name comes from a classical analogy
10:21:57
suppose we have a positively charged nucleus and a negatively charged electron
10:22:04
orbiting that nucleus we are going to have
10:22:11
orbital angular momentum associated with the motion of that electron but there's also the possibility
10:22:18
that the electron itself would be rotating
Two particles system
10:22:24
we've built up over the past few chapters a fairly complete understanding of how single particles behave in
10:22:29
quantum mechanics we can describe them with wave functions like psi of x y z functions of position
10:22:36
which we can use to calculate expected values of for instance of what the x coordinate will be
10:22:42
we know how to calculate the allowed set of energies for bound states for instance of the hydrogen atom when we
10:22:48
can predict the spectra this is very nice and it's very useful but it's of course not the end of the
10:22:54
road for quantum mechanics the next step that we're going to make is to talk about multiple particle
10:23:00
systems to start building things that are more complicated than a single particle in a single potential
10:23:05
the first step then is to expand on our formalism of wave functions to two particle systems
10:23:12
if we're working with a one particle wave function psi of x y and z
10:23:17
if we're working with two particles we're no longer we no longer have the position of just one particle x y and z
10:23:25
we're working with two particles so the wave function psi is going to be a function of six variables x1 y1
10:23:32
z1 and x2 y2 and z2
10:23:38
this means if we construct for instance a probability density for finding the particle at a particular position we're
10:23:43
not finding the particle there are two particles there are two positions and what we get is a joint probability
10:23:49
distribution for the position of both particles so this is if we're talking about two
10:23:55
particles and you can easily imagine what would happen if we had more particles you would have simply more arguments
10:24:00
this is part of what makes quantum mechanics so difficult to compute with since effectively representing functions of
10:24:06
many variables in the computer is a very difficult proposition
10:24:12
if our wave functions are functions of multiple variables you might expect that our hamiltonians would get more complicated as well and they do the
10:24:19
hamiltonian operator which we had before was simply in a single part in the single particle
10:24:25
case was a momentum operator and a potential operator now you'll have to deal with the momentum of each particle separately
10:24:31
so for instance the hamiltonian for a particle might look like minus h bar squared over 2m times and i'll write
10:24:37
this as gradient squared with a subscript 1 minus h bar squared over 2m
10:24:45
gradient squared with a subscript 2 where the gradient with the subscript 1 refers to partial derivatives with
10:24:51
respect to x1 y1 and z1 and the subscript 2 refers to partial derivatives with respect to x2 y2 and z2
10:24:58
essentially this is the momentum of particle 1 in operator formalism with wave functions and this is the momentum
10:25:03
of particle 2. the potential energy now of course will also have to be a function of the
10:25:10
positions of both of these particles so we'll have to add on a potential term which is a function of both r1 vector
10:25:16
and r2 vector there are some simplifications that you can make if the potential is only a
10:25:21
function of the separation of the particles for instance you can do the same sort of thing as you can do in the
10:25:26
case of the two body problem in classical physics namely instead of working with two independent bodies work
10:25:32
with the center of mass and the essentially angular orientation of the bodies about the center of mass
10:25:39
but that's a that's a story for another day the hamiltonian we get here is now a
10:25:45
partial differential equation in multiple variables more many more variables than we were working with originally
10:25:52
so it's much harder to work with our wavefunctions of course still have
10:25:57
to be normalized since we still have to represent well probability densities with them but
10:26:03
the normalizations we're going to work with are a little different in particular while the probability density that we're working with is still
10:26:10
going to be psi star psi we're going to have to integrate it over many many dimensions
10:26:16
six dimensions in this case if i'm working with two particles in three dimensions dx1 dy1 dz1 dx2 dy2 dz2
10:26:27
so if you're trying to normalize a wavefunction for two particles in three dimensions and cartesian coordinates you've got a lot of integrating to do
10:26:36
the time independent schrodinger equation is going to look very similar
10:26:41
essentially h psi equals e psi same as before where
10:26:46
the hamiltonian now is an operator h h hat
10:26:51
the solutions you get to the time independent schrodinger equation are still going to behave the same way they behaved before and this is the very
10:26:57
comforting thing when we derive the time independent schrodinger equation from the time dependent schrodinger equation we still
10:27:04
get the same sort of behavior our wavefunction now is a function of the positions of two particles if i
10:27:10
represent them as vectors r1 and r2 as the spatial part the solution to the time independent schrodinger equation
10:27:16
and the time dependence looks very much the same minus i e t
10:27:21
over h bar the same sort of expression as we got before so adding multiple particles adds a
10:27:27
great deal of complexity to the spatial part of the wave function but if we have a stationary state the temporal evolution is as simple as it was before
10:27:35
the subtle point of multiple particle wave functions comes from whether the particles are distinguishable or
10:27:40
indistinguishable consider combining two one-dimensional systems so the position of particle one
10:27:46
is represented by x1 position of particle two represented by x2 so we have two particles in a
10:27:52
one-dimensional system essentially and the positions of those particles are independent this looks a lot like two independent
10:27:59
variables so you can think about this as in two dimensions an x1 axis and an x2
10:28:04
axis if i measure the positions of these particles at the
10:28:10
same time i illuminate the system with high energy radiation and look for where the radiation is scattered off of the
10:28:15
positions of the particles i can represent the outcome of a measurement by a point in this two-dimensional space suppose this point
10:28:22
is 1 0.3 i might also measure the particles to be
10:28:28
here another possible outcome for this measurement is 0.3 comma 1.
10:28:34
what i mean by whether the particles are indistinguishable or distinguishable is whether these two outcomes 0.31 or 1 0.3
10:28:40
are actually distinct if i was measuring this in a two-dimensional space these points would of course be very distinct but i don't
10:28:47
actually have a two-dimensional space i have a one-dimensional space with two particles in it
10:28:53
so if i measure say this outcome in one dimensional space i'm measuring one particle at point zero
10:29:00
point three and another particle at position one so my wave function then essentially has
10:29:06
a particle there and a particle there if i measure this other outcome 1 0.3
10:29:12
one of my particles is at position one one of my particles is at position 0.3 so my wavefunction essentially looks
10:29:18
like that these guys are essentially the same
10:29:23
what does that mean well if this is particle and this is particle b and this is particle a and
10:29:29
this is particle b then these two outcomes are different but that requires the particles themselves to be distinguishable and if
10:29:35
the particles are not distinguishable if this is an electron and this is an electron there is no difference in
10:29:41
principle between the electrons in these in these two peaks then well electron electron and electron
10:29:48
electron are actually the same outcome and whether or not you count these is different as well
10:29:53
one of the nuances of quantum mechanics the essential fact that you have to keep in mind is that in quantum mechanics the
10:29:59
particles that we're working with electrons protons photons whatever they may be are in principle and distinguishable the
10:30:06
wave function quantum mechanics tells us is all we can principle know about these particles so you can't paint one of them
10:30:13
red or put a little piece of tape on it or do whatever you might do with other objects in order to keep track of
10:30:18
whether or not they've exchanged places for example particles
10:30:24
are indistinguishable
10:30:31
indistinguishable is a painfully long word but essentially what this means is that we can't tell which particle is
10:30:37
which so let's consider what this had what effect this has on quantum mechanics
10:30:44
if you had particles that were distinguishable particle one
10:30:50
its position being represented by the coordinate x one could be in some wave function size in some state psi sub a
10:30:56
and this would be quantum mechanically a complete description all of the information necessary about particle one
10:31:02
likewise for particle two indexed by coordinate x2 in state psi sub b
10:31:08
the combined wave function for the overall state then is going to be psi as a function of x1 and x2
10:31:16
and we can write that down if particle one is inside site state psi a and particle two is in state psi d as simply
10:31:23
the product psi a of x1 times psi b of x2
10:31:29
this gives us the sort of expression that you would expect to get for distinguishable particles namely for instance if i want to calculate the
10:31:35
expected value of x1 for a particle in this state this is the expected position of particle one
10:31:41
my combined wave function here calculating the expected value in this combined wave function will require two
10:31:46
integrals one dx1 and one integral dx2 both integrals are going to go from
10:31:52
minus infinity to infinity and the integrand as before is going to be psi star psi
10:31:59
if i expand that out psi a star of x1 side b star of x2
10:32:05
x1 and psi a x1 psi b of x2
10:32:11
this is the integrand you would get psi star and psi combines together with a multiplication means that's our probability density for position and
10:32:18
this is then of course the expected value of position formula that we're familiar with from single particle quantum mechanics
10:32:25
looking at what's a function of what here we can simplify things a little bit i have functions of x1
10:32:31
and i have functions of x2 if i pull the terms that are not
10:32:37
functions of x2 out of the x2 integral essentially moving them over here what i end up with
10:32:43
is two functions or two integrals that you probably recognize integral from minus infinity to infinity dx1 of psi
10:32:49
sub a star of x1 x1 psi sub a of x1
10:32:54
for my first integral and the integral from minus infinity to infinity of with respect to x2 of side b star of x2
10:33:03
psi b of x2 so these integrals essentially separate out
10:33:09
and this is a normalization integral for size of b
10:33:15
if psi sub b is normalized this is going to go to 1. and this expression on the left the
10:33:21
integral with respect to x1 is the single particle expectation value of the position x1
10:33:28
for a particle in the state a so essentially if i have distinguishable particles
10:33:33
my result looks pretty much as expected
10:33:39
these particles are clearly distinguishable because if the expected value of the position of the particle
10:33:44
were different for state b as for state a well i got the expected value for state a not some combination involving
10:33:51
the expected value for state b so these particles are clearly distinguishable
10:33:56
and there's nothing in principle wrong with writing wave functions like this except for the fact that the fundamental
10:34:01
particles we're working with are not distinguishable so we have to somehow encode the indistinguishability of
10:34:06
particles into our formulation of quantum mechanics so how do we write
10:34:12
what write a wave function for indistinguishable particles the key fact
10:34:18
is what happens if we exchange the particles the wave function for particle one particle two versus the wave
10:34:24
function for particle two particle one exchanging the positions at which we evaluate coordinates
10:34:30
is essentially if you think back to that plot i was making earlier of x1 and x2
10:34:35
it's implying a degree of symmetry between this point and this point that my wavefunction must must be equal
10:34:43
here and here essentially being equal somehow across the axis
10:34:48
this sort of line here where x1 equals x2 that degree of symmetry apply implies
10:34:53
some constraints on allowable forms of the wavefunction
10:34:58
we don't need just that the wavefunction itself doesn't change if i exchange x1 and x2
10:35:04
what we need is for the observables not to change and furthermore we need the observables not to change at all if we
10:35:10
swap the particles back to where they were originally so if we want the exchange of particles
10:35:16
to not matter let's define an exchange operator
10:35:22
p hat now don't worry we're not going to be
10:35:27
working with p-hat in the context of it as a mathematical operator but it's a useful notation to use
10:35:32
what we need in order for the wave function essentially to not change the observables is for p hat
10:35:39
acting on psi x1 x2 which is more or less defined to be psi
10:35:45
of x2 x1 we need that to be equal to plus or
10:35:51
minus psi of x1 x2
10:35:56
you know the way to not change the observables in quantum mechanics is to multiply by a complex phase and this
10:36:02
plus or minus essentially takes care of that complex phase you could imagine any arbitrary e to the
10:36:07
i phi being multiplied by psi and that would not change the observables but the fact that applying the exchange operator
10:36:14
twice gets us back where we started means that the phase that we multiply by has to be either 0 or pi meaning we have
10:36:20
to either go from plus psi to minus i or from plus psi to plus side either we don't change the wave function
10:36:26
at all by exchanging operands or we flip the sign of the wave function by
10:36:32
exchanging the exchanging the particles this is sort of a law of physics
10:36:39
the indistinguishability of particles requires this to hold
10:36:44
if i exchange the order of the arguments of a two particle wave function i must get my original wave function back with
10:36:50
a plus or minus sign this symmetrization or anti-symmetrization
10:36:55
under exchanging the arguments symmetry referring to the plus sign anti-symmetry referring to the minus sign has some
10:37:02
remarkable consequences which we'll talk about over the next couple of lectures
10:37:08
one way however to write down these wave functions since that's what we're going to want to do in the end
10:37:14
is if i have the two single particle states that i was working with in the
10:37:20
past slide psi a psi b my wave function psi of x1 x2
10:37:26
started off as psi sub a of x1 psi sub b
10:37:32
of x2 this was the distinguishable particle wave function and it turns out that if i combine
10:37:38
this with a permutation of x1 and x2 for instance psi a of x2 instead of psi a of
10:37:45
x1 and then psi b of x1 instead of x2 if i combine these two pieces
10:37:52
with either a plus sign or a minus sign i get something that obeys
10:37:57
the requirement that the particles are indistinguishable from the perspective of quantum mechanics
10:38:03
if i'm going to properly normalize this i'll need a normalization constant out front
10:38:08
and for instance you can check this fairly easily if i wanted to know what psi of x2 x1
10:38:16
was here well it's going to be this expression on
10:38:22
the right exchanging twos for ones and ones for twos so it's going to give me a psi a of x2
10:38:29
side b of x1 plus or minus psi a of x1 side b
10:38:35
of x2
10:38:41
if you compare the expression i get after exchanging these particles with the expression i got before exchanging these particles you can see here psi ax1
10:38:48
psi bx2 a1 b2 whereas here's a2 b1 a2 b1 so
10:38:54
these expressions are essentially the same except the plus or minus sign is going to mess things up a little bit if i use the plus sign clearly these two
10:39:01
expressions are the same a1 b2 plus a2 b1 versus a2b1 plus a1 b2 all i've done
10:39:07
is exchanged the order of these two terms since this is just multiplication we're working with wavefunctions there's nothing fancy about the order of the
10:39:13
terms over addition everything commutes that's fine if i use the minus sign
10:39:19
i have a1 b2 and minus a1 b2 in my
10:39:25
exchanged version whereas minus a2b1 becomes plus a2b1 in my exchanged
10:39:30
version so i flip the signs in my wavefunction if i use the minus sign when i calculate my exchanged form
10:39:39
so this trick for making indistinguishable particle wave functions from distinguishable particle wave functions actually always works you
10:39:46
need to combine all the different permutations of all of your particles with appropriate plus or minus signs such that you obey this overall
10:39:53
anti-symmetry under exchange or symmetry under exchange requirement
10:39:59
whether or not we have symmetry or anti-symmetry under exchange is a really interesting topic
10:40:04
and it gets us down to a distinction that i've mentioned earlier on in the context of fermions and bosons
10:40:11
essentially
10:40:17
indistinguishability has a couple of consequences
10:40:23
first of all if i have the
10:40:28
plus version the symmetry under exchange essentially psi of x2 x1 equals psi of
10:40:35
x1 x2 my exchanged version is equal to my original version
10:40:41
this is the case for bosons and bosons were the particles that we talked about earlier that had spin
10:40:47
integer spin 0 1 or 2 etc
10:40:52
if you make the other choice say psi of x to x1 is equal to minus psi of x1
10:41:00
x2 that's the case for fermions and fermions we said earlier were
10:41:06
particles with half integer spin spin one half three halves five halves etc on up to infinity
10:41:15
there's actually quite a lot that you can do with this
10:41:21
for instance the symmetry and anti-symmetry properties of
10:41:26
these wave functions have well it has observable effects and the behavior of fermions and bosons is crucially
10:41:34
different in a lot of ways that have very important consequences for instance
10:41:40
earlier on we talked a little bit about superfluid helium in the context of the domain of quantum mechanics and whether
10:41:46
that was important or not helium atoms are boson with integer spin
10:41:53
and they obey very they have very different behavior than other liquid gases for instance if you wanted to
10:42:00
determine the quantum mechanical behavior of a very cold liquid hydrogen for instance it would behave differently
10:42:06
hydrogen behaves differently from helium in that context the indistinguishability of particles is
10:42:13
something of an axiom in quantum mechanics
10:42:18
the exchange can't affect anything in particular it doesn't affect
10:42:28
the hamiltonian exchanging two particles should not affect the energy of the state if the particles are completely
10:42:34
indistinguishable put another way the exchange operator and the hamiltonian
10:42:40
operator commute the commutator of p-hat and h-hat is zero
10:42:46
what that means is that we can always write
10:42:52
always write wave functions in these forms
10:42:58
x2 x1 after exchange equal to plus or minus psi of x1 x2
10:43:05
we can do that and still be able to come up with stationary states we can come up with a
10:43:11
simultaneous set of eigenstates a set of simultaneous
10:43:16
eigenstates of both this exchange operator and the hamiltonian so it's always possible to write our
10:43:22
wave functions like this this is similar to the reasoning we applied earlier when we were talking
10:43:28
about functions or about the time independent schrodinger equation in one dimension with an even potential you
10:43:34
could always write the solution as either even or odd if the potential is even in one dimension
10:43:40
you can make a similar argument or this is a very similar argument there is a symmetry property that we can
10:43:45
exploit when we're looking for solutions of multiple particle wave functions
10:43:53
so bosons and fermions and exchange these are fundamental properties of nature
10:43:59
and the connection between the spin of the particle and the symmetry or anti-symmetry of the wave function overall
10:44:06
is a really interesting topic that we'll discuss a little more later on one application that you guys have
10:44:12
hopefully heard about from your chemistry class is the pauli exclusion principle
10:44:17
the poly exclusion principle holds for fermions and for fermions we know that the exchange operator acting on the wave
10:44:23
function psi gives you minus the wave function psi so suppose the wave function we were
10:44:29
working with was writable in the form that we were talking about earlier psi of x1 x2 is equal to some normalization
10:44:37
constant times psi a x1 psi b x2 now we're using the minus sign since
10:44:43
we're talking about fermions we're talking about exchange anti-symmetric spatial wave functions
10:44:50
psi a x2 psi b x1 for our second term as before
10:44:58
the polyexclusion principle determines what happens if the two particles are in the same state
10:45:04
if the two particles are in the same state psi a is equal to side b
10:45:10
what that means is that i can rewrite this as psi a
10:45:15
and rewrite this as psi a you can tell what we're left with now we've got psi a x 1 psi a x 2 minus psi
10:45:24
a x 2 psi a x1 we've got essentially something minus itself
10:45:30
so if the particles are in the same state
10:45:35
then psi of x1 x2 equals 0 with this particular fermion
10:45:42
anti-symmetry under exchange this is interesting and i suppose i
10:45:48
shouldn't use an exclamation point here because 0 factorial is 1 and that wouldn't be all that interesting
10:45:55
but what this means is that this well this is not possible first of all
10:46:02
the wave function psi equals 0 is a perfectly valid solution to the schrodinger equation but it doesn't tell you anything so this is not useful it
10:46:08
does not describe a normalizable state
10:46:13
what this means and what the poly exclusion principle says is that two fermions
10:46:21
cannot occupy
10:46:26
the same state
10:46:33
not quite sure how i spelled occupy there but i don't think it was right
10:46:40
two fermions cannot occupy the same quantum mechanical state and that comes from the fact that fermions are required
10:46:46
to obey anti-symmetry under exchange and of course if you have two particles in the same state exchanging things doesn't
10:46:52
do anything it's not going to change your wave function so if it's not going to change your wave function and yet it is going to change
10:46:58
your wave function by giving it a minus sign you've got a problem two fermions cannot occupy the same
10:47:04
quantum mechanical state as a result and this comes just from the nature of
10:47:09
indistinguishable particles the anti-symmetric combination to render
10:47:14
two otherwise distinguishable particles indistinguishable means that those two particles cannot occupy the same state
10:47:21
for bosons though we use the plus sign so that's no
10:47:27
problem if we use a plus sign here we end up with psi a x 1 psi a x 2 plus
10:47:34
psi a x 2 psi a x 1 so just twice psi a x 1 side b x 2
10:47:40
or sorry psi a x 2. so that's a perfectly valid wave function bosons if we use the plus sign make the
10:47:46
symmetric instead of anti-symmetric combination to render the particles indistinguishable those particles can occupy the same state right off the bat
10:47:53
this ability to occupy put multiple particles into the same quantum mechanical state is the
10:47:59
difference between the bizarre behavior of liquid helium and the behavior of liquid hydrogen
10:48:06
as an example consider back to the very beginning the very first quantum mechanical system we
10:48:11
worked with was a particle in a box what happens if we put two particles in a box
10:48:16
well two particles in a box if we're going to write wave functions
10:48:23
as symmetric or anti-symmetric combinations of our distinguishable single particle wavefunctions is a
10:48:28
little bit of a lie because if these particles are anything that we know of realistically those particles will interact
10:48:34
and the interaction in the hamiltonian will affect the potential so we won't be working with a simple v
10:48:41
of x equals zero inside the box and infinity outside the box potential we'll be working with something more
10:48:46
complicated and accounting for that interaction will mean that our stationary states are not simply the stationary states of single particles
10:48:54
but suppose for instance vigorously waving my hands that you can't really see it in a video lecture
10:49:01
suppose those particles didn't actually interact then the potential would not be affected
10:49:06
and the stationary states would indeed be the single particle stationary states
10:49:13
if i have distinguishable particles then i can write down my states as for instance psi
10:49:19
n m of x1 x2 has
10:49:24
well the product of the state for n and the state for m
10:49:30
psi sub n of x1 psi sub m of x2
10:49:36
the ground state for instance and i'm going to smush this down
10:49:43
give myself some space the ground state then has n and m both
10:49:50
equal to one in this case looks like my normalization overall out front 2
10:49:56
over a different normalization since i've got the product of two separately normalized functions times sine of pi x1 over a
10:50:05
sine of pi x 2 over a and it has energy
10:50:11
if i substitute 1 for the energy of one particle and one for the energy of the other particle i'm
10:50:18
just going to get k plus k my total energy is 2k
10:50:24
the first excited state and there are two ways i can do this i could write psi 2 1 or
10:50:30
psi 1 2 depending on which particle i bump up from the ground state
10:50:36
is going to be very similar it's going to be 2 over a
10:50:42
sine pi x 1 over a sine 2 pi
10:50:49
x 2 over a if for instance i use this combination
10:50:55
so there are actually two distinct ways to write the first excited state one where i put the 2 with the x1 and the
10:51:01
other where i put the 2 with the x2 that means this first excited state for
10:51:07
this distinguishable particle's case is doubly degenerate there are two allowable states with the
10:51:13
same energy that's what we mean when we say degeneracy
10:51:18
suppose instead of distinguishable particles i had bosons the states that i would work with then
10:51:25
would look very similar if i had psi 1 1 my ground state
10:51:30
well there's nothing wrong with putting two quantum mechanical particles in the same quantum state with bosons so i'd have to
10:51:37
make the symmetric indistinguishabilization
10:51:42
sure why not i'll make up a word the symmetric form of this sine of pi x1 sine of pi x2
10:51:49
plus sine of pi x2 sine of pi x1 but since they're the same that's all just going to end up
10:51:54
adding up so your ground state is essentially going to be unchanged from your
10:51:59
distinguishable particle case if your distinguishable particles are in the same quantum state are they really all that distinguishable
10:52:06
so psi 1 1 is unchanged
10:52:13
the first excited state however that looks a little different psi one two for instance
10:52:20
let me actually not write it as psi one two let me write it as psi first excited
10:52:28
and that's going to be a symmetric under exchange version of the distinguishable particle
10:52:34
wave function here such that the particles are rendered and distinguishable what it ends up looking like
10:52:40
is root 2 over a times sine of pi x 1 over a
10:52:48
sine 2 pi x 2 over a plus sine 2 pi x
10:52:56
1 over a sine pi x 2 over a
10:53:04
so i've moved the 2 from the term with x 2 to the term with x1 and if you calculate observables with this first
10:53:10
excited state you'll get a different result than if you had two distinguishable particles
10:53:16
for instance if i calculate the expected position of particle 1 or particle 2 i'll get the same answer which is a
10:53:21
requirement if the particles are going to be distinguishable one thing to notice about this is that
10:53:28
if i try to swap which of x1 or x2 has the two it doesn't work i get the same
10:53:34
quantum mechanical state back so this is non-degenerate
10:53:40
there is only one allowed quantum mechanical state for the first excited state for bosons
10:53:47
degeneracy does have consequences in the physical world so the fact that distinguishable
10:53:52
particles and non-distinguishable particles have different degeneracies for the first excited state
10:53:57
means that well it means we're on to something there should be some observable consequences for this prediction
10:54:04
the last possibility fermions
10:54:09
well what about the ground state psi 1 1 the pauli exclusion principle tells us
10:54:14
that no two fermions can occupy the same quantum mechanical state and in fact if you look at our psi 1 1 state here and
10:54:22
try to make a anti-symmetric under interchange version of it by adding on
10:54:27
essentially another term that looks exactly like this or more accurately subtracting off a term that looks
10:54:32
exactly like this you get 0. so the ground state doesn't exist there's no psi 1 1 under these
10:54:39
circumstances our new ground state then
10:54:47
is essentially our first excited state from before but with a minus sign
10:54:52
and i'll indulge in a little copy pasting here just to save myself the writing
10:55:03
the only difference here is that we have a minus sign to render the two states
10:55:10
anti-symmetric under exchange and we're combining two terms such that the resulting state is a valid
10:55:17
state for indistinguishable particles so our ground state again which
10:55:23
corresponded to our first excited state before is also non-degenerate there's only one
10:55:28
allowable state here for our for our ground state only one quantum mechanical state
10:55:35
and well fermions bosons and distinguishable particles obviously behave very differently here fermions
10:55:41
and bosons differ in the sense that the ground state is different indistinguishable particles and
10:55:47
distinguishable particles differ in the sense of the degeneracy of whether or not of the states
10:55:53
so there's a lot of interesting phenomena here and it all boils down to this fundamental fact that
10:55:59
quantum mechanical particles are indistinguishable there is no difference between two electrons any two electrons
10:56:05
are essentially exactly the same they have this they obey the same laws of physics there is no additional information here
10:56:11
that would allow us to keep track of which electron is which we can make
10:56:16
quantum mechanics validate this approach or keep we make quantum mechanics fail to keep
10:56:22
track of which particle is which by making these symmetric or anti-symmetric combinations of what would otherwise be
10:56:27
distinguishable particle wave functions and lo and behold the distinguishable particles bosons and fermions all behave
10:56:33
differently so there's a lot going on here to check
10:56:38
your understanding just to get drive home the complexity of multi-particle wavefunctions i'd like
10:56:44
you to write down the normalization integral for a three-particle wavefunction in three-dimensional space
10:56:50
finally reflect on what it means for two fermions to be non-interacting if they can't occupy the same quantum
10:56:56
mechanical state those two particles in a box that i did on the last slide
10:57:01
for a fermion they couldn't exist in the same state but i wrote down the
10:57:06
ground state excuse me i wrote down the stationary states from
10:57:13
which i was constructing those anti-symmetric and symmetric combinations by stating that the particles didn't
10:57:19
interact so what does it mean for two things that don't interact to exclude each other from doing something
10:57:26
and finally what i've been talking about in the context of the particle in a box is just the spatial wave function we're
10:57:32
just talking about psi of x for instance or in the case of two particles psi of x1 x2
10:57:39
how would that change if i included spin particle one and particle two will now have independent spins which you can
10:57:45
think of as extra arguments to your wavefunction so how might the inclusion of spin affect this symmetrization or
10:57:51
anti-symmetrization these are things to reflect on and if you've got these down i think you've got the basics of
10:57:57
multi-particle quantum mechanics soundly in your mind
Free electrons in conductors
10:58:04
quantum mechanical systems with many particles in them are very difficult to solve in principle imagine trying to write down the wave
10:58:10
function for a system of 10 to the 23rd not quite independent particles that
10:58:16
would be very very complicated and under most circumstances the best that we can hope for is to uncover the general
10:58:22
structure of the solution what sort of energies are going to be allowed for example what we're getting into now is the
10:58:28
basics of the quantum mechanical structure of solids which is of course an incredibly rich subject being as it
10:58:34
is essentially the basis for all of material science all of semiconductor physics
10:58:41
one aspect of the theory of solids that we can actually do reasonably accurately at least from a qualitative perspective
10:58:47
is the behavior of free electrons in conductors and that's the topic of this lecture
10:58:52
free electrons in a conductor are something that we can work with reasonably well because if we think about a chunk of material for instance
10:59:00
as being the space over which some electron
10:59:05
a conduction electron is free to wander the particles are essentially free
10:59:14
the electrons however will never be found outside the box or outside the material it's very unlikely for an
10:59:20
electron to wander off into the air surrounding a chunk of conductor conductors just don't do that so the
10:59:26
particles are not found outside the box the electrons are confined
10:59:36
you can probably see what i'm getting at here we have free particles that are never going to be found outside of some
10:59:42
rectangular region this is starting to look like the particle in a box so maybe we can work with that
10:59:48
what about a particle in a box
10:59:54
well a single particle in a box that's easy enough to handle but what about multiple particles in a box what if i
11:00:00
have a second particle here that's also wandering around on its own well provided i make the
11:00:07
very very inaccurate yet useful assumption that these particles don't interact much
11:00:13
i can actually work with that
11:00:22
now i'll put a star on that sort of a footnote asterisk because this is not a very good assumption that the electrons
11:00:29
in a metal don't interact essentially what the assumption amounts to is that on average particles aren't
11:00:35
going to interact much two randomly chosen electrons in a metal are unlikely to have just recently
11:00:41
collided for example and that on average the vast sea of
11:00:46
electrons that are not free to move about this equalize the charges to the degree that any two conduction electrons
11:00:53
are unlikely to encounter the the free charges of either the nucleus free charges of the other electrons or free
11:00:59
charges of other conduction electrons those are some pretty stiff assumptions and they're probably not correct
11:01:06
but if we make those assumptions we can actually solve this problem and figure out what the quantum mechanical structure is that's a very useful thing
11:01:12
to do so we're going to go ahead and do it the starting point though is a single
11:01:18
particle in a box the single particle in a box in three dimensions is something that we've
11:01:23
talked about and the hamiltonian that we're working with is essentially just given by the momentum squared the
11:01:29
kinetic energy h bar squared over 2m times the gradient operator in three dimensions
11:01:35
we also have to multiply by a potential which is now going to be a function of x y and z where the potential we're
11:01:40
working with v of x y and z now is equal to well 0 if we're inside the
11:01:46
box and that's going to happen for x y and z in between
11:01:52
let's say l sub x l sub y and l sub z and 0 respectively so if x
11:01:59
is between 0 and lx y is between 0 and l y and z is between 0 and lz the particle is officially in the box and the
11:02:06
potential energy function is 0. we say the potential energy is infinity outside the box
11:02:12
to enforce the particle to always be inside the box this is essentially identical to our
11:02:18
one-dimensional particle in a box we just have more dimensions to work with and the solution procedure is very
11:02:23
similar the schrodinger equation we're working with is as usual the time independent schrodinger equation h psi equals e psi
11:02:31
and if we make our usual separation of variables assumption that psi is given by some function of x multiplied by some
11:02:37
function of y multiplied by some function of z what you end up with is
11:02:43
three separate independent one dimensional particles in a box infinite
11:02:48
square well potentials essentially one in the x direction one in the y direction and one in the z direction
11:02:53
the overall energy of your combination after you've done separation of variables is given by essentially the energy contributed by the x and the
11:03:00
energy contributed by the y and the energy contributed by the z independent one-dimensional particles in a box
11:03:08
the wave functions that you get psi of x y and z are products then of one dimension three
11:03:15
one dimensional particles in a box the normalization you get is eight divided by l x l y l z in a square root
11:03:23
sign and then you have your sine functions as usual for the 1d particle in a box sine of nx pi x over lx
11:03:31
where the quantum number that you get as a result of the boundary conditions i'm calling nx for the x part and y for the
11:03:37
y part and nz for the z part and y pi y
11:03:43
over l y sine of n z pi z
11:03:49
over l z that's your wave function for a single particle in a three-dimensional box
11:03:56
the general solution you get in separation of variables as usual has sine and cosine terms in it but the
11:04:02
boundary conditions not only fix our quantization give us quantum numbers n x n y and n z
11:04:07
but also eliminate the cosine terms just because the wave function must go to zero at points where the wave where
11:04:13
the potential diverges to infinity the quantization also sets the allowed
11:04:20
energies of the system and the energy of this state is given by
11:04:25
h bar squared pi squared over 2m and then we have a combination involving
11:04:31
these quantum numbers nx squared over lx squared plus ny squared over ly squared
11:04:37
plus nz squared over lz squared now this looks like a sum of three
11:04:43
things squared and it's useful to make this look more like the magnitude of a vector in three dimensions essentially
11:04:50
i'm going to define this i'm going to define a vector or a scalar quantity for instance k squared a k
11:04:58
vector such that this overall energy here is equal to h bar squared k squared
11:05:03
over 2m looking like the kinetic energy of a particle with
11:05:08
wave vector k k being essentially 2 pi divided by the wavelength
11:05:14
the k vector that we're working with then is for instance given by kx is equal to
11:05:20
pi nx over lx likewise 4k y equals pi
11:05:27
n y over ly and k z is equal to pi n z over l z
11:05:34
where the overall k squared is kx squared plus ky squared plus kz squared
11:05:42
if i make these definitions the overall energy now starts to look like the squared magnitude of a vector in a
11:05:49
three-dimensional space with three separate components kx ky and kz and this k-space three-dimensional space is
11:05:57
the space that you want to think about in terms of the quantum mechanical structure of many particles in a 3d box
11:06:03
which is of course where we're going with this so what happens when we have many particles in a box
11:06:09
well we know we're working with fermions here and fermions obey the polyexclusion
11:06:15
principle which means we're not going to be able to put more than two fermions in exactly
11:06:20
the same quantum state so if i'm trying to occupy
11:06:26
many many many states here i'm going to need many states to be well
11:06:31
i'm going to understand the structure of many states so thinking about this in terms of the
11:06:37
three-dimensional k vectors say this is my kx direction this is my
11:06:43
ky direction and this is my kz direction the overall allowed values that i had
11:06:50
for my energy were given by specific integers essentially dividing these k
11:06:55
axes up into specific points kx was defined by pi and x over lx for instance
11:07:01
so nx being 1 2 3 etc like for our one quan or
11:07:07
one-dimensional particle in a box i essentially have a set of ticks along my x-axis here my
11:07:14
k-x axis that tell me what the allowed values of kx are likewise i have a set of allowed values
11:07:21
for ky and instead of allowed values for kz
11:07:26
and it's going to be hard for me to draw this out in three dimensions but if you think about the allowed
11:07:32
values where these things all intersect
11:07:37
when i have an allowed value of kx and allowed value of ky and an allowed value of kz
11:07:43
i have an intersection point there that means i have an allowed quantum
11:07:48
state here for nx is 1 and y is 1 and nz is 1. i of course also have
11:07:55
an allowed quantum state out here where
11:08:00
nx is 2 and y is 1 and n z is one
11:08:06
and i'm not doing a very good job drawing this but you can see each intersection point here is associated with some cube
11:08:14
between the intersection and the origin and that cube signifies a certain volume
11:08:20
and the volumes in k-space are something that's very useful to think about
11:08:32
so this point now here would represent k y is 2 k z is 1 k x is 1.
11:08:37
each of these points is associated with a cube and the volume of this cube which is going to become important when
11:08:44
we start talking about trying to fill as many of these states as possible is given by well the length of each of
11:08:51
these sides i'm talking about the volume in k-space now this of course being associated with nx
11:08:56
equals one this is pi divided by lx is the length of this side
11:09:03
in k space likewise this is going to be pi over lz the y of course is going to be pi
11:09:09
divided by l y so if i wanted to know the volume of one of these cubes
11:09:17
in k-space it would be
Band structure of energy levels in solids
11:09:23
you saw in the last lecture how just considering the electrons in a conductor to be free particles in a box you could
11:09:29
get a reasonable impression of the quantum mechanical behavior of those electrons what the allowed energies look
11:09:35
like what the behavior of the metal was even to some degree we were able to calculate for instance the
11:09:40
degeneracy pressure of the electrons in that state and get an answer that was comparable to the measurable physical properties like the
11:09:47
bulk modulus of the material that free particle assumption seems very
11:09:53
fishy though because those conduction electrons are going to interact with the atoms in some way so what i'd like to talk about in this
11:10:00
lecture is how we can include the atoms and the results in particular the band structure of energy levels in solids
11:10:08
including the atoms in the behavior of the free electrons in a material for instance is a rather
11:10:14
complicated process you might think about an electron coming in towards some atom
11:10:20
where we have electrons orbiting the nucleus of the atom and how these particles might interact now we know
11:10:26
from quantum mechanics that this picture is just plain not correct that we need to consider the electron as it approaches the atom as some sort of a
11:10:32
wave packet so i'll draw some wave fronts and the atom itself as being composed of
11:10:38
a nucleus which has almost negligible wave nature compared to the wave nature of the electron since the atom is since
11:10:44
the nucleus is so much heavier surrounded by some cloud of electron
11:10:51
describing the interaction of a wave packet like this and an atom with an electron cloud surrounding it is a very
11:10:57
complicated process in principle but whatever the interaction is it's going to be encoded by some hamiltonian
11:11:05
h hat which is going to include the kinetic energies of the particles and then some potential that tells you how the energy
11:11:12
of this interaction takes place if the electron were very close to the atom would there be an attraction force would
11:11:17
there be a repulsive force would there be an increase in energy or the or decrease of energy now typically you can assume that the
11:11:23
potentials like this are related just to the relative displacement between the
11:11:28
atom and the electrons so some difference between the position of the electron and the position of the atom
11:11:34
perhaps the potential even only depends on the absolute magnitude of that vector only depending on the distance between
11:11:39
the electron and the atom either way these potentials can come in a variety of forms
11:11:45
but if you're trying to consider a material with many electrons and many atoms
11:11:52
what you're going to have to work with is actually going to be a sum over all the atoms of the material of the contribution of each atom to the energy
11:11:59
of an electron if we have multiple electrons we'll have to have lots of different kinetic energy terms and we'll
11:12:05
have to have a sum over electrons here as well so this is a very complicated hamiltonian we can't really hope to
11:12:10
solve it analytically we can however make some analytical progress if we make some simplifications
11:12:17
and i'm going to make three simplifications for this lecture first of all this potential which is in
11:12:23
principle a function of the distance between the electron the position of the electron and the position of the atom
11:12:29
i'm going to pretend it only depends on the magnitude of the distance and i'm going to make a very crude
11:12:35
approximation to this potential namely that if the electron is right on top of the atom it experiences a very strong repulsive force if the electron is
11:12:42
displaced by the at from the atom significantly the atom overall looks neutral and there is no energy associated with that reaction
11:12:48
the approximation i'm actually going to make then is that the potential contribution of a single atom to an electron is given by a dirac delta
11:12:56
function some proportionality constant describing the strength of the delta function times
11:13:02
the delta function itself as the distance of between the electron and
11:13:07
the atom so this is the potential that we're going to work with
11:13:13
this is just for a single an interaction between a single electron and a single atom however and we're going to have to
11:13:18
consider multiple atoms and in order to make any mathematical progress we're going to have to know the positions of
11:13:24
all the atoms in any realistic material the atoms will be more or less randomly distributed though there may be some overall
11:13:30
structure dictated by the structure of the bonds between those atoms i'm going to assume a very very simple structure here i'm
11:13:36
going to assume that we're working with a crystal so we're working with a regular array of
11:13:43
atoms for example furthermore
11:13:48
i don't really want to mess with trying to express this regular array of atoms in three dimensions so i'm going to
11:13:53
assume that we're only working with a one-dimensional system essentially a one-dimensional crystal
11:14:00
just looking at a slice through a potential three-dimensional crystal
11:14:05
this is not the most relevant physical scenario since a dirac delta function in one dimension
11:14:11
extrapolated to three dimensions is sort of a sheet delta function not an array of point delta functions like a crystal
11:14:17
so this is not the most realistic scenario but it does actually reproduce a lot of the observed behavior of well
11:14:24
real electrons in real crystals the potential we're talking about here then is going to be a one dimensional
11:14:31
array of delta functions so our v of x is going to look something like this
11:14:38
it's going to be zero whenever you're not on top of an atom and it's going to spike up whenever you are on top of an
11:14:43
atom and this is going to continue potentially infinitely in both
11:14:49
directions this is called a dirac comb since i guess it kind of looks like a
11:14:55
comb and it's made of delta functions so this is the potential we're going to
11:15:00
work with the nice feature of this potential is that if these atoms are say spaced by some distance a
11:15:07
this is a periodic potential and there are theorems that help us deal with periodic potentials
11:15:13
one of these theorems is called blocks theorem and what it states is that or for a potential that's periodic namely
11:15:19
the potential evaluated at some displacement a from the current position is just equal to the potential at the
11:15:24
current position the solutions to the time independent schrodinger equation for that potential
11:15:30
can be written as follows psi of x plus a displacing the argument of psi
11:15:35
is essentially the psi at the current location multiplied by some complex constant with magnitude 1 e to the i k a
11:15:43
for some unknown constant k essentially what this means is that
11:15:50
the observables don't change you know multiplying the wave function by some complex number isn't going to some
11:15:57
complex phase e to the ika isn't going to change the answer well
11:16:04
essentially what this means is that for a completely periodic potential the observables aren't going to change
11:16:10
from one period to the next and that's more or less a requirement periodic
11:16:15
potentials should have periodic solutions to the schrodinger equation we don't know anything necessarily about
11:16:21
this constant k but essentially what we're talking about if we apply this to our
11:16:28
delta function potential or our dirac comb potential
11:16:35
is atoms spaced apart by some distance a and block's theorem tells us that the
11:16:41
wave function here gives us the wave function here gives us the wave function here gives us the wave function here so we don't need to worry
11:16:47
about the entire space we can only worry about a sub portion of the space
11:16:53
this is very useful one unfortunate consequence of blocks
11:17:00
theorem is that it only works for completely periodic potentials so if we're talking about a material
11:17:06
a chunk of silicon say
11:17:11
there are edges in the inside here we definitely have a periodic potential
11:17:16
we have a silicon crystal we have an array of atoms that's fine we're working with
11:17:22
something periodic but at the edges we're going to have problems
11:17:29
since at the edges well the periodicity obviously breaks down under these circumstances then
11:17:34
block's theorem isn't going to apply so we need to find out some approximation some simplification or at least some
11:17:40
plausibility argument for how we can still apply lock's theorem to these cases
11:17:45
well we've already made a lot of simplifying assumptions so what's one more our potential v of x is this direct cone
11:17:53
structure that potentially continues
11:17:58
to infinity if we're working with an a real realistic material we're going to have
11:18:05
something like 10 to the 23 atoms here as such the contribution of the atoms
11:18:12
you would expect if you had a free electron here it's going to be much much much more sensitive to the atoms nearby
11:18:17
than to the boundaries of the material as such you wouldn't expect the edge effects to be terribly significant
11:18:25
so one way to fix blocks theorem if we're willing to
11:18:30
ignore the edge effects and deal just with electrons near the interior of the material
11:18:36
is to take our delta function potential and wrap it around essentially treat
11:18:42
this edge of the material as connected somehow through a wormhole to this edge of the material wrapping the material
11:18:48
around in a circle for instance working with a donut of material instead of a block of material
11:18:54
what this periodicity means that we're assuming the potential is
11:19:00
periodic overall not just periodic from one atom to the next
11:19:06
is that our wave function psi of n times a essentially the wave function on the right edge of our material
11:19:13
has to be equal to the wave function on our left edge of the material
11:19:18
and let me rewrite this i have my wavefunction as a function of
11:19:24
x and if i add n times a where i have n atoms from one side of the material to
11:19:29
the other side of the material times the separation of the atoms i've essentially wrapped all the way around and gotten
11:19:35
back where i started that has to give me my original wave function back
11:19:40
so that's my periodicity and under these circumstances
11:19:46
bloch's theorem which tells me how to displace my wavefunction by a certain amount tells me what i need to know
11:19:56
block's theorem gives us that psi of x plus n a is going to be equal
11:20:03
to e to the i capital n capital k a times my original wave function psi of x
11:20:10
my periodicity then means this is going to be equal to psi of x which i can just cancel out then from
11:20:16
this periodicity equation giving me e to the i n capital k a
11:20:22
equals one that tells me that this capital k
11:20:27
constant i have can only take on specific values and those specific values are given by
11:20:33
what will make the exponential one essentially two pi times an integer divided by
11:20:38
capital n a the argument here has to be 2 pi times an integer
11:20:44
and this is then the value of k that's going to give you 2 pi times an integer when you multiply it by n times a essentially
11:20:51
so n now is going to be some integer either 0 plus or minus 1 plus or minus 2
11:20:56
etc knowing something about this constant tells us how the wave function in one
11:21:02
region relates to the wave function in the next region and we have a variety of allowed values for this overall constant
11:21:12
so we have now what we need to solve the time independent schrodinger equation the potential we're working with and
11:21:18
i'll just draw a chunk of it here just with say two spikes
11:21:24
let's say this is the spike at x equals zero and this is the spike at x equals a
11:21:30
i'll add another spike here on the left at x equals minus a
11:21:35
we need to go through our usual machinery for solving the time independent schrodinger equation
11:21:40
we have our potential and in regions say there
11:21:47
the potential d of x is equal to zero which means our time independent schrodinger equation is just going to be
11:21:54
the free particle equation minus h bar squared over 2m times the second derivative of psi
11:22:00
with respect to x is equal to e times psi you know what the solution to this is
11:22:06
we've done the free particle case many many times our general solution is that psi of x
11:22:12
is equal to a times sine kx plus b
11:22:18
cosine kx where k squared is equal to 2 m e
11:22:24
over h bar squared this should all look familiar it's solving a second order differential equation essentially the
11:22:30
simplest second order differential equation you can think of
11:22:35
the subtlety with solving the schrodinger equation under these circumstances is that the general solution in one sub region isn't enough
11:22:41
we have to find the solution in all regions which means we're going to have to match boundary conditions
11:22:47
so it's also useful to know then what the solution is at some other region so that i can match those two solutions
11:22:52
together across the delta function block's theorem tells us that the solution in this region is going to be
11:22:59
the solution in this region multiplied by some e to the i k x e to
11:23:04
the i k a excuse me since we're not shifting to the right we're shifting to the left it's actually e to the minus i k a but our solution in
11:23:11
this region psi of x is equal to e to the minus i capital k a
11:23:17
times our solution in this region a sine k
11:23:22
x x plus a
11:23:29
plus b cosine x plus a
11:23:35
so i'm writing now this x is referring to negative values so i have to shift it over to make it correspond to the values
11:23:42
in the other region and i multiply by this overall constant to make sure everything matches up
11:23:49
so we have our solutions now in this region and in this region and these are general solutions
11:23:55
we have this capital k in here which we know a little bit about from the overall periodicity but we also have this unknown k constant
11:24:02
which is given in terms of the energy now typically the solutions to the schrodinger equation matching boundary conditions
11:24:09
tells us something about the allowed energies and that's going to be the case here as well but these are our two general solutions
11:24:15
and let's figure out how boundary condition matching at this boundary works since that's going to tell us
11:24:21
something about the energy something about these a's and b's and so how that information all connects to these capital k's
11:24:28
so the boundary conditions we have are going to match these two solutions together
11:24:34
we have two boundary conditions and just to recap we have our delta function potential
11:24:40
x equals zero and we have our solution in this region and our solution in this region and we're
11:24:46
matching them across the delta function at x equals zero so the two boundary conditions we
11:24:53
have for the wave function first of all the wave function has to be continuous
11:24:59
what that means is that psi of zero plus has to be equal to psi of zero minus the
11:25:05
solution just on this side has to be equal to the solution just on this side of our boundary
11:25:11
and if i plug these in the solution for 0 plus substituting 0 in for x the sine term is going to drop
11:25:18
out since sine of 0 is 0 and the cosine term is going to go to 1 since the cosine of 0 is 1.
11:25:23
so the b is all i'm going to get that's all that's left here this term's dropped out this term is just equal to b
11:25:29
so my equation then is b is equal to whatever i get when i plug 0
11:25:35
in for the solution on this side so substituting in zero for x the x's are going to drop out and i'm just going to
11:25:41
get cosine ka and sine ka my a and b and my e to the minus i ka
11:25:46
e to the minus i capital k a times a
11:25:52
sine lower case k a plus b cosine
11:25:58
lowercase k a so that's our continuity boundary condition
11:26:04
the other boundary condition that we have to work with is that typically the first derivative of the
11:26:09
wave function is continuous the exception to that typical boundary condition is when the potential goes to
11:26:15
infinity you can have a discontinuity in the first derivative and the only case
11:26:20
that we know of that we can solve so far in this course is the delta function potential we talked about this when we
11:26:26
were doing bound states for the delta function so if you're fuzzy on how this actually works i suggest you go back and refer to
11:26:33
the lecture on bounce state solutions to the delta function potential otherwise the equation we need to tell
11:26:40
us how d psi d x is discontinuous
11:26:46
relates the size of the discontinuity to the strength of the delta function
11:26:51
potential the equation and this is equation 2 125 in your textbook is that the delta of
11:26:59
the psi dx is equal to 2m alpha over h bar squared
11:27:07
psi so we need to calculate the first derivative of the wavefunction from the left and from the right subtract those
11:27:14
two and that's then going to be related to the value of the wave function and these constants where alpha here is the
11:27:19
same constant that we used to describe the strength of the delta function potential when we first introduced the structure
11:27:24
of the potential so if you actually go through calculate the derivative of this with respect to x
11:27:30
the derivative of this with respect to x what you end up with is well the derivative for this we're then
11:27:37
evaluating our derivatives at x equals zero and a lot of the terms drop out the derivative of this term from the plus
11:27:43
direction at x equals zero is k times a this is a lowercase k now
11:27:53
the derivative from the left derivative of this potential with respect to x evaluated at x equals zero is
11:28:02
e to the minus i capital k a times
11:28:09
times a lowercase k from the derivative and then capital a cosine
11:28:15
ka minus capital b sine
11:28:20
ka that's the left-hand side of our discontinuity equation here
11:28:27
discontinuity in the first derivative then being equal to 2 m alpha over h bar squared and the value of psi
11:28:35
at 0 well i could use either the left-hand side of this equation or the right-hand side of the equation but well left-hand
11:28:41
side here is much simpler so i'm just going to use capital b for the value of my equation
11:28:46
now we have two equations and we have a lot of unknowns to work with we have capital a capital b
11:28:52
capital k and lowercase k but it turns out we can come up with a useful relationship just by manipulating these
11:28:57
equations to eliminate capital a and capital b essentially what you want to do is you
11:29:03
want to solve this equation for a sine ka multiply this equation through by sine
11:29:09
ka so that we have an a sine ka here and an a sine ka here and then use the result of solving this
11:29:16
equation to eliminate capital a so making that substitution you're going
11:29:22
to have a capital b from this equation so you'll have a capital b in this term capital b in this term and then capital b in this term in
11:29:28
this term as before which means you can divide out your capital b's so you've successfully eliminated both capital a
11:29:35
and capital b from your equation the subtle term as far as simplification
11:29:41
goes is trying to get rid of this e to the minus i capital ka and but if you see if you make the
11:29:46
appropriate simplifications you can reduce this down not to completely eliminate capital k but to at least get rid of the complex
11:29:52
form of the exponential you end up with a cosine e to the i capital k a when you
11:29:57
finish solving this so subject to a lot of algebra that i'm
11:30:03
skipping the end result here that we can actually work with can be expressed as cosine
11:30:09
capital k a is equal to cosine lowercase ka
11:30:15
plus m alpha over h bar squared lowercase k
11:30:22
sine lowercase ka so this is an equation that relates
11:30:29
lowercase k which is related to our energy to uppercase k which is what we got out
11:30:34
of block's theorem and the strength of the delta function the massive and the mass of the particle
11:30:40
this is then going to tell us essentially the allowed energies there were very few restrictions on the value of this capital k that was just related
11:30:46
to some integer the equation then just copying it over from the last page
11:30:52
can be expressed well this is just the previous equation capital k is related to some integer n
11:30:59
and lowercase k is related to the energy so if i look at the left hand side here
11:31:06
what do i actually have to work with well my capital k think about the set of allowed values for capital k
11:31:15
cap k just being related to an integer which can be positive or negative is going to have a lot of allowed values
11:31:22
keep in mind now that capital n here is something of order 10 to the 23. so we have a lot
11:31:30
a very large number in the denominator and we have potentially relatively smaller numbers in the numerator so
11:31:35
capital k is going to have very densely spaced allowed values going you know over the allowable values
11:31:42
of n which are essentially the integers up to some very large number
11:31:47
so my allowed space of k value of capital k values are a packed negative
11:31:52
and negative and positive keep in mind however that my capital k's
11:31:58
are being substituted into a cosine so no matter what i use for capital k it
11:32:03
gets multiplied by a i'm going to have something between 0 or between -1 and 1 for the outcome here
11:32:11
the right-hand side of this equation depends on lowercase k which depends on the energy
11:32:18
so you can think of lowercase k here as being essentially the energy of the state so we have something that depends on the energy and it looks like cosine
11:32:25
of something related to the energy plus some constant times sine of something related to the energy divided by
11:32:30
something related to the energy you can simplify these a little bit in particular i'm going to write
11:32:38
i'm going to redefine the variable z equal to lowercase k times a
11:32:45
which means this is going to be cosine z plus some constant times sine z over z
11:32:51
so i'm going to define beta being equal to where to go it's going to be
11:32:58
m alpha a over h bar squared leaving me with a ka in the denominator
11:33:04
so my right hand side now which is what i'm plotting here is going to be cosine z plus beta
11:33:13
sine z over z so if i plot my right hand side for a particular value in this case i'm using
11:33:19
beta equals 10 beta just being a combination of the strength of the delta function spacing of the potentials mass
11:33:25
of the particle and plots constant you end up with a function that looks sort of like this
11:33:30
it looks kind of like sine x over x well it does but this z parameter is now
11:33:35
related to the energy so essentially we have an x axis here that tells us the energies
11:33:41
and we know we can have solutions whenever it's possible to solve this
11:33:46
our capital k space densely packed with allowable values of
11:33:51
capital k being plugged into cosine is going to give us very densely packed
11:33:56
values of well essentially the y-axis here whatever the y-coordinate is
11:34:02
since there are so many allowable values of capital k since capital n here is a
11:34:08
very large number you can think of these essentially as a continuing continuum of allowable values
11:34:14
on the y-axis the places where i have a solution that are going to depend on
11:34:19
well the right-hand side of my equation which is only between -1 and 1 for certain values of the energy
11:34:25
so these shaded regions here where the energy of the state is such that the
11:34:32
right hand side of this equation corresponds to values between -1 and 1
11:34:37
for which we can find a nearby allowable value of cosine capital ka these are the
11:34:42
allowed energies
11:34:48
and they come in bands there is no single isolated value of the
11:34:55
ground state energy there is sort of a continuum of allowable energies subject to these approximations that capital n
11:35:02
is very large for instance so for dealing with a macroscopic chunk of material the allowed energy states
11:35:08
for a free electron that's encountering these atoms are going to come in energy
11:35:13
bands this is actually a really really nice result because it allows us to
11:35:19
understand a lot of the properties of things like conductors insulators and semiconductors
11:35:25
if for instance we allowed bound states to exist as well they would have negative energies
11:35:30
so our free electron states are going to appear in separate bands our bound states are also going to appear in bands
11:35:36
as well and you can verify that by going through the solution process using delta function wells instead of delta function
11:35:41
barriers but if we have no bound or no free electrons if we just
11:35:49
have bound electrons if we just have states down here essentially we don't have enough free
11:35:55
electrons don't have enough electrons period in this state to occupy all of our possible bound states
11:36:02
then we have an insulator
11:36:07
if we have states populated again same as in the previous lecture starting with the lowest energy and populating states as
11:36:13
you go up you'll have an insulator until all of these bound states are filled
11:36:19
once you start filling states in this first sort of energy band of free electrons you have a conductor
11:36:29
it's very easy for electrons in an energy state here to shift to another state energy state of slightly higher or
11:36:35
slightly lower energy that may be slightly displaced in the conductor so it's possible for an
11:36:41
electron to move from one side of the conductor by moving from one of these free particle states to another
11:36:48
if we have all of our bound states filled and
11:36:53
the complete conduction band or a complete band here also filled well that's going to be an
11:36:59
insulator again because it's impossible for electrons to move from one state to the other if all of the available states are filled the only way for an electron
11:37:06
to effectively become free here is for it to jump up to the next energy band across this gap
11:37:14
we have gaps between our bands and that determines whether or not we've
11:37:20
got a conductor or an insulator a third case that you've probably heard of is if we have well all of our
11:37:28
bound elect bounce states filled and almost all or perhaps just a few
11:37:36
states in the next energy low energy state filled this we would call a semiconductor
11:37:45
it can act like a conductor if you have these few extra electrons filling the lowest energy states in a
11:37:52
mostly empty band but if you lack these few electrons then
11:37:57
you've gone to the insulating state so they're states that are sort of on the boundary between
11:38:03
entirely filled and mostly empty if you add a few electrons that acts like a conductor if you
11:38:08
subtract a few electrons it acts like an insulator and this transition between conductor and insulator is something that we can
11:38:14
arrange chemically and electrically and this is essentially the basis of all of
11:38:20
semiconductor physics we'll talk in the next lecture about how semiconductor devices like diodes
11:38:27
and transistors actually work in the context of these allowed energy bands and what sort of chemical modifications
11:38:34
happen as a result another note here is that
11:38:39
the temperature affects the energies that are allowed here the next section in your textbook after
11:38:46
this talks about quantum statistical mechanics which tells you about as a function of the temperature of the material how
11:38:52
uh how these energy states are likely to be populated the approximation that we're making here by saying start filling the energy
11:38:59
states from the lowest energy possible and continue until you run out of free electrons isn't entirely accurate
11:39:05
that's essentially assuming that everything is at absolute zero that there is no additional energy available
11:39:10
to these materials now conductors insulators and semiconductors behave differently in the
11:39:16
context of temperature because for instance consider a conductor or consider an
11:39:21
insulator if i have an insulator like this or an insulator like this if i add energy to
11:39:26
that insulator i'm essentially going to be contributing some additional energy to some of these
11:39:32
electrons which would otherwise be filling the lowest possible energy state so i would be kicking them up to higher
11:39:37
energy states and if i have an insulator like this that isn't hasn't even filled all of its
11:39:43
all of its bound states well adding energy is going to kick them up to higher energy bound states it's unlikely
11:39:49
to make those electrons free but if i have a conductor that is almost that has entirely filled a sort of free
11:39:55
electron state and i add energy i may kick more and more and more electrons up to the next higher energy band
11:40:02
transitioning that insulator into a conductor so if i have an insulation material and i increase the temperature
11:40:08
the conductivity of the material tends to increase if i have a conductor on the other hand
11:40:15
and i add energy to these states while i'm not actually making any more pre-conduction electrons
11:40:21
i'm more just rearranging existing conduction electrons and that rearrangement actually happens to be
11:40:26
unfavorable under most circumstances the classical explanation that's usually given is that as you increase the
11:40:32
temperature of a material the orderedness of the material goes away essentially that nice
11:40:38
periodic array of delta function potentials becomes slightly disordered and that disrupts the band structure and makes it more difficult for electrons to
11:40:45
transition from one energy state to the next thinking about it classically the electrons are more likely to collide
11:40:51
into atoms that are vibrating rapidly than into atoms that are nice and stationary
11:40:56
so if i increase the temperature of an insulator i make it more conducting if i increase the temperature of a conductor
11:41:01
i make it less conducting if i increase the temperature of semiconductors you can actually do some
11:41:07
math to figure out what's going on i'm not going to ask you to do that but if you increase the temperature of a semiconductor typically you increase the
11:41:13
conductivity so we can understand a lot of the properties of how insulators and
11:41:19
conductors even semiconductors behave just with this simple periodic array of delta functions
11:41:26
which describes that the result are going to have the resulting energy states that are available for a
11:41:33
bound or free electron in this material are going to come in bands and the relative population of those
11:41:39
bands determines essentially the nature of the material
11:41:44
to check your understanding of this here are a few questions namely asking you to recall what that trick was to
11:41:50
figure out the boundary condition in terms of the discontinuity in the first derivative of the wave function at
11:41:56
a delta function uh finally describe how you suspect the solutions would change if the delta
11:42:02
function wells had been used instead of barriers we used barriers assuming that if the electron was right on top of the atom it
11:42:08
would be strongly repelled by essentially running into the atom but maybe it's actually attracted maybe there's maybe there are bound states as
11:42:14
well finally going back and looking at that equation for that gave you the energy
11:42:20
bands how do the energy bands look what is their spacing how wide are they et cetera as the energy becomes very large
11:42:28
and finally there's this essay that's uh intentionally humorous electron band structure in germanium my ass
11:42:34
i'd like you to read through that it's fun i'm not actually asking you to do all that much here and then explain qualitatively what the plot that he
11:42:40
describes should have looked like